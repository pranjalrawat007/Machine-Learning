{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47e41387-c6c0-4f60-9bfb-2ae047236e3c",
   "metadata": {},
   "source": [
    "### Double ML Partially Linear Regression \n",
    "\n",
    "1. Split Data: Randomly split data into $K$-folds.\n",
    "\n",
    "2. Estimate Nuisance Functions:\n",
    "   For each fold $k \\in \\{1, \\dots, K\\}$:\n",
    "   - Train $\\hat{\\ell}(X) \\approx \\mathbb{E}[Y \\mid X]$ and $\\hat{m}(X) \\approx \\mathbb{E}[D \\mid X]$ on $\\mathcal{I}_{-k}$.\n",
    "   - Predict $\\hat{\\ell}(X_i)$ and $\\hat{m}(X_i)$ for $i \\in \\mathcal{I}_k$.\n",
    "\n",
    "3. Residualization:\n",
    "   For all $i \\in \\{1, \\dots, N\\}$:\n",
    "   $ \\tilde{Y}_i = Y_i - \\hat{\\ell}(X_i), \\quad \\tilde{D}_i = D_i - \\hat{m}(X_i). $\n",
    "\n",
    "4. Second-Stage Regression:\n",
    "   - Regress $\\tilde{Y}$ on $\\tilde{D}$ to estimate:\n",
    "     $ \\hat{\\theta}_0 = \\frac{\\sum_{i=1}^N \\tilde{Y}_i \\tilde{D}_i}{\\sum_{i=1}^N \\tilde{D}_i^2}. $\n",
    "\n",
    "5. Variance and Confidence Interval:\n",
    "   - Compute residuals $\\hat{u}_i = \\tilde{Y}_i - \\hat{\\theta}_0 \\tilde{D}_i$.\n",
    "   - Estimate variance:\n",
    "     $ \\text{Var}(\\hat{\\theta}_0) = \\frac{\\frac{1}{N} \\sum_{i=1}^N \\hat{u}_i^2}{\\sum_{i=1}^N \\tilde{D}_i^2}. $\n",
    "   - Construct confidence interval:\n",
    "     $ \\text{CI} = \\hat{\\theta}_0 \\pm z_{1-\\alpha/2} \\sqrt{\\text{Var}(\\hat{\\theta}_0)}. $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6e254f3-300c-43df-9d30-cb0eb5550acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated theta: 0.457\n",
      "Standard error: 0.049\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMRegressor\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Generate synthetic data\n",
    "N = 500  # Number of observations\n",
    "p = 5  # Number of covariates\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Covariance matrix for multivariate normal\n",
    "Sigma = np.array([[0.7 ** abs(i - j) for j in range(p)] for i in range(p)])\n",
    "\n",
    "# Covariates X ~ N(0, Î£)\n",
    "X = np.random.multivariate_normal(mean=np.zeros(p), cov=Sigma, size=N)\n",
    "\n",
    "# True nuisance functions\n",
    "def m0(x):\n",
    "    return x[0] + 0.25 / (1 + np.exp(x[2])) + np.exp(x[3]) / (1 + np.exp(x[3]))\n",
    "\n",
    "def g0(x):\n",
    "    return np.exp(x[0]) / (1 + np.exp(x[0])) + 0.25 * x[3]\n",
    "\n",
    "theta_0 = 0.5  # True causal effect\n",
    "\n",
    "# Generate treatment D and outcome Y\n",
    "v = np.random.normal(size=N)\n",
    "zeta = np.random.normal(size=N)\n",
    "\n",
    "m_values = np.array([m0(x) for x in X])\n",
    "g_values = np.array([g0(x) for x in X])\n",
    "\n",
    "D = m_values + v\n",
    "Y = theta_0 * D + g_values + zeta\n",
    "\n",
    "# Function to implement the cross-fitting procedure\n",
    "def cross_fitting_plr(Y, D, X, K=10):\n",
    "    # Initialize residuals\n",
    "    Y_residual = np.zeros_like(Y)\n",
    "    D_residual = np.zeros_like(D)\n",
    "\n",
    "    # K-fold cross-splitting\n",
    "    kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        # Train nuisance models on training folds\n",
    "        model_m = LGBMRegressor(verbose=-1)\n",
    "        model_ell = LGBMRegressor(verbose=-1)\n",
    "\n",
    "        model_m.fit(X[train_idx], D[train_idx])  # Predict E[D|X]\n",
    "        model_ell.fit(X[train_idx], Y[train_idx])  # Predict E[Y|X]\n",
    "\n",
    "        # Predict on validation folds\n",
    "        m_hat = model_m.predict(X[val_idx])\n",
    "        ell_hat = model_ell.predict(X[val_idx])\n",
    "\n",
    "        # Compute residuals\n",
    "        Y_residual[val_idx] = Y[val_idx] - ell_hat\n",
    "        D_residual[val_idx] = D[val_idx] - m_hat\n",
    "\n",
    "    # Second-stage regression\n",
    "    model = sm.OLS(Y_residual, sm.add_constant(D_residual)).fit()\n",
    "\n",
    "    # Return theta estimate and standard error\n",
    "    return model.params[1], model.bse[1]\n",
    "\n",
    "# Run the cross-fitting procedure\n",
    "theta_hat, stderr = cross_fitting_plr(Y, D, X, K=10)\n",
    "\n",
    "# Output results\n",
    "print(f\"Estimated theta: {theta_hat:.3f}\")\n",
    "print(f\"Standard error: {stderr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87c8b5f-ae39-4d28-b2b9-81414a2b81b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
