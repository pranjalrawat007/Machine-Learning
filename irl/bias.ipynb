{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NXFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid size: 3x3\n",
      "Number of features: 3\n",
      "Number of states: 9\n",
      "Number of trajectories: 500\n",
      "Trajectory length: 10\n",
      "\n",
      "True weights: [0.4 0.2 0.4]\n",
      "\n",
      "Estimation Results:\n",
      " Weight  True  Estimate  Std. Error\n",
      "Theta_1   0.4  0.411456    0.578342\n",
      "Theta_2   0.2  0.204024    0.577305\n",
      "Theta_3   0.4  0.386063    0.577772\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import logsumexp\n",
    "from time import time\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Define the GridWorld environment\n",
    "class IcyGridWorld:\n",
    "    def __init__(self, size=3):\n",
    "        self.size = size  # Grid size (e.g., 3x3)\n",
    "        self.features = np.zeros((size, size))\n",
    "        self.weights = {}\n",
    "        self.define_features()\n",
    "        self.states = [(x, y) for x in range(size) for y in range(size)]\n",
    "\n",
    "    def define_features(self):\n",
    "        for x in range(self.size):\n",
    "            for y in range(self.size):\n",
    "                if (x + y) % 3 == 0:\n",
    "                    self.features[x, y] = 1\n",
    "                elif (x + y) % 3 == 1:\n",
    "                    self.features[x, y] = 2\n",
    "                else:\n",
    "                    self.features[x, y] = 3\n",
    "        # Normalize weights to sum to 1 and stay in (0,1)\n",
    "        raw_weights = {1: 0.4, 2: 0.2, 3: 0.4}\n",
    "        abs_values = np.abs(list(raw_weights.values()))\n",
    "        total = sum(abs_values)\n",
    "        self.weights = {key: val / total for key, val in raw_weights.items()}\n",
    "\n",
    "    def get_weight(self, position):\n",
    "        x, y = position\n",
    "        feature = self.features[x, y]\n",
    "        return self.weights[feature]\n",
    "\n",
    "# Define the Agent class\n",
    "class Agent:\n",
    "    def __init__(self, gridworld, discount_factor=0.95):\n",
    "        self.gridworld = gridworld\n",
    "        self.discount_factor = discount_factor\n",
    "        self.actions = ['up', 'down', 'left', 'right', 'stay']\n",
    "        self.num_states = len(gridworld.states)\n",
    "        self.state_indices = {state: idx for idx, state in enumerate(gridworld.states)}\n",
    "\n",
    "    def move(self, position, action):\n",
    "        x, y = position\n",
    "        if action == 'up' and x > 0:\n",
    "            x -= 1\n",
    "        elif action == 'down' and x < self.gridworld.size - 1:\n",
    "            x += 1\n",
    "        elif action == 'left' and y > 0:\n",
    "            y -= 1\n",
    "        elif action == 'right' and y < self.gridworld.size - 1:\n",
    "            y += 1\n",
    "        return (x, y)\n",
    "\n",
    "    def value_iteration(self, max_iter=1000, tol=1e-7):\n",
    "        V = np.zeros(self.num_states)\n",
    "        for iteration in range(max_iter):\n",
    "            V_prev = V.copy()\n",
    "            for idx, state in enumerate(self.gridworld.states):\n",
    "                action_values = []\n",
    "                for action in self.actions:\n",
    "                    new_state = self.move(state, action)\n",
    "                    reward = self.gridworld.get_weight(new_state)\n",
    "                    new_state_idx = self.state_indices[new_state]\n",
    "                    Q_value = reward + self.discount_factor * V_prev[new_state_idx]\n",
    "                    action_values.append(Q_value)\n",
    "                V[idx] = logsumexp(action_values)\n",
    "            if np.max(np.abs(V - V_prev)) < tol:\n",
    "                break\n",
    "        return V\n",
    "\n",
    "    def get_action_probabilities(self, state, V):\n",
    "        action_values = []\n",
    "        for action in self.actions:\n",
    "            new_state = self.move(state, action)\n",
    "            reward = self.gridworld.get_weight(new_state)\n",
    "            new_state_idx = self.state_indices[new_state]\n",
    "            Q_value = reward + self.discount_factor * V[new_state_idx]\n",
    "            action_values.append(Q_value)\n",
    "        log_probs = action_values - logsumexp(action_values)\n",
    "        return np.exp(log_probs)\n",
    "\n",
    "def generate_trajectory(agent, start_position, max_steps=10):\n",
    "    trajectory = []\n",
    "    position = start_position\n",
    "    V = agent.value_iteration()\n",
    "    for _ in range(max_steps):\n",
    "        probs = agent.get_action_probabilities(position, V)\n",
    "        action = np.random.choice(agent.actions, p=probs)\n",
    "        new_position = agent.move(position, action)\n",
    "        reward = agent.gridworld.get_weight(new_position)\n",
    "        trajectory.append((position, action, reward))\n",
    "        position = new_position\n",
    "    return trajectory\n",
    "\n",
    "def generate_expert_data(agent, num_trajectories, max_steps=10):\n",
    "    trajectories = []\n",
    "    for _ in range(num_trajectories):\n",
    "        start_position = (random.randint(0, agent.gridworld.size - 1), random.randint(0, agent.gridworld.size - 1))\n",
    "        trajectories.append(generate_trajectory(agent, start_position, max_steps))\n",
    "    return trajectories\n",
    "\n",
    "def nfxp_log_likelihood(params, agent, trajectories):\n",
    "    agent.gridworld.weights = {1: params[0], 2: params[1], 3: params[2]}\n",
    "    V = agent.value_iteration()\n",
    "    log_likelihood = 0\n",
    "    for trajectory in trajectories:\n",
    "        for (position, action, _) in trajectory:\n",
    "            probs = agent.get_action_probabilities(position, V)\n",
    "            action_index = agent.actions.index(action)\n",
    "            log_likelihood += np.log(probs[action_index] + 1e-12)\n",
    "    return -log_likelihood\n",
    "\n",
    "def estimate_nfxp(agent, trajectories):\n",
    "    initial_params = [0.3, 0.3, 0.4]  # Initial guess (sum to 1)\n",
    "    bounds = [(0.01, 0.99)] * 3  # Enforce (0,1) constraint\n",
    "    result = minimize(\n",
    "        nfxp_log_likelihood,\n",
    "        initial_params,\n",
    "        args=(agent, trajectories),\n",
    "        bounds=bounds,\n",
    "        method='L-BFGS-B',\n",
    "        options={'maxiter': 1000, 'disp': False}\n",
    "    )\n",
    "\n",
    "    # Calculate standard errors from the Hessian\n",
    "    hessian_inv = result.hess_inv.todense() if hasattr(result.hess_inv, 'todense') else result.hess_inv\n",
    "    standard_errors = np.sqrt(np.diag(hessian_inv)) if hessian_inv is not None else [np.nan] * len(initial_params)\n",
    "\n",
    "    return result.x, standard_errors\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    gridworld = IcyGridWorld()\n",
    "    agent = Agent(gridworld)\n",
    "    num_trajectories = 500\n",
    "    max_steps = 10\n",
    "\n",
    "    print(f\"Grid size: {gridworld.size}x{gridworld.size}\")\n",
    "    print(f\"Number of features: {len(gridworld.weights)}\")\n",
    "    print(f\"Number of states: {len(gridworld.states)}\")\n",
    "    print(f\"Number of trajectories: {num_trajectories}\")\n",
    "    print(f\"Trajectory length: {max_steps}\")\n",
    "\n",
    "    expert_data = generate_expert_data(agent, num_trajectories, max_steps)\n",
    "\n",
    "    # True weights\n",
    "    true_weights = np.array(list(gridworld.weights.values()))\n",
    "    print(f\"\\nTrue weights: {true_weights}\")\n",
    "\n",
    "    # Estimate weights and standard errors\n",
    "    estimated_weights, standard_errors = estimate_nfxp(agent, expert_data)\n",
    "\n",
    "    # Display results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Weight': ['Theta_1', 'Theta_2', 'Theta_3'],\n",
    "        'True': true_weights,\n",
    "        'Estimate': estimated_weights,\n",
    "        'Std. Error': standard_errors\n",
    "    })\n",
    "\n",
    "    print(\"\\nEstimation Results:\")\n",
    "    print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAXENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "# Define the GridWorld environment\n",
    "class IcyGridWorld:\n",
    "    def __init__(self, size=3):\n",
    "        self.size = size  # Grid size (e.g., 3x3)\n",
    "        self.features = np.zeros((size, size))\n",
    "        self.weights = {}\n",
    "        self.define_features()\n",
    "        self.states = [(x, y) for x in range(size) for y in range(size)]\n",
    "\n",
    "    def define_features(self):\n",
    "        for x in range(self.size):\n",
    "            for y in range(self.size):\n",
    "                if (x + y) % 3 == 0:\n",
    "                    self.features[x, y] = 1\n",
    "                elif (x + y) % 3 == 1:\n",
    "                    self.features[x, y] = 2\n",
    "                else:\n",
    "                    self.features[x, y] = 3\n",
    "        self.weights = {1: 0.7, 2: 0.2, 3: 0.1}\n",
    "\n",
    "    def get_weight(self, position):\n",
    "        x, y = position\n",
    "        feature = self.features[x, y]\n",
    "        return self.weights[feature]\n",
    "\n",
    "# Define the Agent class\n",
    "class Agent:\n",
    "    def __init__(self, gridworld, discount_factor=0.95):\n",
    "        self.gridworld = gridworld\n",
    "        self.discount_factor = discount_factor\n",
    "        self.actions = ['up', 'down', 'left', 'right', 'stay']\n",
    "        self.num_states = len(gridworld.states)\n",
    "        self.state_indices = {state: idx for idx, state in enumerate(gridworld.states)}\n",
    "\n",
    "    def move(self, position, action):\n",
    "        x, y = position\n",
    "        if action == 'up' and x > 0:\n",
    "            x -= 1\n",
    "        elif action == 'down' and x < self.gridworld.size - 1:\n",
    "            x += 1\n",
    "        elif action == 'left' and y > 0:\n",
    "            y -= 1\n",
    "        elif action == 'right' and y < self.gridworld.size - 1:\n",
    "            y += 1\n",
    "        return (x, y)\n",
    "\n",
    "    def value_iteration(self, max_iter=1000, tol=1e-7):\n",
    "        V = np.zeros(self.num_states)\n",
    "        for iteration in range(max_iter):\n",
    "            V_prev = V.copy()\n",
    "            for idx, state in enumerate(self.gridworld.states):\n",
    "                action_values = []\n",
    "                for action in self.actions:\n",
    "                    new_state = self.move(state, action)\n",
    "                    reward = self.gridworld.get_weight(new_state)\n",
    "                    new_state_idx = self.state_indices[new_state]\n",
    "                    Q_value = reward + self.discount_factor * V_prev[new_state_idx]\n",
    "                    action_values.append(Q_value)\n",
    "                V[idx] = logsumexp(action_values)\n",
    "            if np.max(np.abs(V - V_prev)) < tol:\n",
    "                break\n",
    "        return V\n",
    "\n",
    "    def get_action_probabilities(self, state, V):\n",
    "        action_values = []\n",
    "        for action in self.actions:\n",
    "            new_state = self.move(state, action)\n",
    "            reward = self.gridworld.get_weight(new_state)\n",
    "            new_state_idx = self.state_indices[new_state]\n",
    "            Q_value = reward + self.discount_factor * V[new_state_idx]\n",
    "            action_values.append(Q_value)\n",
    "        log_probs = action_values - logsumexp(action_values)\n",
    "        return np.exp(log_probs)\n",
    "\n",
    "# Generate trajectories\n",
    "def generate_trajectory(agent, start_position, max_steps=10):\n",
    "    trajectory = []\n",
    "    position = start_position\n",
    "    V = agent.value_iteration()\n",
    "    for _ in range(max_steps):\n",
    "        probs = agent.get_action_probabilities(position, V)\n",
    "        action = np.random.choice(agent.actions, p=probs)\n",
    "        new_position = agent.move(position, action)\n",
    "        trajectory.append((position, action))\n",
    "        position = new_position\n",
    "    return trajectory\n",
    "\n",
    "def generate_expert_data(agent, num_trajectories, max_steps=10):\n",
    "    trajectories = []\n",
    "    for _ in range(num_trajectories):\n",
    "        start_position = (np.random.randint(0, agent.gridworld.size), np.random.randint(0, agent.gridworld.size))\n",
    "        trajectories.append(generate_trajectory(agent, start_position, max_steps))\n",
    "    return trajectories\n",
    "\n",
    "# MaxEnt IRL\n",
    "def maxent_irl(features, trajectories, num_states, discount, learning_rate=0.1, num_iterations=1000):\n",
    "    num_features = features.shape[1]\n",
    "    theta = np.random.uniform(size=num_features)\n",
    "    mu_D = np.zeros(num_features)\n",
    "    \n",
    "    # Compute feature expectations from trajectories\n",
    "    for trajectory in trajectories:\n",
    "        for state, _ in trajectory:\n",
    "            mu_D += features[state]\n",
    "    mu_D /= len(trajectories)\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        # Compute rewards and policy\n",
    "        rewards = features @ theta\n",
    "        V = np.zeros(num_states)\n",
    "        for _ in range(1000):  # Value iteration\n",
    "            V_prev = V.copy()\n",
    "            for s in range(num_states):\n",
    "                V[s] = logsumexp([rewards[s] + discount * V_prev[s]])\n",
    "            if np.max(np.abs(V - V_prev)) < 1e-7:\n",
    "                break\n",
    "\n",
    "        # Policy from value function\n",
    "        policy = np.zeros((num_states, num_states))\n",
    "        for s in range(num_states):\n",
    "            policy[s] = np.exp(rewards[s] + discount * V[s] - logsumexp(rewards + discount * V))\n",
    "        \n",
    "        # State visitation frequencies\n",
    "        mu_theta = np.zeros(num_features)\n",
    "        for s in range(num_states):\n",
    "            mu_theta += features[s] * policy[s].sum()\n",
    "\n",
    "        # Update parameters\n",
    "        grad = mu_D - mu_theta\n",
    "        theta += learning_rate * grad\n",
    "\n",
    "    return theta / theta.sum()\n",
    "\n",
    "# Main integration\n",
    "if __name__ == \"__main__\":\n",
    "    gridworld = IcyGridWorld(size=3)\n",
    "    agent = Agent(gridworld)\n",
    "    trajectories = generate_expert_data(agent, num_trajectories=500, max_steps=20)\n",
    "\n",
    "    num_states = len(gridworld.states)\n",
    "    features = np.zeros((num_states, 3))\n",
    "    for x in range(gridworld.size):\n",
    "        for y in range(gridworld.size):\n",
    "            idx = agent.state_indices[(x, y)]\n",
    "            features[idx, int(gridworld.features[x, y]) - 1] = 1\n",
    "\n",
    "    weights = maxent_irl(features, trajectories, num_states, discount=0.95)\n",
    "    print(f\"Recovered Weights: {weights}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxmargin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'IcyGridWorld' object has no attribute 'state_indices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 134\u001b[0m\n\u001b[1;32m    132\u001b[0m gridworld \u001b[38;5;241m=\u001b[39m IcyGridWorld(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    133\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(gridworld)\n\u001b[0;32m--> 134\u001b[0m trajectories \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_expert_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_trajectories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m features \u001b[38;5;241m=\u001b[39m state_features(gridworld)\n\u001b[1;32m    137\u001b[0m alpha, margin \u001b[38;5;241m=\u001b[39m maxmargin_irl(features, trajectories, \u001b[38;5;28mlen\u001b[39m(gridworld\u001b[38;5;241m.\u001b[39mstates), discount\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 64\u001b[0m, in \u001b[0;36mAgent.generate_expert_data\u001b[0;34m(self, num_trajectories, max_steps)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_trajectories):\n\u001b[1;32m     63\u001b[0m     start_position \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgridworld\u001b[38;5;241m.\u001b[39msize), np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgridworld\u001b[38;5;241m.\u001b[39msize))\n\u001b[0;32m---> 64\u001b[0m     trajectories\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trajectories\n",
      "Cell \u001b[0;32mIn[9], line 56\u001b[0m, in \u001b[0;36mAgent.generate_trajectory\u001b[0;34m(self, start_position, max_steps)\u001b[0m\n\u001b[1;32m     54\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions)\n\u001b[1;32m     55\u001b[0m     new_position \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmove(position, action)\n\u001b[0;32m---> 56\u001b[0m     trajectory\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgridworld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_indices\u001b[49m[position], action))\n\u001b[1;32m     57\u001b[0m     position \u001b[38;5;241m=\u001b[39m new_position\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trajectory\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'IcyGridWorld' object has no attribute 'state_indices'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "# Define the GridWorld environment\n",
    "class IcyGridWorld:\n",
    "    def __init__(self, size=3):\n",
    "        self.size = size  # Grid size (e.g., 3x3)\n",
    "        self.features = np.zeros((size, size))\n",
    "        self.weights = {}\n",
    "        self.define_features()\n",
    "        self.states = [(x, y) for x in range(size) for y in range(size)]\n",
    "\n",
    "    def define_features(self):\n",
    "        for x in range(self.size):\n",
    "            for y in range(self.size):\n",
    "                if (x + y) % 3 == 0:\n",
    "                    self.features[x, y] = 1\n",
    "                elif (x + y) % 3 == 1:\n",
    "                    self.features[x, y] = 2\n",
    "                else:\n",
    "                    self.features[x, y] = 3\n",
    "        self.weights = {1: 0.4, 2: 0.2, 3: 0.4}\n",
    "\n",
    "    def get_weight(self, position):\n",
    "        x, y = position\n",
    "        feature = self.features[x, y]\n",
    "        return self.weights[feature]\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, gridworld, discount_factor=0.95):\n",
    "        self.gridworld = gridworld\n",
    "        self.discount_factor = discount_factor\n",
    "        self.actions = ['up', 'down', 'left', 'right', 'stay']\n",
    "        self.num_states = len(gridworld.states)\n",
    "        self.state_indices = {state: idx for idx, state in enumerate(gridworld.states)}\n",
    "\n",
    "    def move(self, position, action):\n",
    "        x, y = position\n",
    "        if action == 'up' and x > 0:\n",
    "            x -= 1\n",
    "        elif action == 'down' and x < self.gridworld.size - 1:\n",
    "            x += 1\n",
    "        elif action == 'left' and y > 0:\n",
    "            y -= 1\n",
    "        elif action == 'right' and y < self.gridworld.size - 1:\n",
    "            y += 1\n",
    "        return (x, y)\n",
    "\n",
    "    def generate_trajectory(self, start_position, max_steps=10):\n",
    "        trajectory = []\n",
    "        position = start_position\n",
    "        for _ in range(max_steps):\n",
    "            action = np.random.choice(self.actions)\n",
    "            new_position = self.move(position, action)\n",
    "            trajectory.append((self.gridworld.states.index(position), action))\n",
    "            position = new_position\n",
    "        return trajectory\n",
    "\n",
    "    def generate_expert_data(self, num_trajectories, max_steps=10):\n",
    "        trajectories = []\n",
    "        for _ in range(num_trajectories):\n",
    "            start_position = (np.random.randint(0, self.gridworld.size), np.random.randint(0, self.gridworld.size))\n",
    "            trajectories.append(self.generate_trajectory(start_position, max_steps))\n",
    "        return trajectories\n",
    "\n",
    "# Max-Margin IRL\n",
    "\n",
    "def state_features(gridworld):\n",
    "    num_states = len(gridworld.states)\n",
    "    num_features = 3  # Assuming 3 features as defined in IcyGridWorld\n",
    "    features = np.zeros((num_states, num_features))\n",
    "    for x in range(gridworld.size):\n",
    "        for y in range(gridworld.size):\n",
    "            state_index = gridworld.states.index((x, y))\n",
    "            feature_type = int(gridworld.features[x, y]) - 1  # Map features {1, 2, 3} to indices {0, 1, 2}\n",
    "            features[state_index, feature_type] = 1\n",
    "    return features\n",
    "    return np.eye(num_states)\n",
    "\n",
    "def feature_expectations(features, trajectories, discount):\n",
    "    expectations = np.zeros(features.shape[1])\n",
    "    for trajectory in trajectories:\n",
    "        for t, (state, _) in enumerate(trajectory):\n",
    "            expectations += (discount ** t) * features[state]\n",
    "    expectations /= len(trajectories)\n",
    "    return expectations\n",
    "\n",
    "def maxmargin_irl(features, trajectories, num_states, discount, num_comparator_policies=5):\n",
    "    mu_D = feature_expectations(features, trajectories, discount)\n",
    "    comparator_expectations = []\n",
    "\n",
    "    for _ in range(num_comparator_policies):\n",
    "        random_policy = np.random.dirichlet(np.ones(len(features[0])), size=num_states)\n",
    "        random_policy /= random_policy.sum(axis=1, keepdims=True)\n",
    "        mu_j = feature_expectations(features, agent.generate_expert_data(num_trajectories=10, max_steps=10), discount)\n",
    "        comparator_expectations.append(mu_j)\n",
    "\n",
    "    comparator_expectations = np.array(comparator_expectations)\n",
    "    num_features = features.shape[1]\n",
    "\n",
    "    c = np.zeros(num_features + 1)\n",
    "    c[-1] = -1.0\n",
    "\n",
    "    A_ub = []\n",
    "    b_ub = []\n",
    "    for mu_j in comparator_expectations:\n",
    "        diff = mu_D - mu_j\n",
    "        row = np.zeros(num_features + 1)\n",
    "        row[:num_features] = -diff\n",
    "        row[-1] = 1.0\n",
    "        A_ub.append(row)\n",
    "        b_ub.append(0.0)\n",
    "\n",
    "    bounds = [(0, None)] * num_features + [(0, None)]\n",
    "    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n",
    "\n",
    "    if not res.success:\n",
    "        raise ValueError(f\"Linear Program did not find a feasible solution. Debug info:\n",
    "A_ub: {A_ub}\n",
    "b_ub: {b_ub}\n",
    "mu_D: {mu_D}\n",
    "mu_j: {comparator_expectations}\")\n",
    "\n",
    "    x = res.x\n",
    "    alpha = x[:-1]\n",
    "    margin = x[-1]\n",
    "\n",
    "    return alpha, margin\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gridworld = IcyGridWorld(size=3)\n",
    "    agent = Agent(gridworld)\n",
    "    trajectories = agent.generate_expert_data(num_trajectories=50, max_steps=10)\n",
    "\n",
    "    features = state_features(gridworld)\n",
    "    alpha, margin = maxmargin_irl(features, trajectories, len(gridworld.states), discount=0.95)\n",
    "\n",
    "    print(f\"Recovered Weights: {alpha}\")\n",
    "    print(f\"Margin: {margin}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
