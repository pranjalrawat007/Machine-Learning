import numpy as np
from numpy.random import uniform
from functools import reduce
from scipy.stats import norm
from scipy.optimize import linprog
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors

############################
# Helper Functions
############################

def pi_star(s):
    """
    Optimal policy: 
    If s < 0: action = 1 (stay)
    If s >= 0: action = -1 (switch)
    """
    return 1 if s < 0 else -1

def noise():
    """Random noise uniform in [-0.5, 0.5]."""
    return uniform(-0.5, 0.5)

def truncate_state(s):
    """
    Truncate state s to be within [-1, 1].
    """
    if s > 1:
        return 1
    elif s < -1:
        return -1
    else:
        return s

def step(s, policy):
    """
    Given a state s and a policy (function), 
    compute the next state:
    next_state = policy(s)*s + noise(), truncated to [-1,1].
    """
    return truncate_state(policy(s)*s + noise())

def monte_carlo_sim(policy, 
                    n_trajectories=20, 
                    trajectory_length=20):
    """
    Generate multiple trajectories under a given policy.
    Returns a list of trajectories, each a list of states.
    """
    trajectories = []
    for _ in range(n_trajectories):
        s0 = uniform(-1, 1)
        # We'll store the full trajectory including initial state
        traj = [s0]
        for __ in range(trajectory_length - 1):
            s_next = step(traj[-1], policy)
            traj.append(s_next)
        trajectories.append(traj)
    return trajectories

def calc_value_function_estimate(trajectories, discount=0.9, n_basis=21):
    """
    Calculate the value function estimates for a given set of trajectories.
    The value function is estimated for each Gaussian basis function.
    
    - We create 21 Gaussian basis functions with means in seq(-1, 1, length=21)
      and a fixed sd = 2/22 (as in the original code).
    - For each basis function i, we compute the discounted sum of rewards 
      if the reward was given by that basis function.
    - Then we take the average over trajectories.
    
    Returns: A numpy array of length n_basis representing the V_i^{pi}(s_0) estimates.
    """
    means = np.linspace(-1, 1, n_basis)
    sd = 2/22
    # Each trajectory length:
    T = len(trajectories[0]) if len(trajectories) > 0 else 0
    discounts = np.array([discount**t for t in range(T)])
    
    values = np.zeros(n_basis)
    for i, m in enumerate(means):
        # Compute discounted sum of phi_i(s_t)
        # phi_i(s) = dnorm(s, mean=m, sd=2/22)
        # For each trajectory:
        traj_returns = []
        for traj in trajectories:
            phi_values = norm.pdf(traj, loc=m, scale=sd)
            traj_return = np.sum(phi_values * discounts)
            traj_returns.append(traj_return)
        # Average over all trajectories:
        values[i] = np.mean(traj_returns) if len(traj_returns) > 0 else 0.0
    return values

def reward_function(s, alpha, n_basis=21):
    """
    Compute R(s) = sum(alpha_i * phi_i(s)), where phi_i are Gaussian basis functions.
    """
    means = np.linspace(-1, 1, n_basis)
    sd = 2/22
    phi_values = norm.pdf(s, loc=means, scale=sd)
    return np.sum(alpha * phi_values)

def construct_random_policy():
    """
    Generate a random policy:
    The policy is defined by a shape in {-1,1} and an inflection point in seq(-1,1,21).
    If s < inflection: action = -shape
    else: action = shape
    """
    shape = np.random.choice([-1, 1])
    inflection = np.random.choice(np.linspace(-1, 1, 21))
    
    def policy(s):
        return -shape if s < inflection else shape
    
    return policy

def solve_lp_for_alphas(value_pi_star, list_of_other_values):
    """
    Solve the LP that finds alpha:
    max sum (V^{pi_star}(s0) - V^{pi_i}(s0))
    s.t. alpha_i <= 1 for all i
         alpha_i >= 0 
        
    Since linprog in Python minimizes the objective, we negate the coefficients to perform maximization.
    """
    
    # Combine differences from all policies:
    V_pi_star = list_of_other_values[-1]
    total_diff = np.zeros_like(V_pi_star)
    for j in range(len(list_of_other_values) - 1):
        total_diff += (V_pi_star - list_of_other_values[j])
    
    # We want to maximize total_diff' * alpha
    # linprog does minimization, so we minimize - (total_diff' * alpha)
    c = -total_diff  # minimize negative to get maximize
    
    # Constraints:
    # alpha_i <= 1
    # alpha_i >= 0 (linprog defaults alpha_i >= 0)
    A_ub = np.eye(len(c))
    b_ub = np.ones(len(c))
    
    # Solve the LP
    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=(0, 1), method='highs')
    if res.success:
        return res.x
    else:
        raise ValueError("LP did not find a feasible solution.")

def lp_irl_sample_trajectories(data, 
                               max_iter=20, 
                               print_progress=True, 
                               discount=0.9, 
                               n_basis=21):
    """
    Run the iterative IRL process:
    1. Start with data from pi_star
    2. Initialize alpha and a random policy pi1
    3. Iteratively:
       - Generate trajectories from pi1
       - Compute value function estimate
       - Solve LP to update alphas
       - Check for convergence, if converged, return
    
    Returns (alpha, all_value_functions)
    """
    # Compute value function for pi_star
    value_pi_star = calc_value_function_estimate(data, discount=discount, n_basis=n_basis)
    value_functions = [value_pi_star]
    
    alpha = np.zeros(n_basis)  # Initialize alphas
    
    # Start with a random policy
    pi_current = construct_random_policy()
    
    for i in range(max_iter):
        # Generate trajectories with pi_current
        traj = monte_carlo_sim(pi_current, n_trajectories=20, trajectory_length=20)
        
        # Compute value function for this new policy
        value_pi_current = calc_value_function_estimate(traj, discount=discount, n_basis=n_basis)
        
        # Append the new value function at the beginning; pi_star remains last
        value_functions = [value_pi_current] + value_functions
        
        # Solve LP to update alphas
        alpha_next = solve_lp_for_alphas(value_functions[-1], value_functions)
        
        # Check for convergence after minimum iterations = 5
        if i >= 4 and np.max(np.abs(alpha_next - alpha)) < 0.1:
            if print_progress:
                print(f"Converged at iteration {i+1}")
            return alpha_next, value_functions
        
        if print_progress:
            print(f"Iteration {i+1}")
            print(alpha_next)
        
        alpha = alpha_next
        
        # Update policy based on the new alpha
        def pi_updated(s, alpha=alpha):
            return 1 if reward_function(s, alpha, n_basis=n_basis) > 0.5 else -1
        pi_current = pi_updated
    
    return alpha, value_functions


############################
# Example usage and demonstration
############################

if __name__ == "__main__":
    # Seed for reproducibility
    np.random.seed(1234)
    
    # 1. Create data using pi_star
    # The original R code used trajectory_length=100 for value function calculations
    data = monte_carlo_sim(pi_star, n_trajectories=20, trajectory_length=100)
    
    # 2. Run the IRL algorithm 5 times and collect the estimated alphas
    results = []
    for run in range(5):
        print(f"\nStarting IRL Run {run+1}")
        alpha_est, _ = lp_irl_sample_trajectories(data, print_progress=False)
        results.append(alpha_est)
    
    # 3. Define the true reward function
    # In the original R code, the true reward is 1 for the first 10 basis points and 0 for the rest
    true_reward = np.array([1]*10 + [0]*11)
    
    # 4. Print the estimated alphas and compare to true
    print("\nEstimated alphas from 5 runs:")
    for i, alpha_est in enumerate(results, start=1):
        print(f"Run {i}: {alpha_est}")
    
    print("\nTrue reward (for reference):", true_reward)
    
    # 5. Plot the estimated reward functions against the true reward
    x_vals = np.linspace(-1, 1, 21)
    
    plt.figure(figsize=(12, 8))
    
    # Define a color map with transparency for better visualization
    cmap = plt.get_cmap('tab10')
    colors = cmap.colors[:5]  # Use first 5 colors for the 5 runs
    
    for idx, alpha_est in enumerate(results):
        plt.scatter(x_vals, alpha_est, color=colors[idx], alpha=0.6, label=f'Estimate Run {idx+1}')
        plt.plot(x_vals, alpha_est, color=colors[idx], alpha=0.6)
    
    # Plot the true reward function
    plt.scatter(x_vals, true_reward, color='red', label='True Reward', zorder=5)
    plt.plot(x_vals, true_reward, color='red', linewidth=2, linestyle='--', zorder=5)
    
    plt.title("Approximated Reward Functions vs True Reward Function", fontsize=16)
    plt.xlabel("State Space (s)", fontsize=14)
    plt.ylabel("Reward R(s)", fontsize=14)
    plt.legend(fontsize=12)
    plt.grid(True)
    plt.tight_layout()
    plt.show()
