{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2]), array([0, 1, 2]), array([[[0.277272  , 0.25856784, 0.06806276],\n",
      "        [0.3684607 , 0.44929278, 0.23866789],\n",
      "        [0.2629952 , 0.29941424, 0.16636881]],\n",
      "\n",
      "       [[0.38306208, 0.43576104, 0.37098515],\n",
      "        [0.25908501, 0.09709523, 0.02312625],\n",
      "        [0.67618646, 0.15400447, 0.45143983]],\n",
      "\n",
      "       [[0.33966592, 0.30567111, 0.56095209],\n",
      "        [0.37245429, 0.45361199, 0.73820586],\n",
      "        [0.06081835, 0.54658129, 0.38219136]]]), array([0.3, 0.5, 0.8]), 0.9)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "n_S = 3\n",
    "n_A = 3\n",
    "gamma = 0.9\n",
    "\n",
    "def get_mdp(n_S, n_A, gamma):\n",
    "    S = np.arange(n_S)\n",
    "    A = np.arange(n_A)\n",
    "    P = np.zeros((n_S, n_S, n_A)) # Prob(s'|s,a)\n",
    "    for a in range(n_A):\n",
    "        for s in range(n_S):\n",
    "            p = np.random.rand(n_S)  # Random values for s'\n",
    "            P[:, s, a] = p / p.sum()  # Normalize to ensure sum over s' is 1\n",
    "    R = np.round(np.random.uniform(0,1,(n_S)),1) # R(s)\n",
    "    return (S, A, P, R, gamma)\n",
    "\n",
    "M = get_mdp(n_S, n_A, gamma)\n",
    "(S, A, P, R, gamma) = M\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17201566 0.319032   0.50895234]\n",
      " [0.08365263 0.42062593 0.49572143]\n",
      " [0.33136679 0.5548387  0.11379451]]\n"
     ]
    }
   ],
   "source": [
    "def get_random_policy(n_S, n_A):\n",
    "    pi = np.zeros((n_S, n_A))  # Prob(a|s)\n",
    "    for s in range(n_S):\n",
    "        p = np.random.rand(n_A)  # Random values for actions\n",
    "        pi[s, :] = p / p.sum()  # Normalize to ensure sum over actions is 1\n",
    "    return pi\n",
    "\n",
    "# Example usage\n",
    "pi_rand = get_random_policy(n_S, n_A)\n",
    "print(pi_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "[4.12252298 4.13077474 4.07099645]\n"
     ]
    }
   ],
   "source": [
    "def get_optimal_policy(M, T):\n",
    "    S, A, P, R, gamma = M\n",
    "    n_S = len(S)\n",
    "    n_A = len(A)\n",
    "    \n",
    "    V = np.zeros((T+1, n_S))  # V[t][s]: value at time t for state s\n",
    "    \n",
    "    # Backward induction for finite horizon\n",
    "    for t in range(T-1, -1, -1):\n",
    "        for s in range(n_S):\n",
    "            Q_sa = np.zeros(n_A)\n",
    "            for a in range(n_A):\n",
    "                Q_sa[a] = sum(P[s_prime, s, a] * (R[s_prime] + gamma * V[t+1][s_prime]) for s_prime in range(n_S))\n",
    "            V[t][s] = max(Q_sa)\n",
    "    \n",
    "    # Extract optimal policy at t=0\n",
    "    pi_opt = np.zeros((n_S, n_A))\n",
    "    for s in range(n_S):\n",
    "        Q_sa = np.zeros(n_A)\n",
    "        for a in range(n_A):\n",
    "            Q_sa[a] = sum(P[s_prime, s, a] * (R[s_prime] + gamma * V[1][s_prime]) for s_prime in range(n_S))\n",
    "        optimal_action = np.argmax(Q_sa)\n",
    "        pi_opt[s, optimal_action] = 1\n",
    "    \n",
    "    return pi_opt, V[0]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "T = 10\n",
    "pi_opt, V_opt = get_optimal_policy(M, T)\n",
    "print(pi_opt)\n",
    "print(V_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory States: [2, 0, 2, 2, 0, 2, 2, 2, 1, 0, 2]\n",
      "Trajectory Actions: [1, 2, 1, 1, 2, 1, 1, 1, 2, 2]\n",
      "Trajectory Rewards: [0.8, 0.3, 0.8, 0.8, 0.3, 0.8, 0.8, 0.8, 0.5, 0.3]\n",
      "Discounted Reward G: 4.1096720717\n"
     ]
    }
   ],
   "source": [
    "# initial state distribution \n",
    "\n",
    "def get_init_dist(n_S):\n",
    "    p = np.random.rand(n_S)\n",
    "    D = p / p.sum()\n",
    "    return D\n",
    "\n",
    "def get_init_state(S, D):\n",
    "    s0 = np.random.choice(S, p = D)\n",
    "    return s0\n",
    "\n",
    "D = get_init_dist(n_S)\n",
    "s0 = get_init_state(S, D)\n",
    "\n",
    "\n",
    "def get_trajectory(T, M, pi, s0):\n",
    "    S, A, P, R, gamma = M\n",
    "    traj_s = [s0]\n",
    "    traj_a = []\n",
    "    traj_r = [R[s0]]  # Include reward for the initial state\n",
    "    for t in range(T):\n",
    "        s = traj_s[t]\n",
    "        if np.sum(pi[s]) == 1 and (pi[s] == np.eye(len(A))[np.argmax(pi[s])]).all():\n",
    "            a = np.argmax(pi[s])\n",
    "        else:\n",
    "            a = np.random.choice(A, p=pi[s])\n",
    "        sp = np.random.choice(S, p=P[:, s, a])\n",
    "        traj_s.append(sp)\n",
    "        traj_a.append(a)\n",
    "        if t + 1 < T:\n",
    "            traj_r.append(R[sp])\n",
    "    G = sum(gamma**t * traj_r[t] for t in range(len(traj_r)))\n",
    "    return traj_s, traj_a, traj_r, G\n",
    "\n",
    "\n",
    "D = get_init_dist(n_S)\n",
    "s0 = get_init_state(S, D)\n",
    "T = 10\n",
    "traj_s, traj_a, traj_r, G = get_trajectory(T, M, pi_opt, s0)\n",
    "print(\"Trajectory States:\", traj_s)\n",
    "print(\"Trajectory Actions:\", traj_a)\n",
    "print(\"Trajectory Rewards:\", traj_r)\n",
    "print(\"Discounted Reward G:\", G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unconditional Average G over D: 4.056193366448799\n",
      "Conditional Average G for s0=0: 3.783977090678108\n"
     ]
    }
   ],
   "source": [
    "def get_unconditional_G(num_trajectories, T, M, pi, D):\n",
    "    total_G = 0\n",
    "    for _ in range(num_trajectories):\n",
    "        s0 = get_init_state(M[0], D)  # Sample s0 from initial distribution D\n",
    "        _, _, _, G = get_trajectory(T, M, pi, s0)\n",
    "        total_G += G\n",
    "    return total_G / num_trajectories\n",
    "\n",
    "def get_conditional_G(num_trajectories, T, M, pi, s0):\n",
    "    total_G = 0\n",
    "    for _ in range(num_trajectories):\n",
    "        _, _, _, G = get_trajectory(T, M, pi, s0)  # Use given s0\n",
    "        total_G += G\n",
    "    return total_G / num_trajectories\n",
    "\n",
    "D = get_init_dist(n_S)  # Initialize distribution\n",
    "num_trajectories = 1000\n",
    "avg_G_unconditional = get_unconditional_G(num_trajectories, T, M, pi_opt, D)\n",
    "print(f\"Unconditional Average G over D: {avg_G_unconditional}\")\n",
    "\n",
    "s0 = 0  # Specific starting state\n",
    "avg_G_conditional = get_conditional_G(num_trajectories, T, M, pi_opt, s0)\n",
    "print(f\"Conditional Average G for s0={s0}: {avg_G_conditional}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
