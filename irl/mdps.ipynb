{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2]), array([0, 1, 2]), array([[[0.29984948, 0.42367271, 0.44573531],\n",
      "        [0.3378003 , 0.60626228, 0.31581472],\n",
      "        [0.36242946, 0.34974274, 0.48133725]],\n",
      "\n",
      "       [[0.01891492, 0.42487334, 0.23432187],\n",
      "        [0.14931944, 0.02378561, 0.6725836 ],\n",
      "        [0.40162751, 0.34936108, 0.26057285]],\n",
      "\n",
      "       [[0.68123561, 0.15145395, 0.31994282],\n",
      "        [0.51288026, 0.36995211, 0.01160168],\n",
      "        [0.23594303, 0.30089618, 0.2580899 ]]]), array([0.7, 0.6, 0.3]), 0.9)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "n_S = 3\n",
    "n_A = 3\n",
    "gamma = 0.9\n",
    "\n",
    "def get_mdp(n_S, n_A, gamma):\n",
    "    S = np.arange(n_S)\n",
    "    A = np.arange(n_A)\n",
    "    P = np.zeros((n_S, n_S, n_A)) # Prob(s'|s,a)\n",
    "    for a in range(n_A):\n",
    "        for s in range(n_S):\n",
    "            p = np.random.rand(n_S)  # Random values for s'\n",
    "            P[:, s, a] = p / p.sum()  # Normalize to ensure sum over s' is 1\n",
    "    R = np.round(np.random.uniform(0,1,(n_S)),1) # R(s)\n",
    "    return (S, A, P, R, gamma)\n",
    "\n",
    "M = get_mdp(n_S, n_A, gamma)\n",
    "(S, A, P, R, gamma) = M\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49339288 0.07828154 0.42832558]\n",
      " [0.17141226 0.76102181 0.06756593]\n",
      " [0.64160828 0.00632262 0.3520691 ]]\n"
     ]
    }
   ],
   "source": [
    "def get_random_policy(n_S, n_A):\n",
    "    pi = np.zeros((n_S, n_A))  # Prob(a|s)\n",
    "    for s in range(n_S):\n",
    "        p = np.random.rand(n_A)  # Random values for actions\n",
    "        pi[s, :] = p / p.sum()  # Normalize to ensure sum over actions is 1\n",
    "    return pi\n",
    "\n",
    "# Example usage\n",
    "pi_rand = get_random_policy(n_S, n_A)\n",
    "print(pi_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "[3.96302434 4.00882195 3.9279263 ]\n"
     ]
    }
   ],
   "source": [
    "def get_optimal_policy(M, T):\n",
    "    S, A, P, R, gamma = M\n",
    "    n_S = len(S)\n",
    "    n_A = len(A)\n",
    "    \n",
    "    V = np.zeros((T+1, n_S))  # V[t][s]: value at time t for state s\n",
    "    \n",
    "    # Backward induction for finite horizon\n",
    "    for t in range(T-1, -1, -1):\n",
    "        for s in range(n_S):\n",
    "            Q_sa = np.zeros(n_A)\n",
    "            for a in range(n_A):\n",
    "                Q_sa[a] = sum(P[s_prime, s, a] * (R[s_prime] + gamma * V[t+1][s_prime]) for s_prime in range(n_S))\n",
    "            V[t][s] = max(Q_sa)\n",
    "    \n",
    "    # Extract optimal policy at t=0\n",
    "    pi_opt = np.zeros((n_S, n_A))\n",
    "    for s in range(n_S):\n",
    "        Q_sa = np.zeros(n_A)\n",
    "        for a in range(n_A):\n",
    "            Q_sa[a] = sum(P[s_prime, s, a] * (R[s_prime] + gamma * V[1][s_prime]) for s_prime in range(n_S))\n",
    "        optimal_action = np.argmax(Q_sa)\n",
    "        pi_opt[s, optimal_action] = 1\n",
    "    \n",
    "    return pi_opt, V[0]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "T = 10\n",
    "pi_opt, V_opt = get_optimal_policy(M, T)\n",
    "print(pi_opt)\n",
    "print(V_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory States: [2, 1, 1, 1, 0, 0, 2, 1, 0, 0, 1]\n",
      "Trajectory Actions: [0, 2, 2, 2, 1, 1, 0, 2, 1, 1]\n",
      "Trajectory Rewards: [0.3, 0.6, 0.6, 0.6, 0.7, 0.7, 0.3, 0.6, 0.7, 0.7]\n",
      "Discounted Reward G: 3.6549448293000006\n"
     ]
    }
   ],
   "source": [
    "# initial state distribution \n",
    "\n",
    "def get_init_dist(n_S):\n",
    "    p = np.random.rand(n_S)\n",
    "    D = p / p.sum()\n",
    "    return D\n",
    "\n",
    "def get_init_state(S, D):\n",
    "    s0 = np.random.choice(S, p = D)\n",
    "    return s0\n",
    "\n",
    "D = get_init_dist(n_S)\n",
    "s0 = get_init_state(S, D)\n",
    "\n",
    "\n",
    "def get_trajectory(T, M, pi, s0):\n",
    "    S, A, P, R, gamma = M\n",
    "    traj_s = [s0]\n",
    "    traj_a = []\n",
    "    traj_r = [R[s0]]  # Include reward for the initial state\n",
    "    for t in range(T):\n",
    "        s = traj_s[t]\n",
    "        if np.sum(pi[s]) == 1 and (pi[s] == np.eye(len(A))[np.argmax(pi[s])]).all():\n",
    "            a = np.argmax(pi[s])\n",
    "        else:\n",
    "            a = np.random.choice(A, p=pi[s])\n",
    "        sp = np.random.choice(S, p=P[:, s, a])\n",
    "        traj_s.append(sp)\n",
    "        traj_a.append(a)\n",
    "        if t + 1 < T:\n",
    "            traj_r.append(R[sp])\n",
    "    G = sum(gamma**t * traj_r[t] for t in range(len(traj_r)))\n",
    "    return traj_s, traj_a, traj_r, G\n",
    "\n",
    "\n",
    "D = get_init_dist(n_S)\n",
    "s0 = get_init_state(S, D)\n",
    "T = 10\n",
    "traj_s, traj_a, traj_r, G = get_trajectory(T, M, pi_opt, s0)\n",
    "print(\"Trajectory States:\", traj_s)\n",
    "print(\"Trajectory Actions:\", traj_a)\n",
    "print(\"Trajectory Rewards:\", traj_r)\n",
    "print(\"Discounted Reward G:\", G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unconditional Average G over D: 4.056193366448799\n",
      "Conditional Average G for s0=0: 3.783977090678108\n"
     ]
    }
   ],
   "source": [
    "def get_unconditional_G(num_trajectories, T, M, pi, D):\n",
    "    total_G = 0\n",
    "    for _ in range(num_trajectories):\n",
    "        s0 = get_init_state(M[0], D)  # Sample s0 from initial distribution D\n",
    "        _, _, _, G = get_trajectory(T, M, pi, s0)\n",
    "        total_G += G\n",
    "    return total_G / num_trajectories\n",
    "\n",
    "def get_conditional_G(num_trajectories, T, M, pi, s0):\n",
    "    total_G = 0\n",
    "    for _ in range(num_trajectories):\n",
    "        _, _, _, G = get_trajectory(T, M, pi, s0)  # Use given s0\n",
    "        total_G += G\n",
    "    return total_G / num_trajectories\n",
    "\n",
    "D = get_init_dist(n_S)  # Initialize distribution\n",
    "num_trajectories = 1000\n",
    "avg_G_unconditional = get_unconditional_G(num_trajectories, T, M, pi_opt, D)\n",
    "print(f\"Unconditional Average G over D: {avg_G_unconditional}\")\n",
    "\n",
    "s0 = 0  # Specific starting state\n",
    "avg_G_conditional = get_conditional_G(num_trajectories, T, M, pi_opt, s0)\n",
    "print(f\"Conditional Average G for s0={s0}: {avg_G_conditional}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
