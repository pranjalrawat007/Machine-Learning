{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 01000 | Loss(SVF): 0.0076 | GradNorm: 0.0076 | PolDiff: 0.9582 | ValDiff: 11.8977 | RewDiff: 1.5524\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3938 0.3171 0.2891]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5041 16.6004 16.0799]\n",
      "    s=1, Q=[16.6887 17.1129 17.2093]\n",
      "    s=2, Q=[17.0966 16.576  17.0002]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.3629 0.3996 0.2375]\n",
      "    s=1, π=[0.2375 0.3629 0.3996]\n",
      "    s=2, π=[0.3996 0.2375 0.3629]\n",
      "  Reward: [0.2974 0.9063 0.7935]\n",
      "\n",
      "Iter 02000 | Loss(SVF): 0.0008 | GradNorm: 0.0008 | PolDiff: 0.9798 | ValDiff: 11.9721 | RewDiff: 1.5823\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3911 0.3227 0.2863]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5035 16.6286 16.0745]\n",
      "    s=1, Q=[16.7225 17.1515 17.2766]\n",
      "    s=2, Q=[17.1303 16.5762 17.0052]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.3591 0.407  0.2339]\n",
      "    s=1, π=[0.2339 0.3591 0.407 ]\n",
      "    s=2, π=[0.407  0.2339 0.3591]\n",
      "  Reward: [0.2825 0.9305 0.7842]\n",
      "\n",
      "Iter 03000 | Loss(SVF): 0.0001 | GradNorm: 0.0001 | PolDiff: 0.9832 | ValDiff: 11.9831 | RewDiff: 1.5872\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3906 0.3232 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5043 16.6316 16.0732]\n",
      "    s=1, Q=[16.7263 17.1573 17.2846]\n",
      "    s=2, Q=[17.1357 16.5774 17.0084]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4077 0.2333]\n",
      "    s=1, π=[0.2333 0.359  0.4077]\n",
      "    s=2, π=[0.4077 0.2333 0.359 ]\n",
      "  Reward: [0.28   0.9331 0.7841]\n",
      "\n",
      "Iter 04000 | Loss(SVF): 0.0000 | GradNorm: 0.0000 | PolDiff: 0.9838 | ValDiff: 11.9849 | RewDiff: 1.5880\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3906 0.3233 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5045 16.6319 16.073 ]\n",
      "    s=1, Q=[16.7267 17.1582 17.2857]\n",
      "    s=2, Q=[17.1366 16.5776 17.0091]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4078 0.2332]\n",
      "    s=1, π=[0.2332 0.359  0.4078]\n",
      "    s=2, π=[0.4078 0.2332 0.359 ]\n",
      "  Reward: [0.2796 0.9334 0.7843]\n",
      "\n",
      "Iter 05000 | Loss(SVF): 0.0000 | GradNorm: 0.0000 | PolDiff: 0.9839 | ValDiff: 11.9851 | RewDiff: 1.5882\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3905 0.3233 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5045 16.632  16.0729]\n",
      "    s=1, Q=[16.7268 17.1584 17.2859]\n",
      "    s=2, Q=[17.1367 16.5777 17.0093]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4078 0.2332]\n",
      "    s=1, π=[0.2332 0.359  0.4078]\n",
      "    s=2, π=[0.4078 0.2332 0.359 ]\n",
      "  Reward: [0.2795 0.9334 0.7843]\n",
      "\n",
      "Iter 06000 | Loss(SVF): 0.0000 | GradNorm: 0.0000 | PolDiff: 0.9839 | ValDiff: 11.9852 | RewDiff: 1.5882\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3905 0.3233 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5045 16.632  16.0729]\n",
      "    s=1, Q=[16.7268 17.1584 17.2859]\n",
      "    s=2, Q=[17.1368 16.5777 17.0093]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4078 0.2332]\n",
      "    s=1, π=[0.2332 0.359  0.4078]\n",
      "    s=2, π=[0.4078 0.2332 0.359 ]\n",
      "  Reward: [0.2795 0.9334 0.7843]\n",
      "\n",
      "Iter 07000 | Loss(SVF): 0.0000 | GradNorm: 0.0000 | PolDiff: 0.9839 | ValDiff: 11.9852 | RewDiff: 1.5882\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3905 0.3233 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5045 16.632  16.0729]\n",
      "    s=1, Q=[16.7268 17.1584 17.2859]\n",
      "    s=2, Q=[17.1368 16.5777 17.0093]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4078 0.2332]\n",
      "    s=1, π=[0.2332 0.359  0.4078]\n",
      "    s=2, π=[0.4078 0.2332 0.359 ]\n",
      "  Reward: [0.2795 0.9334 0.7843]\n",
      "\n",
      "Iter 08000 | Loss(SVF): 0.0000 | GradNorm: 0.0000 | PolDiff: 0.9839 | ValDiff: 11.9852 | RewDiff: 1.5882\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3905 0.3233 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5045 16.632  16.0729]\n",
      "    s=1, Q=[16.7268 17.1584 17.2859]\n",
      "    s=2, Q=[17.1368 16.5777 17.0093]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4078 0.2332]\n",
      "    s=1, π=[0.2332 0.359  0.4078]\n",
      "    s=2, π=[0.4078 0.2332 0.359 ]\n",
      "  Reward: [0.2795 0.9334 0.7843]\n",
      "\n",
      "Iter 09000 | Loss(SVF): 0.0000 | GradNorm: 0.0000 | PolDiff: 0.9839 | ValDiff: 11.9852 | RewDiff: 1.5882\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3905 0.3233 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5045 16.632  16.0729]\n",
      "    s=1, Q=[16.7268 17.1584 17.2859]\n",
      "    s=2, Q=[17.1368 16.5777 17.0093]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4078 0.2332]\n",
      "    s=1, π=[0.2332 0.359  0.4078]\n",
      "    s=2, π=[0.4078 0.2332 0.359 ]\n",
      "  Reward: [0.2795 0.9334 0.7843]\n",
      "\n",
      "Iter 10000 | Loss(SVF): 0.0000 | GradNorm: 0.0000 | PolDiff: 0.9839 | ValDiff: 11.9852 | RewDiff: 1.5882\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3905 0.3233 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5045 16.632  16.0729]\n",
      "    s=1, Q=[16.7268 17.1584 17.2859]\n",
      "    s=2, Q=[17.1368 16.5777 17.0093]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4078 0.2332]\n",
      "    s=1, π=[0.2332 0.359  0.4078]\n",
      "    s=2, π=[0.4078 0.2332 0.359 ]\n",
      "  Reward: [0.2795 0.9334 0.7843]\n",
      "\n",
      "\n",
      "Final Results:\n",
      "True Rewards:       [0.5  0.25 0.1 ]\n",
      "Estimated Rewards:  [0.2795 0.9334 0.7843]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Triangular MDP Setup\n",
    "# ---------------------------\n",
    "# States: 0,1,2 (think of them arranged in a triangle)\n",
    "# Actions: 0=left, 1=right, 2=stay\n",
    "# We'll define transitions with a small noise probability eps.\n",
    "eps = 0.05\n",
    "n_states = 3\n",
    "n_actions = 3\n",
    "gamma = 0.9\n",
    "\n",
    "def build_transition_matrix(eps=0.05):\n",
    "    \"\"\"\n",
    "    Triangular 3-state MDP with 3 actions.\n",
    "    P[(s, a)] = probability distribution over next states [p0, p1, p2].\n",
    "    \n",
    "    Action 0 = 'left' \n",
    "    Action 1 = 'right'\n",
    "    Action 2 = 'stay'\n",
    "    \"\"\"\n",
    "    # Initialize dictionary\n",
    "    P = {}\n",
    "    # Helper to set distribution with main target and small noise to others\n",
    "    def dist(target):\n",
    "        # With prob (1 - eps) go to the target, with prob eps the rest is evenly distributed\n",
    "        d = np.ones(n_states) * (eps / (n_states))\n",
    "        d[target] += (1 - eps)\n",
    "        return d\n",
    "    \n",
    "    # For each state s, define transitions for each action\n",
    "    for s in range(n_states):\n",
    "        # Action 'left'\n",
    "        if s == 0:\n",
    "            P[(0, 0)] = dist(target=2)  # from 0, left goes to 2 ideally\n",
    "        elif s == 1:\n",
    "            P[(1, 0)] = dist(target=0)  # from 1, left goes to 0\n",
    "        else: # s==2\n",
    "            P[(2, 0)] = dist(target=1)  # from 2, left goes to 1\n",
    "        \n",
    "        # Action 'right'\n",
    "        if s == 0:\n",
    "            P[(0, 1)] = dist(target=1)  # from 0, right -> 1\n",
    "        elif s == 1:\n",
    "            P[(1, 1)] = dist(target=2)  # from 1, right -> 2\n",
    "        else: # s==2\n",
    "            P[(2, 1)] = dist(target=0)  # from 2, right -> 0\n",
    "        \n",
    "        # Action 'stay'\n",
    "        P[(s, 2)] = dist(target=s)     # from s, stay in s\n",
    "    return P\n",
    "\n",
    "# Construct the transitions\n",
    "P = build_transition_matrix(eps)\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Features & True Rewards\n",
    "# ---------------------------\n",
    "# We'll use one-hot features for each state: phi(s) = e_s\n",
    "features = np.eye(n_states)\n",
    "\n",
    "# Set a \"true\" reward for demonstration. \n",
    "# The user can choose any arbitrary vector.\n",
    "true_rewards = np.array([0.5, 0.25, 0.10])\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Soft Value Iteration\n",
    "# ---------------------------\n",
    "def soft_value_iteration(reward, P, tol=1e-6, max_iter=200):\n",
    "    \"\"\"\n",
    "    V(s) <- log sum_a exp( R(s) + gamma * sum_{s'} P(s'|s,a)*V(s') )\n",
    "    \"\"\"\n",
    "    V = np.zeros(n_states)\n",
    "    for _ in range(max_iter):\n",
    "        V_prev = V.copy()\n",
    "        for s in range(n_states):\n",
    "            Q_sa = []\n",
    "            for a in range(n_actions):\n",
    "                Q_sa.append(reward[s] + gamma * np.dot(P[(s,a)], V_prev))\n",
    "            # log-sum-exp\n",
    "            V[s] = np.log(np.sum(np.exp(Q_sa)))\n",
    "        if np.max(np.abs(V - V_prev)) < tol:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Compute Policy\n",
    "# ---------------------------\n",
    "def compute_policy(V, reward, P):\n",
    "    \"\"\"\n",
    "    pi(a|s) = exp(Q(s,a)) / sum_{a'} exp(Q(s,a'))\n",
    "    Q(s,a) = R(s) + gamma * sum_{s'} P(s'|s,a]*V(s')\n",
    "    \"\"\"\n",
    "    policy = np.zeros((n_states, n_actions))\n",
    "    Q_values = np.zeros((n_states, n_actions))  # for debug printing\n",
    "    for s in range(n_states):\n",
    "        Q_sa = []\n",
    "        for a in range(n_actions):\n",
    "            q = reward[s] + gamma * np.dot(P[(s,a)], V)\n",
    "            Q_sa.append(q)\n",
    "        Q_sa = np.array(Q_sa)\n",
    "        # For debugging\n",
    "        Q_values[s] = Q_sa\n",
    "        # Stable softmax\n",
    "        shift = Q_sa - np.max(Q_sa)\n",
    "        policy[s] = np.exp(shift) / np.sum(np.exp(shift))\n",
    "    return policy, Q_values\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Compute State Visitation Frequencies\n",
    "# ---------------------------\n",
    "def compute_svf(policy, P, start_state=0, trajectory_length=5):\n",
    "    \"\"\"\n",
    "    Accumulate visitation frequencies over 'trajectory_length' steps,\n",
    "    starting with a single initial state (prob=1).\n",
    "    \"\"\"\n",
    "    d_t = np.zeros(n_states)\n",
    "    d_t[start_state] = 1.0\n",
    "\n",
    "    svf = np.zeros(n_states)\n",
    "    for _ in range(trajectory_length):\n",
    "        svf += d_t\n",
    "        next_d_t = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                next_d_t += d_t[s] * policy[s, a] * P[(s,a)]\n",
    "        d_t = next_d_t\n",
    "    return svf\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Generate \"Expert\" Trajectories\n",
    "# ---------------------------\n",
    "def generate_soft_optimal_trajectories(policy, P, n_trajectories=100, trajectory_length=5):\n",
    "    \"\"\"\n",
    "    Sample states by following 'policy'. \n",
    "    Each trajectory has length 'trajectory_length'.\n",
    "    \"\"\"\n",
    "    trajectories = []\n",
    "    for _ in range(n_trajectories):\n",
    "        traj = []\n",
    "        # Start from a random state\n",
    "        state = np.random.choice(n_states)\n",
    "        for _ in range(trajectory_length):\n",
    "            action = np.random.choice(n_actions, p=policy[state])\n",
    "            next_state = np.random.choice(n_states, p=P[(state, action)])\n",
    "            traj.append(state)\n",
    "            state = next_state\n",
    "        trajectories.append(traj)\n",
    "    return trajectories\n",
    "\n",
    "# ---------------------------\n",
    "# 7) MaxEnt IRL\n",
    "# ---------------------------\n",
    "def maxent_irl(features, expert_trajectories,\n",
    "               P,  # transition model\n",
    "               true_rewards=None, true_policy=None, true_value=None,\n",
    "               gamma=0.9, lr=0.01, n_iters=10000, print_every=1000):\n",
    "    \"\"\"\n",
    "    Gradient-based MaxEnt IRL: R(s) = theta^T phi(s).\n",
    "    Additional debugging prints added.\n",
    "    \"\"\"\n",
    "    n_states, d_features = features.shape\n",
    "    # Initialize random weights\n",
    "    reward_weights = np.random.uniform(size=d_features)\n",
    "\n",
    "    # Compute expert state visitation frequency (normalized)\n",
    "    expert_svf = np.zeros(n_states)\n",
    "    total_steps = 0\n",
    "    for traj in expert_trajectories:\n",
    "        for s in traj:\n",
    "            expert_svf[s] += 1\n",
    "        total_steps += len(traj)\n",
    "    expert_svf /= total_steps\n",
    "\n",
    "    for it in range(n_iters):\n",
    "        # 1) Current reward\n",
    "        reward_est = features @ reward_weights\n",
    "        \n",
    "        # 2) Soft Value Iteration\n",
    "        V_est = soft_value_iteration(reward_est, P)\n",
    "        \n",
    "        # 3) Compute policy & Q-values\n",
    "        policy_est, Q_values = compute_policy(V_est, reward_est, P)\n",
    "        \n",
    "        # 4) Predicted SVF\n",
    "        svf_est = compute_svf(policy_est, P, start_state=0, trajectory_length=5)\n",
    "        svf_est /= np.sum(svf_est)\n",
    "\n",
    "        # 5) Gradient step\n",
    "        grad = expert_svf - svf_est\n",
    "        reward_weights += lr * features.T @ grad\n",
    "\n",
    "        # 6) Debug prints\n",
    "        if (it+1) % print_every == 0:\n",
    "            loss_svf = np.linalg.norm(expert_svf - svf_est)  # L2\n",
    "            grad_norm = np.linalg.norm(grad)\n",
    "            msg = f\"Iter {it+1:05d} | Loss(SVF): {loss_svf:.4f} | GradNorm: {grad_norm:.4f}\"\n",
    "            \n",
    "            if true_policy is not None:\n",
    "                # L1 difference across all states, all actions\n",
    "                pol_diff = np.sum(np.abs(policy_est - true_policy))\n",
    "                msg += f\" | PolDiff: {pol_diff:.4f}\"\n",
    "            if true_value is not None:\n",
    "                # L1 difference in value\n",
    "                val_diff = np.sum(np.abs(V_est - true_value))\n",
    "                msg += f\" | ValDiff: {val_diff:.4f}\"\n",
    "            if true_rewards is not None:\n",
    "                # L1 difference in reward vector\n",
    "                rew_diff = np.sum(np.abs(reward_est - true_rewards))\n",
    "                msg += f\" | RewDiff: {rew_diff:.4f}\"\n",
    "\n",
    "            print(msg)\n",
    "            # Print SVF side by side\n",
    "            print(f\"  Expert SVF: {expert_svf}\")\n",
    "            print(f\"  Pred   SVF: {svf_est}\")\n",
    "            # Print Q-values\n",
    "            print(\"  Q-values (s x a):\")\n",
    "            for s in range(n_states):\n",
    "                print(f\"    s={s}, Q={Q_values[s]}\")\n",
    "            # Print policy\n",
    "            print(\"  Policy (s x a):\")\n",
    "            for s in range(n_states):\n",
    "                print(f\"    s={s}, π={policy_est[s]}\")\n",
    "            # Print reward\n",
    "            print(f\"  Reward: {reward_est}\")\n",
    "            print(\"\")\n",
    "\n",
    "    return features @ reward_weights\n",
    "\n",
    "# ---------------------------\n",
    "# 8) Main Demo\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(0)  # for reproducibility, if desired\n",
    "\n",
    "    # 1) Compute the \"true\" V, policy from the known reward\n",
    "    V_true = soft_value_iteration(true_rewards, P)\n",
    "    policy_true, _ = compute_policy(V_true, true_rewards, P)\n",
    "\n",
    "    # 2) Generate \"expert\" data from the \"true\" policy\n",
    "    n_sample_trajectories = 500\n",
    "    expert_trajectories = generate_soft_optimal_trajectories(\n",
    "        policy_true, P, n_trajectories=n_sample_trajectories, trajectory_length=100\n",
    "    )\n",
    "\n",
    "    # 3) Run MaxEnt IRL with extra prints\n",
    "    estimated_rewards = maxent_irl(\n",
    "        features,\n",
    "        expert_trajectories,\n",
    "        P,\n",
    "        true_rewards=true_rewards,\n",
    "        true_policy=policy_true,\n",
    "        true_value=V_true,\n",
    "        lr=0.01,\n",
    "        n_iters=10000,\n",
    "        print_every=1000\n",
    "    )\n",
    "\n",
    "    # Final results\n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(\"True Rewards:      \", true_rewards)\n",
    "    print(\"Estimated Rewards: \", estimated_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAXENT + NXFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Base Experiment Info ===\n",
      "States=3, Actions=3, eps=0.05, gamma=0.9\n",
      "True Rewards: [0.   0.25 0.9 ]\n",
      "NumTraj=500, TrajLen=50\n",
      "\n",
      "\n",
      "=== MaxEnt IRL Results ===\n",
      "AnchorMode        reg_lambda     R(0)    R(1)    R(2)    RewDiff    PolDiff    GradNorm    ValDiff    SvfDiff    LogLik    TimeSec\n",
      "FixNone                 0     -2.1748  0.7848  1.3909     3.2005     1.1914      0.0021     5.0764     0.0035  -33382.4    46.1096\n",
      "FixNone                 0.05  -1.1384  0.2913  0.8471     1.2327     0.7558      0.0724     6.6523     0.1138  -27773.2    45.9838\n",
      "FixNone                 0.1   -0.8596  0.1807  0.6789     1.15       0.5266      0.111      9.0881     0.1719  -26822      47.1638\n",
      "FixNone                 0.2   -0.6029  0.0991  0.5038     1.15       0.3358      0.1584    10.9757     0.2412  -26329.4    46.0111\n",
      "FixNone                 0.7   -0.2574  0.0291  0.2284     1.15       0.549       0.2417    12.7077     0.3604  -26512      45.9233\n",
      "FixFirst                0      0       2.3865  3.0045     4.241      1.0886      0.0181    54.1033     0.0295  -31088.2    50.2889\n",
      "FixFirst                0.05   0       0.7542  1.3466     0.9508     0.3758      0.1304    11.0431     0.2101  -26433.7    46.5188\n",
      "FixFirst                0.1    0       0.4356  0.9669     0.2525     0.1484      0.1758     2.601      0.2805  -26146.9    46.2782\n",
      "FixFirst                0.2    0       0.219   0.6443     0.2867     0.3044      0.2199     3.7022     0.3453  -26231.2    46.5925\n",
      "FixFirst                0.7    0       0.0498  0.2519     0.8482     0.6794      0.2774     9.9866     0.4225  -26795.7    45.7001\n",
      "FixFirstSecond          0      0       0.25    1.4644     0.5644     0.713       0.1588     8.7931     0.2246  -26880.3    46.2594\n",
      "FixFirstSecond          0.05   0       0.25    1.1135     0.2135     0.2734      0.169      3.123      0.2744  -26236      46.0972\n",
      "FixFirstSecond          0.1    0       0.25    0.9012     0.0012     0.0015      0.1879     0.0165     0.3051  -26116.4    45.8858\n",
      "FixFirstSecond          0.2    0       0.25    0.6519     0.2481     0.3163      0.2184     3.3161     0.3411  -26238.3    45.6473\n",
      "FixFirstSecond          0.7    0       0.25    0.2709     0.6291     0.784       0.2735     7.7663     0.394   -26960.6    45.2854\n",
      "\n",
      "=== NxFP IRL Results ===\n",
      "AnchorMode        reg_lambda     R(0)     R(1)    R(2)    RewDiff    PolDiff    GradNorm    ValDiff    SvfDiff    LogLik    TimeSec\n",
      "FixNone                 0     -0.4355  -0.163   0.4609     1.2877     0.026       0.1877    12.92       0.304   -26115.1     0.665\n",
      "FixNone                 0.05  -0.4355  -0.163   0.461      1.2876     0.026       0.1877    12.9184     0.3039  -26115.1     0.7188\n",
      "FixNone                 0.1   -0.4354  -0.163   0.4608     1.2876     0.0261      0.1877    12.9191     0.304   -26115.1     0.9557\n",
      "FixNone                 0.2   -0.4349  -0.1624  0.4615     1.2858     0.026       0.1877    12.9011     0.3039  -26115.1     0.7763\n",
      "FixNone                 0.7   -0.4355  -0.1632  0.4608     1.2879     0.0259      0.1877    12.9217     0.304   -26115.1     0.4186\n",
      "FixFirst                0      0        0.2725  0.8964     0.0261     0.026       0.1877     0.146      0.304   -26115.1     0.6277\n",
      "FixFirst                0.05   0        0.2725  0.8964     0.026      0.026       0.1877     0.1464     0.3039  -26115.1     0.5368\n",
      "FixFirst                0.1    0        0.2723  0.8963     0.026      0.0259      0.1877     0.1425     0.304   -26115.1     2.0814\n",
      "FixFirst                0.2    0        0.2725  0.8964     0.026      0.026       0.1877     0.1459     0.304   -26115.1     0.4968\n",
      "FixFirst                0.7    0        0.2721  0.8957     0.0264     0.0261      0.1878     0.133      0.3041  -26115.1     0.5422\n",
      "FixFirstSecond          0      0        0.25    0.884      0.016      0.0205      0.1897     0.2243     0.3076  -26115.7     0.1808\n",
      "FixFirstSecond          0.05   0        0.25    0.8837     0.0163     0.0209      0.1898     0.228      0.3077  -26115.7     0.1502\n",
      "FixFirstSecond          0.1    0        0.25    0.884      0.016      0.0205      0.1897     0.2239     0.3076  -26115.7     0.4204\n",
      "FixFirstSecond          0.2    0        0.25    0.8837     0.0163     0.0208      0.1898     0.2276     0.3077  -26115.7     0.599\n",
      "FixFirstSecond          0.7    0        0.25    0.8837     0.0163     0.0209      0.1898     0.2282     0.3077  -26115.7     0.2382\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "from scipy.special import logsumexp\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Shared Setup\n",
    "# -------------------------------------------------\n",
    "eps = 0.05\n",
    "n_states = 3\n",
    "n_actions = 3\n",
    "gamma = 0.9\n",
    "\n",
    "def build_transition_matrix(eps=0.05):\n",
    "    P = {}\n",
    "    def dist(target):\n",
    "        d = np.ones(n_states) * (eps / n_states)\n",
    "        d[target] += (1 - eps)\n",
    "        return d\n",
    "    for s in range(n_states):\n",
    "        if s == 0:\n",
    "            P[(0,0)] = dist(target=2)\n",
    "            P[(0,1)] = dist(target=1)\n",
    "            P[(0,2)] = dist(target=0)\n",
    "        elif s == 1:\n",
    "            P[(1,0)] = dist(target=0)\n",
    "            P[(1,1)] = dist(target=2)\n",
    "            P[(1,2)] = dist(target=1)\n",
    "        else:  # s=2\n",
    "            P[(2,0)] = dist(target=1)\n",
    "            P[(2,1)] = dist(target=0)\n",
    "            P[(2,2)] = dist(target=2)\n",
    "    return P\n",
    "\n",
    "P = build_transition_matrix(eps)\n",
    "features = np.eye(n_states)\n",
    "true_rewards = np.array([0.0, 0.25, 0.9])\n",
    "\n",
    "def soft_value_iteration(reward, P, tol=1e-6, max_iter=200):\n",
    "    V = np.zeros(n_states)\n",
    "    for _ in range(max_iter):\n",
    "        V_prev = V.copy()\n",
    "        for s in range(n_states):\n",
    "            Q_sa = [reward[s] + gamma*np.dot(P[(s,a)], V_prev) for a in range(n_actions)]\n",
    "            V[s] = logsumexp(Q_sa)\n",
    "        if np.max(np.abs(V - V_prev)) < tol:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def compute_policy(V, reward, P):\n",
    "    policy = np.zeros((n_states, n_actions))\n",
    "    Q_values = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        Q_sa = [reward[s] + gamma*np.dot(P[(s,a)], V) for a in range(n_actions)]\n",
    "        Q_values[s] = Q_sa\n",
    "        shift = Q_sa - np.max(Q_sa)\n",
    "        policy[s] = np.exp(shift) / np.sum(np.exp(shift))\n",
    "    return policy, Q_values\n",
    "\n",
    "def generate_soft_optimal_trajectories(policy, P, n_trajectories=100, trajectory_length=5):\n",
    "    trajectories = []\n",
    "    for _ in range(n_trajectories):\n",
    "        traj, state = [], np.random.choice(n_states)\n",
    "        for __ in range(trajectory_length):\n",
    "            a = np.random.choice(n_actions, p=policy[state])\n",
    "            s_next = np.random.choice(n_states, p=P[(state,a)])\n",
    "            traj.append((state, a, s_next))\n",
    "            state = s_next\n",
    "        trajectories.append(traj)\n",
    "    return trajectories\n",
    "\n",
    "def compute_svf(policy, P, start_state=0, trajectory_length=5):\n",
    "    d_t = np.zeros(n_states)\n",
    "    d_t[start_state] = 1.0\n",
    "    svf = np.zeros(n_states)\n",
    "    for _ in range(trajectory_length):\n",
    "        svf += d_t\n",
    "        nxt = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                nxt += d_t[s] * policy[s,a] * P[(s,a)]\n",
    "        d_t = nxt\n",
    "    return svf\n",
    "\n",
    "def compute_expert_log_likelihood(reward, expert_trajectories, P, use_transitions=True):\n",
    "    V_est = soft_value_iteration(reward, P)\n",
    "    policy_est, _ = compute_policy(V_est, reward, P)\n",
    "    total_loglik = 0.0\n",
    "    eps_small = 1e-12\n",
    "    for traj in expert_trajectories:\n",
    "        for (s, a, s_next) in traj:\n",
    "            p_a = policy_est[s,a]\n",
    "            total_loglik += np.log(p_a + eps_small)\n",
    "            if use_transitions:\n",
    "                total_loglik += np.log(P[(s,a)][s_next] + eps_small)\n",
    "    return total_loglik\n",
    "\n",
    "# -------------------------------------------------\n",
    "# MaxEnt IRL\n",
    "# -------------------------------------------------\n",
    "def maxent_irl(features, expert_trajectories, P, anchor_mode=0, anchor_values=None,\n",
    "               reg_lambda=0.0, gamma=0.9, lr=0.01, n_iters=10000, print_every=1000,\n",
    "               verbose=True, true_rewards=None):\n",
    "    n_states, d_features = features.shape\n",
    "    w = np.random.uniform(-1e-3, 1e-3, d_features)\n",
    "    if anchor_mode >= 1:\n",
    "        w[0] = anchor_values[0]\n",
    "    if anchor_mode == 2:\n",
    "        w[1] = anchor_values[1]\n",
    "\n",
    "    # Expert SVF\n",
    "    expert_svf = np.zeros(n_states)\n",
    "    tot_steps = 0\n",
    "    for traj in expert_trajectories:\n",
    "        for (s,a,s_next) in traj:\n",
    "            expert_svf[s] += 1\n",
    "        tot_steps += len(traj)\n",
    "    expert_svf /= tot_steps\n",
    "\n",
    "    for it in range(n_iters):\n",
    "        reward_est = features @ w\n",
    "        V_est = soft_value_iteration(reward_est, P)\n",
    "        policy_est, _ = compute_policy(V_est, reward_est, P)\n",
    "        svf_est = compute_svf(policy_est, P, 0, 5)\n",
    "        svf_est /= np.sum(svf_est)\n",
    "\n",
    "        grad_main = expert_svf - svf_est\n",
    "        grad_w = features.T @ grad_main\n",
    "        if reg_lambda > 0:\n",
    "            grad_w -= reg_lambda * w\n",
    "        w += lr*grad_w\n",
    "\n",
    "        if anchor_mode >= 1:\n",
    "            w[0] = anchor_values[0]\n",
    "        if anchor_mode == 2:\n",
    "            w[1] = anchor_values[1]\n",
    "\n",
    "        if verbose and (it+1) % print_every == 0:\n",
    "            pass  # optional prints\n",
    "\n",
    "    return w\n",
    "\n",
    "# -------------------------------------------------\n",
    "# NxFP IRL\n",
    "# -------------------------------------------------\n",
    "def nfxp_objective(params, anchor_mode, anchor_values, reg_lambda, expert_trajectories):\n",
    "    if anchor_mode == 0:\n",
    "        r0, r1, r2 = params\n",
    "    elif anchor_mode == 1:\n",
    "        r0 = anchor_values[0]\n",
    "        r1, r2 = params\n",
    "    else:\n",
    "        r0, r1 = anchor_values\n",
    "        (r2,) = params\n",
    "    reward = np.array([r0, r1, r2])\n",
    "\n",
    "    V_est = soft_value_iteration(reward, P)\n",
    "    policy_est, _ = compute_policy(V_est, reward, P)\n",
    "    nll = 0.0\n",
    "    eps = 1e-12\n",
    "    for traj in expert_trajectories:\n",
    "        for (s,a,_) in traj:\n",
    "            p_a = policy_est[s,a]\n",
    "            nll += -np.log(p_a + eps)\n",
    "    penalty = reg_lambda*np.sum(params**2)\n",
    "    return nll + penalty\n",
    "\n",
    "def estimate_nfxp(expert_trajectories, anchor_mode, anchor_values, reg_lambda):\n",
    "    if anchor_mode == 0:\n",
    "        x0 = np.array([0.1, 0.1, 0.1])\n",
    "        bnds = [(-2,2)]*3\n",
    "    elif anchor_mode == 1:\n",
    "        x0 = np.array([0.1, 0.1])\n",
    "        bnds = [(-2,2)]*2\n",
    "    else:\n",
    "        x0 = np.array([0.1])\n",
    "        bnds = [(-2,2)]\n",
    "\n",
    "    def wrapper(x):\n",
    "        return nfxp_objective(x, anchor_mode, anchor_values, reg_lambda, expert_trajectories)\n",
    "\n",
    "    res = minimize(wrapper, x0, method='L-BFGS-B', bounds=bnds,\n",
    "                   options={'maxiter':300, 'disp':False})\n",
    "\n",
    "    if anchor_mode == 0:\n",
    "        r0, r1, r2 = res.x\n",
    "    elif anchor_mode == 1:\n",
    "        r0 = anchor_values[0]\n",
    "        r1, r2 = res.x\n",
    "    else:\n",
    "        r0, r1 = anchor_values\n",
    "        (r2,) = res.x\n",
    "    return np.array([r0, r1, r2]), res.fun\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Unified Demo\n",
    "# -------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(0)\n",
    "    print(\"=== Base Experiment Info ===\")\n",
    "    print(f\"States={n_states}, Actions={n_actions}, eps={eps}, gamma={gamma}\")\n",
    "    print(\"True Rewards:\", true_rewards)\n",
    "    n_sample_trajectories = 500\n",
    "    trajectory_length = 50\n",
    "    print(f\"NumTraj={n_sample_trajectories}, TrajLen={trajectory_length}\\n\")\n",
    "\n",
    "    # Generate expert data\n",
    "    V_true = soft_value_iteration(true_rewards, P)\n",
    "    policy_true, _ = compute_policy(V_true, true_rewards, P)\n",
    "    expert_trajectories = generate_soft_optimal_trajectories(\n",
    "        policy_true, P, n_trajectories=n_sample_trajectories, trajectory_length=trajectory_length\n",
    "    )\n",
    "\n",
    "    # Precompute an expert_svf_raw for scoring\n",
    "    expert_svf_raw = np.zeros(n_states)\n",
    "    tot_steps = 0\n",
    "    for traj in expert_trajectories:\n",
    "        for (s, a, _) in traj:\n",
    "            expert_svf_raw[s] += 1\n",
    "        tot_steps += len(traj)\n",
    "    expert_svf_raw /= tot_steps\n",
    "\n",
    "    anchor_modes = [0, 1, 2]\n",
    "    reg_lambdas = [0.0, 0.05, 0.1, 0.2, 0.7]\n",
    "\n",
    "    # Run both MaxEnt IRL and NxFP IRL\n",
    "    results_maxent = {}\n",
    "    results_nfxp = {}\n",
    "\n",
    "    # --- MaxEnt IRL ---\n",
    "    for am in anchor_modes:\n",
    "        if am == 0:\n",
    "            anc_vals = None\n",
    "            am_str = \"FixNone\"\n",
    "        elif am == 1:\n",
    "            anc_vals = [0.0]\n",
    "            am_str = \"FixFirst\"\n",
    "        else:\n",
    "            anc_vals = [0.0, 0.25]\n",
    "            am_str = \"FixFirstSecond\"\n",
    "\n",
    "        for rl in reg_lambdas:\n",
    "            start_t = time.time()\n",
    "            w = maxent_irl(features, expert_trajectories, P, anchor_mode=am,\n",
    "                           anchor_values=anc_vals, reg_lambda=rl, verbose=False)\n",
    "            end_t = time.time()\n",
    "\n",
    "            est_rew = features @ w\n",
    "            V_est = soft_value_iteration(est_rew, P)\n",
    "            policy_est, _ = compute_policy(V_est, est_rew, P)\n",
    "\n",
    "            pol_diff = np.sum(np.abs(policy_est - policy_true))\n",
    "            rew_diff = np.sum(np.abs(est_rew - true_rewards))\n",
    "            svf_est = compute_svf(policy_est, P, 0, 5)\n",
    "            svf_est /= np.sum(svf_est)\n",
    "            svf_diff = np.sum(np.abs(svf_est - expert_svf_raw))\n",
    "            grad_norm = np.linalg.norm(expert_svf_raw - svf_est)\n",
    "            val_diff = np.sum(np.abs(V_est - V_true))\n",
    "            loglik = compute_expert_log_likelihood(est_rew, expert_trajectories, P, use_transitions=False)\n",
    "            tsec = end_t - start_t\n",
    "\n",
    "            results_maxent[(am_str, rl)] = {\n",
    "                'R': est_rew,\n",
    "                'RewDiff': rew_diff,\n",
    "                'PolDiff': pol_diff,\n",
    "                'GradNorm': grad_norm,\n",
    "                'ValDiff': val_diff,\n",
    "                'SvfDiff': svf_diff,\n",
    "                'LogLik': loglik,\n",
    "                'TimeSec': tsec\n",
    "            }\n",
    "\n",
    "    # --- NxFP IRL ---\n",
    "    for am in anchor_modes:\n",
    "        if am == 0:\n",
    "            anc_vals = None\n",
    "            am_str = \"FixNone\"\n",
    "        elif am == 1:\n",
    "            anc_vals = [0.0]\n",
    "            am_str = \"FixFirst\"\n",
    "        else:\n",
    "            anc_vals = [0.0, 0.25]\n",
    "            am_str = \"FixFirstSecond\"\n",
    "\n",
    "        for rl in reg_lambdas:\n",
    "            start_t = time.time()\n",
    "            est_rew, _ = estimate_nfxp(expert_trajectories, am, anc_vals, rl)\n",
    "            end_t = time.time()\n",
    "\n",
    "            V_est = soft_value_iteration(est_rew, P)\n",
    "            policy_est, _ = compute_policy(V_est, est_rew, P)\n",
    "\n",
    "            pol_diff = np.sum(np.abs(policy_est - policy_true))\n",
    "            rew_diff = np.sum(np.abs(est_rew - true_rewards))\n",
    "            svf_est = compute_svf(policy_est, P, 0, 5)\n",
    "            svf_est /= np.sum(svf_est)\n",
    "            svf_diff = np.sum(np.abs(svf_est - expert_svf_raw))\n",
    "            grad_norm = np.linalg.norm(expert_svf_raw - svf_est)\n",
    "            val_diff = np.sum(np.abs(V_est - V_true))\n",
    "            loglik = compute_expert_log_likelihood(est_rew, expert_trajectories, P, use_transitions=False)\n",
    "            tsec = end_t - start_t\n",
    "\n",
    "            results_nfxp[(am_str, rl)] = {\n",
    "                'R': est_rew,\n",
    "                'RewDiff': rew_diff,\n",
    "                'PolDiff': pol_diff,\n",
    "                'GradNorm': grad_norm,\n",
    "                'ValDiff': val_diff,\n",
    "                'SvfDiff': svf_diff,\n",
    "                'LogLik': loglik,\n",
    "                'TimeSec': tsec\n",
    "            }\n",
    "\n",
    "    # Print tables\n",
    "    hdrs = [\"AnchorMode\",\"reg_lambda\",\"R(0)\",\"R(1)\",\"R(2)\",\"RewDiff\",\"PolDiff\",\"GradNorm\",\"ValDiff\",\"SvfDiff\",\"LogLik\",\"TimeSec\"]\n",
    "\n",
    "    def build_rows(res):\n",
    "        rows = []\n",
    "        for am in anchor_modes:\n",
    "            if am == 0:\n",
    "                am_str = \"FixNone\"\n",
    "            elif am == 1:\n",
    "                am_str = \"FixFirst\"\n",
    "            else:\n",
    "                am_str = \"FixFirstSecond\"\n",
    "            for rl in reg_lambdas:\n",
    "                vals = res[(am_str, rl)]\n",
    "                r0, r1, r2 = vals['R']\n",
    "                row = [\n",
    "                    am_str, f\"{rl:.2f}\", f\"{r0:.4f}\", f\"{r1:.4f}\", f\"{r2:.4f}\",\n",
    "                    f\"{vals['RewDiff']:.4f}\", f\"{vals['PolDiff']:.4f}\",\n",
    "                    f\"{vals['GradNorm']:.4f}\", f\"{vals['ValDiff']:.4f}\",\n",
    "                    f\"{vals['SvfDiff']:.4f}\", f\"{vals['LogLik']:.4f}\",\n",
    "                    f\"{vals['TimeSec']:.4f}\"\n",
    "                ]\n",
    "                rows.append(row)\n",
    "        return rows\n",
    "\n",
    "    print(\"\\n=== MaxEnt IRL Results ===\")\n",
    "    rows_maxent = build_rows(results_maxent)\n",
    "    print(tabulate(rows_maxent, headers=hdrs, tablefmt=\"plain\"))\n",
    "\n",
    "    print(\"\\n=== NxFP IRL Results ===\")\n",
    "    rows_nfxp = build_rows(results_nfxp)\n",
    "    print(tabulate(rows_nfxp, headers=hdrs, tablefmt=\"plain\"))\n",
    "    print(\"===================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRIDWORLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MaxEnt IRL (3 Rows) ===\n",
      "Anchor      RankCorr    Corr    RMSE    PolDiff    SvfDiff    ValDiff     Time    LogLik\n",
      "Anch=0          0.97    0.94    0.3       0.028      0.027      2.404  212.634   -2913.7\n",
      "Anch=1          0.88    0.84    0.33      0.043      0.028      1.09   215.785   -2978.9\n",
      "Anch=2          0.73    0.53    0.48      0.099      0.03       1.708  215.803   -3211.8\n",
      "\n",
      "=== NxFP IRL (3 Rows) ===\n",
      "Anchor      RankCorr    Corr    RMSE    PolDiff    SvfDiff    ValDiff    Time    LogLik\n",
      "Anch=0          0.97    0.98    0.35      0.017      0.046      3.106  45.401   -2893.8\n",
      "Anch=1          0.97    0.98    1.04      0.017      0.034     10.497  47.572   -2893.8\n",
      "Anch=2          0.98    0.99    0.95      0.016      0.044      9.58   64.357   -2894.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "from scipy.special import logsumexp\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "np.random.seed(0)\n",
    "\n",
    "# ---------------------------\n",
    "# Environment Setup (5x5)\n",
    "# ---------------------------\n",
    "N = 5\n",
    "n_states = N*N\n",
    "actions = {\"LEFT\":0, \"RIGHT\":1, \"UP\":2, \"DOWN\":3, \"STAY\":4}\n",
    "n_actions = len(actions)\n",
    "gamma = 0.9\n",
    "\n",
    "def to_rc(s):\n",
    "    return divmod(s, N)  # row,col\n",
    "\n",
    "def to_s(r,c):\n",
    "    return r*N + c\n",
    "\n",
    "def build_transition_matrix():\n",
    "    P = {}\n",
    "    for s in range(n_states):\n",
    "        r,c = to_rc(s)\n",
    "        # Possible next states\n",
    "        left_s  = to_s(r, max(c-1, 0))\n",
    "        right_s = to_s(r, min(c+1, N-1))\n",
    "        up_s    = to_s(max(r-1, 0), c)\n",
    "        down_s  = to_s(min(r+1, N-1), c)\n",
    "        stay_s  = s\n",
    "        next_map = [left_s, right_s, up_s, down_s, stay_s]\n",
    "        for a in range(n_actions):\n",
    "            dist = np.zeros(n_states)\n",
    "            dist[next_map[a]] = 1.0  # deterministic transitions\n",
    "            P[(s,a)] = dist\n",
    "    return P\n",
    "\n",
    "P = build_transition_matrix()\n",
    "\n",
    "# One-hot features for each of 25 states\n",
    "features = np.eye(n_states)\n",
    "\n",
    "# ---------------------------\n",
    "# True Rewards (fixed random)\n",
    "# ---------------------------\n",
    "true_rewards = np.random.uniform(-1,1,n_states)\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "def soft_value_iteration(reward, P, tol=1e-6, max_iter=200):\n",
    "    V = np.zeros(n_states)\n",
    "    for _ in range(max_iter):\n",
    "        V_prev = V.copy()\n",
    "        for s in range(n_states):\n",
    "            Q_sa = []\n",
    "            for a in range(n_actions):\n",
    "                Q_sa.append(reward[s] + gamma * np.dot(P[(s,a)], V_prev))\n",
    "            V[s] = logsumexp(Q_sa)\n",
    "        if np.max(np.abs(V - V_prev)) < tol:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def compute_policy(V, reward, P):\n",
    "    policy = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        Q_sa = []\n",
    "        for a in range(n_actions):\n",
    "            Q_sa.append(reward[s] + gamma * np.dot(P[(s,a)], V))\n",
    "        Q_sa = np.array(Q_sa)\n",
    "        shift = Q_sa - np.max(Q_sa)\n",
    "        policy[s] = np.exp(shift)/np.sum(np.exp(shift))\n",
    "    return policy\n",
    "\n",
    "def generate_expert_trajectories(policy, P, n_trajectories=200, traj_len=10):\n",
    "    trajectories = []\n",
    "    for _ in range(n_trajectories):\n",
    "        s = np.random.choice(n_states)\n",
    "        traj = []\n",
    "        for __ in range(traj_len):\n",
    "            a = np.random.choice(n_actions, p=policy[s])\n",
    "            s_next = np.random.choice(n_states, p=P[(s,a)])\n",
    "            traj.append((s,a,s_next))\n",
    "            s = s_next\n",
    "        trajectories.append(traj)\n",
    "    return trajectories\n",
    "\n",
    "def compute_expert_loglik(rew, trajectories, P):\n",
    "    V_est = soft_value_iteration(rew, P)\n",
    "    pol_est = compute_policy(V_est, rew, P)\n",
    "    loglik = 0.0\n",
    "    eps = 1e-12\n",
    "    for traj in trajectories:\n",
    "        for (s,a,_) in traj:\n",
    "            loglik += np.log(pol_est[s,a]+eps)\n",
    "    return loglik\n",
    "\n",
    "def compute_svf(policy, P, start_count=10, traj_len=10):\n",
    "    # Approx. occupancy by simulating from random start states\n",
    "    counts = np.zeros(n_states)\n",
    "    total_steps = start_count * traj_len\n",
    "    for _ in range(start_count):\n",
    "        s = np.random.choice(n_states)\n",
    "        for __ in range(traj_len):\n",
    "            counts[s] += 1\n",
    "            a = np.random.choice(n_actions, p=policy[s])\n",
    "            s = np.random.choice(n_states, p=P[(s,a)])\n",
    "    return counts / total_steps\n",
    "\n",
    "# ---------------------------\n",
    "# Metrics\n",
    "# ---------------------------\n",
    "def reward_metrics(true_r, est_r):\n",
    "    # rank corr, corr, RMSE\n",
    "    r_rank, _ = spearmanr(true_r, est_r)\n",
    "    r_corr, _ = pearsonr(true_r, est_r)\n",
    "    rmse = np.sqrt(np.mean((true_r - est_r)**2))\n",
    "    return r_rank, r_corr, rmse\n",
    "\n",
    "def policy_diff(pol1, pol2):\n",
    "    return np.mean(np.abs(pol1 - pol2))\n",
    "\n",
    "def vector_diff(v1, v2):\n",
    "    return np.mean(np.abs(v1 - v2))\n",
    "\n",
    "# ---------------------------\n",
    "# Expert Data\n",
    "# ---------------------------\n",
    "V_true = soft_value_iteration(true_rewards, P)\n",
    "policy_true = compute_policy(V_true, true_rewards, P)\n",
    "expert_trajectories = generate_expert_trajectories(policy_true, P)\n",
    "expert_svf = compute_svf(policy_true, P)\n",
    "\n",
    "# ---------------------------\n",
    "# MaxEnt IRL\n",
    "# ---------------------------\n",
    "def maxent_irl(features, expert_trajectories, P, anchor_mode=0, anchor_val=0.0, reg=0.0,\n",
    "               lr=0.01, n_iters=5000):\n",
    "    w = np.zeros(features.shape[1])\n",
    "    # If anchor_mode = 1 or 2, fix w[0]\n",
    "    # anchor_mode=2 => also apply a reg in gradient\n",
    "    # Expert occupancy\n",
    "    counts = np.zeros(n_states)\n",
    "    for traj in expert_trajectories:\n",
    "        for (s,a,sn) in traj:\n",
    "            counts[s] += 1\n",
    "    counts /= np.sum(counts)\n",
    "    \n",
    "    for _ in range(n_iters):\n",
    "        r_est = features @ w\n",
    "        V_est = soft_value_iteration(r_est, P)\n",
    "        pol_est = compute_policy(V_est, r_est, P)\n",
    "        svf_est = compute_svf(pol_est, P)\n",
    "        grad = features.T @ (counts - svf_est)\n",
    "        if anchor_mode == 2:\n",
    "            grad -= reg * w\n",
    "        w += lr * grad\n",
    "        if anchor_mode >= 1:\n",
    "            w[0] = anchor_val\n",
    "    return features @ w\n",
    "\n",
    "# ---------------------------\n",
    "# NxFP IRL\n",
    "# ---------------------------\n",
    "def nfxp_objective(params, anchor_mode, anchor_val, reg, expert_trajectories):\n",
    "    # anchor_mode=0 => free r, anchor_mode=1 => anchor r0, anchor_mode=2 => anchor + reg\n",
    "    if anchor_mode == 0:\n",
    "        r = params\n",
    "    else:\n",
    "        # Force r[0] = anchor_val\n",
    "        r = np.concatenate([[anchor_val], params])\n",
    "    V_est = soft_value_iteration(r, P)\n",
    "    pol_est = compute_policy(V_est, r, P)\n",
    "    nll = 0.0\n",
    "    eps = 1e-12\n",
    "    for traj in expert_trajectories:\n",
    "        for (s,a,_) in traj:\n",
    "            nll -= np.log(pol_est[s,a]+eps)\n",
    "    if anchor_mode == 2:\n",
    "        nll += reg * np.sum(params**2)\n",
    "    return nll\n",
    "\n",
    "def estimate_nfxp(expert_trajectories, anchor_mode=0, anchor_val=0.0, reg=0.0):\n",
    "    # For simplicity, dimension = 25. If anchor=1 or 2 => param dimension = 24\n",
    "    if anchor_mode == 0:\n",
    "        x0 = np.zeros(n_states)\n",
    "        bnds = [(-2,2)]*n_states\n",
    "    else:\n",
    "        x0 = np.zeros(n_states-1)\n",
    "        bnds = [(-2,2)]*(n_states-1)\n",
    "    \n",
    "    def wrapper(p):\n",
    "        return nfxp_objective(p, anchor_mode, anchor_val, reg, expert_trajectories)\n",
    "    \n",
    "    res = minimize(wrapper, x0, method='L-BFGS-B', bounds=bnds, options={'maxiter':500, 'disp':False})\n",
    "    if anchor_mode == 0:\n",
    "        r = res.x\n",
    "    else:\n",
    "        r = np.concatenate([[anchor_val], res.x])\n",
    "    return r\n",
    "\n",
    "# ---------------------------\n",
    "# Run 3 Experiments\n",
    "# ---------------------------\n",
    "# 1) Anchor None (no reg)\n",
    "# 2) Anchor One (no reg)\n",
    "# 3) Anchor One + reg\n",
    "anchor_settings = [\n",
    "    (0, 0.0, 0.0),   # no anchor\n",
    "    (1, 1.0, 0.0),   # anchor r[0]=1\n",
    "    (2, 1.0, 0.1)    # anchor r[0]=1 + regularization\n",
    "]\n",
    "\n",
    "results_maxent = []\n",
    "results_nfxp = []\n",
    "\n",
    "for (mode, aval, regval) in anchor_settings:\n",
    "    # MaxEnt\n",
    "    start_t = time.time()\n",
    "    est_r = maxent_irl(features, expert_trajectories, P, anchor_mode=mode,\n",
    "                       anchor_val=aval, reg=regval)\n",
    "    tsec = time.time() - start_t\n",
    "    \n",
    "    # Evaluate\n",
    "    rrank, rcorr, rmse = reward_metrics(true_rewards, est_r)\n",
    "    V_est = soft_value_iteration(est_r, P)\n",
    "    pol_est = compute_policy(V_est, est_r, P)\n",
    "    pol_d = policy_diff(pol_est, policy_true)\n",
    "    svf_est = compute_svf(pol_est, P)\n",
    "    svf_d = vector_diff(svf_est, expert_svf)\n",
    "    val_d = vector_diff(V_est, V_true)\n",
    "    ll = compute_expert_loglik(est_r, expert_trajectories, P)\n",
    "    \n",
    "    results_maxent.append([\n",
    "        f\"Anch={mode}\", f\"{rrank:.2f}\", f\"{rcorr:.2f}\", f\"{rmse:.2f}\",\n",
    "        f\"{pol_d:.3f}\", f\"{svf_d:.3f}\", f\"{val_d:.3f}\",\n",
    "        f\"{tsec:.3f}\", f\"{ll:.1f}\"\n",
    "    ])\n",
    "    \n",
    "    # NxFP\n",
    "    start_t = time.time()\n",
    "    est_r_nfxp = estimate_nfxp(expert_trajectories, mode, aval, regval)\n",
    "    tsec = time.time() - start_t\n",
    "    \n",
    "    # Evaluate\n",
    "    rrank, rcorr, rmse = reward_metrics(true_rewards, est_r_nfxp)\n",
    "    V_est = soft_value_iteration(est_r_nfxp, P)\n",
    "    pol_est = compute_policy(V_est, est_r_nfxp, P)\n",
    "    pol_d = policy_diff(pol_est, policy_true)\n",
    "    svf_est = compute_svf(pol_est, P)\n",
    "    svf_d = vector_diff(svf_est, expert_svf)\n",
    "    val_d = vector_diff(V_est, V_true)\n",
    "    ll = compute_expert_loglik(est_r_nfxp, expert_trajectories, P)\n",
    "    \n",
    "    results_nfxp.append([\n",
    "        f\"Anch={mode}\", f\"{rrank:.2f}\", f\"{rcorr:.2f}\", f\"{rmse:.2f}\",\n",
    "        f\"{pol_d:.3f}\", f\"{svf_d:.3f}\", f\"{val_d:.3f}\",\n",
    "        f\"{tsec:.3f}\", f\"{ll:.1f}\"\n",
    "    ])\n",
    "\n",
    "# ---------------------------\n",
    "# Print Tables\n",
    "# ---------------------------\n",
    "headers = [\"Anchor\",\"RankCorr\",\"Corr\",\"RMSE\",\"PolDiff\",\"SvfDiff\",\"ValDiff\",\"Time\",\"LogLik\"]\n",
    "\n",
    "print(\"=== MaxEnt IRL (3 Rows) ===\")\n",
    "print(tabulate(results_maxent, headers=headers, tablefmt=\"plain\"))\n",
    "print(\"\\n=== NxFP IRL (3 Rows) ===\")\n",
    "print(tabulate(results_nfxp, headers=headers, tablefmt=\"plain\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
