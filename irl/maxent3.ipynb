{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original reward (partial): [0.   0.   0.   0.   0.   0.   0.   0.   0.65 0.  ]\n",
      "Recovered reward (partial): [nan nan nan nan nan nan nan nan nan nan]\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b7/1tvk5qmx0ds9c6gk2lrlhv380000gn/T/ipykernel_25809/1827469641.py:308: RuntimeWarning: overflow encountered in exp\n",
      "  self.w *= np.exp(update)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "##############################################\n",
    "# 1. GridWorld MDP and Some Utilities\n",
    "##############################################\n",
    "\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    Simple GridWorld with:\n",
    "      - 5x5 states\n",
    "      - 4 possible actions (up, down, left, right)\n",
    "      - slip probability\n",
    "    \"\"\"\n",
    "    def __init__(self, size=5, p_slip=0.2):\n",
    "        self.size = size\n",
    "        self.n_states = size * size\n",
    "        self.n_actions = 4  # up, right, down, left\n",
    "        self.actions = [0, 1, 2, 3]\n",
    "        self.p_slip = p_slip\n",
    "        # build transitions:\n",
    "        #   p_transition[s_from, s_to, a] = probability of going from s_from to s_to via action a\n",
    "        self.p_transition = self._build_transition()\n",
    "    \n",
    "    def _build_transition(self):\n",
    "        \"\"\"\n",
    "        Returns a 3D array [n_states, n_states, n_actions], with transition probabilities.\n",
    "        \"\"\"\n",
    "        T = np.zeros((self.n_states, self.n_states, self.n_actions))\n",
    "        for s in range(self.n_states):\n",
    "            r, c = divmod(s, self.size)\n",
    "            for a in range(self.n_actions):\n",
    "                # nominal next-state (assuming no slip)\n",
    "                nr, nc = r, c\n",
    "                if a == 0 and r > 0:         # up\n",
    "                    nr = r - 1\n",
    "                elif a == 1 and c < self.size - 1:  # right\n",
    "                    nc = c + 1\n",
    "                elif a == 2 and r < self.size - 1:  # down\n",
    "                    nr = r + 1\n",
    "                elif a == 3 and c > 0:             # left\n",
    "                    nc = c - 1\n",
    "                s_next = nr * self.size + nc\n",
    "                \n",
    "                # if we do not slip, go to s_next\n",
    "                # else we slip, choose random among all actions with prob p_slip\n",
    "                for slip_a in range(self.n_actions):\n",
    "                    # nominal next-state for slip_a:\n",
    "                    slip_n_r, slip_n_c = r, c\n",
    "                    if slip_a == 0 and r > 0: slip_n_r = r - 1\n",
    "                    if slip_a == 1 and c < self.size - 1: slip_n_c = c + 1\n",
    "                    if slip_a == 2 and r < self.size - 1: slip_n_r = r + 1\n",
    "                    if slip_a == 3 and c > 0: slip_n_c = c - 1\n",
    "                    slip_s_next = slip_n_r * self.size + slip_n_c\n",
    "                    \n",
    "                    if slip_a == a:\n",
    "                        # correct action => no slip => prob = 1.0 - p_slip\n",
    "                        T[s, slip_s_next, a] += (1.0 - self.p_slip)\n",
    "                    else:\n",
    "                        # slip => prob = p_slip / (n_actions - 1)\n",
    "                        T[s, slip_s_next, a] += (self.p_slip / (self.n_actions - 1))\n",
    "        \n",
    "        return T\n",
    "    \n",
    "    def state_features(self):\n",
    "        \"\"\"\n",
    "        One-hot features: each of the n_states has a d=n_states dimensional\n",
    "        feature-vector that is all zeros except 1 at the index of the state.\n",
    "        \"\"\"\n",
    "        eye = np.eye(self.n_states)\n",
    "        return eye\n",
    "\n",
    "##############################################\n",
    "# 2. Expert Trajectories (via a known reward)\n",
    "##############################################\n",
    "\n",
    "def value_iteration(p_transition, reward, discount=0.9, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Standard value-iteration for a given MDP with transitions p_transition,\n",
    "    per-state reward, and discount. Returns the value function.\n",
    "    \"\"\"\n",
    "    n_states, _, n_actions = p_transition.shape\n",
    "    v = np.zeros(n_states)\n",
    "    while True:\n",
    "        v_old = v.copy()\n",
    "        for s in range(n_states):\n",
    "            q_sa = []\n",
    "            for a in range(n_actions):\n",
    "                val = 0.0\n",
    "                for s_next in range(n_states):\n",
    "                    val += p_transition[s, s_next, a] * (reward[s] + discount * v_old[s_next])\n",
    "                q_sa.append(val)\n",
    "            v[s] = max(q_sa)\n",
    "        delta = np.max(np.abs(v - v_old))\n",
    "        if delta < eps:\n",
    "            break\n",
    "    return v\n",
    "\n",
    "def stochastic_policy_from_value(world, value, weighting=lambda x: x**50):\n",
    "    \"\"\"\n",
    "    Convert a value function into a *stochastic* policy (used for demonstration).\n",
    "    weighting(...) emphasizes near-optimal actions (down-weighting suboptimal).\n",
    "    \"\"\"\n",
    "    n_states, _, n_actions = world.p_transition.shape\n",
    "    policy = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    for s in range(n_states):\n",
    "        # compute q-values\n",
    "        q_sa = []\n",
    "        for a in range(n_actions):\n",
    "            val = 0.0\n",
    "            for s_next in range(n_states):\n",
    "                val += world.p_transition[s, s_next, a] * (reward[s] + 0.9 * value[s_next])\n",
    "            q_sa.append(val)\n",
    "        # weighting\n",
    "        q_sa = np.array(q_sa)\n",
    "        w = weighting(q_sa - np.max(q_sa))  # shift so max is 0 => stable\n",
    "        w = np.clip(w, 1e-8, np.inf)        # avoid 0\n",
    "        w /= w.sum()                        # normalize\n",
    "        policy[s] = w\n",
    "    return policy\n",
    "\n",
    "def generate_trajectories(num_trajectories, world, policy_exec, start_states, terminal):\n",
    "    \"\"\"\n",
    "    Generate a set of trajectories by following a (stochastic) policy\n",
    "    from the given start states until hitting a terminal state.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(123)\n",
    "    for _ in range(num_trajectories):\n",
    "        s = rng.choice(start_states)\n",
    "        traj_states = [s]\n",
    "        traj_actions = []\n",
    "        \n",
    "        while s not in terminal:\n",
    "            # pick action from policy\n",
    "            a = policy_exec(s, rng)\n",
    "            traj_actions.append(a)\n",
    "            \n",
    "            # sample next state\n",
    "            p = world.p_transition[s, :, a]\n",
    "            s_next = rng.choice(world.n_states, p=p)\n",
    "            \n",
    "            s = s_next\n",
    "            traj_states.append(s)\n",
    "        \n",
    "        yield Trajectory(traj_states, traj_actions)\n",
    "\n",
    "class Trajectory:\n",
    "    \"\"\"\n",
    "    Simple wrapper to store state and action sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, states, actions):\n",
    "        self._states = states\n",
    "        self._actions = actions\n",
    "    \n",
    "    def states(self):\n",
    "        return self._states\n",
    "    \n",
    "    def actions(self):\n",
    "        return self._actions\n",
    "    \n",
    "    def transitions(self):\n",
    "        \"\"\"\n",
    "        Return (s,a,s_next) for each step, ignoring final if any.\n",
    "        \"\"\"\n",
    "        return list(zip(self._states[:-1], self._actions, self._states[1:]))\n",
    "\n",
    "def stochastic_policy_adapter(policy):\n",
    "    \"\"\"\n",
    "    Turns an NxA policy array into a function that picks an action\n",
    "    from a distribution policy[s].\n",
    "    \"\"\"\n",
    "    def fn(s, rng):\n",
    "        return rng.choice(len(policy[s]), p=policy[s])\n",
    "    return fn\n",
    "\n",
    "##############################################\n",
    "# 3. Some IRL Helper Functions\n",
    "##############################################\n",
    "\n",
    "def feature_expectation_from_trajectories(features, trajectories):\n",
    "    \"\"\"\n",
    "    Compute the empirical average of features from the provided trajectories.\n",
    "    \"\"\"\n",
    "    n_states, n_features = features.shape\n",
    "    fe = np.zeros(n_features)\n",
    "    for traj in trajectories:\n",
    "        for s in traj.states():\n",
    "            fe += features[s]\n",
    "    return fe / len(trajectories)\n",
    "\n",
    "def initial_probabilities_from_trajectories(n_states, trajectories):\n",
    "    \"\"\"\n",
    "    Probability p(s0) that s0 is the start-state of a trajectory.\n",
    "    \"\"\"\n",
    "    p = np.zeros(n_states)\n",
    "    for traj in trajectories:\n",
    "        first_state = traj.transitions()[0][0]\n",
    "        p[first_state] += 1.0\n",
    "    return p / len(trajectories)\n",
    "\n",
    "def compute_expected_svf(p_transition, p_initial, terminal, reward, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Compute the (approximate) expected state visitation frequency:\n",
    "      1) backward pass => local action probabilities\n",
    "      2) forward pass => sum over time\n",
    "    \"\"\"\n",
    "    n_states, _, n_actions = p_transition.shape\n",
    "    nonterminal = set(range(n_states)) - set(terminal)\n",
    "    \n",
    "    # backward pass\n",
    "    zs = np.zeros(n_states)\n",
    "    zs[list(terminal)] = 1.0\n",
    "    \n",
    "    # for extra safety, iterate 2*N\n",
    "    for _ in range(2 * n_states):\n",
    "        za = np.zeros((n_states, n_actions))\n",
    "        for s_from, a in product(range(n_states), range(n_actions)):\n",
    "            for s_to in range(n_states):\n",
    "                za[s_from, a] += (p_transition[s_from, s_to, a] *\n",
    "                                  np.exp(reward[s_from]) *\n",
    "                                  zs[s_to])\n",
    "        zs = za.sum(axis=1)\n",
    "    p_action = za / zs[:, None]\n",
    "    \n",
    "    # forward pass\n",
    "    d = np.zeros((n_states, 2 * n_states))  # d[s,t]\n",
    "    d[:, 0] = p_initial\n",
    "    \n",
    "    for t in range(1, 2 * n_states):\n",
    "        for s_to in range(n_states):\n",
    "            for s_from, a in product(nonterminal, range(n_actions)):\n",
    "                d[s_to, t] += (d[s_from, t-1] *\n",
    "                               p_action[s_from, a] *\n",
    "                               p_transition[s_from, s_to, a])\n",
    "    return d.sum(axis=1)\n",
    "\n",
    "##############################################\n",
    "# 4. The MaxEnt IRL Routine\n",
    "##############################################\n",
    "\n",
    "def maxent_irl(p_transition, features, terminal, trajectories, optimizer, init, eps=1e-4):\n",
    "    \"\"\"\n",
    "    - p_transition: shape = [n_states, n_states, n_actions]\n",
    "    - features:     shape = [n_states, n_features]\n",
    "    - terminal:     list of terminal states\n",
    "    - trajectories: expert demonstration\n",
    "    - optimizer:    e.g. an ExpSga or simple gradient approach\n",
    "    - init:         function returning initial w\n",
    "    \"\"\"\n",
    "    n_states, _, n_actions = p_transition.shape\n",
    "    _, n_features = features.shape\n",
    "    \n",
    "    # 1) empirical feature expectation\n",
    "    e_features = feature_expectation_from_trajectories(features, trajectories)\n",
    "    \n",
    "    # 2) probability of initial states\n",
    "    p_initial = initial_probabilities_from_trajectories(n_states, trajectories)\n",
    "    \n",
    "    # 3) optimize\n",
    "    w = init(n_features)\n",
    "    delta = np.inf\n",
    "    optimizer.reset(w)\n",
    "    \n",
    "    while delta > eps:\n",
    "        w_old = w.copy()\n",
    "        # compute state reward\n",
    "        r_s = features.dot(w)\n",
    "        \n",
    "        # expected svf\n",
    "        e_svf = compute_expected_svf(p_transition, p_initial, terminal, r_s)\n",
    "        \n",
    "        grad = e_features - features.T.dot(e_svf)\n",
    "        optimizer.step(grad)\n",
    "        delta = np.max(np.abs(w_old - w))\n",
    "    \n",
    "    # final reward\n",
    "    return features.dot(w)\n",
    "\n",
    "##############################################\n",
    "# 5. Simple Optimizers\n",
    "##############################################\n",
    "\n",
    "class Constant:\n",
    "    \"\"\"\n",
    "    Initialization: all weights = constant c\n",
    "    \"\"\"\n",
    "    def __init__(self, c):\n",
    "        self.c = c\n",
    "    def __call__(self, dim):\n",
    "        return np.full(dim, self.c)\n",
    "\n",
    "class ExpSga:\n",
    "    \"\"\"\n",
    "    Exponentiated Stochastic Gradient Ascent (for IRL).\n",
    "    Optionally combine with a decaying learning rate.\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr_func = lr\n",
    "    def reset(self, w_init):\n",
    "        self.t = 0\n",
    "        self.w = w_init\n",
    "    def step(self, grad):\n",
    "        lr = self.lr_func(self.t)\n",
    "        self.t += 1\n",
    "        # w <- w * exp(lr * grad)\n",
    "        update = lr * grad\n",
    "        self.w *= np.exp(update)\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self.w\n",
    "\n",
    "def linear_decay(lr0=0.1, decay=1e-3):\n",
    "    \"\"\"\n",
    "    Example learning-rate schedule: lr = lr0 / (1 + decay*t)\n",
    "    \"\"\"\n",
    "    def f(t):\n",
    "        return lr0 / (1.0 + decay * t)\n",
    "    return f\n",
    "\n",
    "##############################################\n",
    "# 6. Putting It All Together (main)\n",
    "##############################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # -- create the GridWorld MDP\n",
    "    world = GridWorld(size=5, p_slip=0.2)\n",
    "    \n",
    "    # set up some reward\n",
    "    reward = np.zeros(world.n_states)\n",
    "    reward[-1] = 1.0   # top-right corner\n",
    "    reward[8]  = 0.65  # just for variety\n",
    "    terminal   = [24]  # top-right corner is terminal\n",
    "    \n",
    "    # build an expert policy (for demonstration only)\n",
    "    discount   = 0.9\n",
    "    value = value_iteration(world.p_transition, reward, discount)\n",
    "    expert_pol = stochastic_policy_from_value(world, value)\n",
    "    policy_exec = stochastic_policy_adapter(expert_pol)\n",
    "    \n",
    "    # generate trajectories\n",
    "    n_trajectories = 200\n",
    "    start = [0]   # bottom-left\n",
    "    tjs = list(generate_trajectories(n_trajectories, world, policy_exec, start, terminal))\n",
    "    \n",
    "    # gather features\n",
    "    features = world.state_features()\n",
    "    \n",
    "    # pick init strategy\n",
    "    init_strategy = Constant(1.0)\n",
    "    \n",
    "    # pick optimizer\n",
    "    optim = ExpSga(lr=linear_decay(lr0=0.2))\n",
    "    \n",
    "    # run max-ent IRL\n",
    "    recovered_reward = maxent_irl(world.p_transition,\n",
    "                                  features,\n",
    "                                  terminal,\n",
    "                                  tjs,\n",
    "                                  optim,\n",
    "                                  init_strategy)\n",
    "    \n",
    "    print(\"Original reward (partial):\", reward[:10])\n",
    "    print(\"Recovered reward (partial):\", recovered_reward[:10])\n",
    "    print(\"Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
