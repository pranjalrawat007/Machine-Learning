{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: |\u001b[43m \u001b[0m: :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | :\u001b[43m \u001b[0m:\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[42mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : :\u001b[42m_\u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : :\u001b[42m_\u001b[0m|\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| :\u001b[42m_\u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m:\u001b[42m_\u001b[0m| : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "# Initialize the Taxi environment\n",
    "env = gym.make('Taxi-v3', render_mode='ansi')\n",
    "env = env.unwrapped  # Access the underlying environment to get transition probabilities\n",
    "\n",
    "# Initialize value function\n",
    "V = np.zeros(env.observation_space.n)\n",
    "gamma = 0.9  # Discount factor\n",
    "theta = 1e-6  # Convergence threshold\n",
    "\n",
    "def one_step_lookahead(state, V):\n",
    "    \"\"\"\n",
    "    Helper function to calculate the value for all actions in a given state.\n",
    "    \"\"\"\n",
    "    A = np.zeros(env.action_space.n)\n",
    "    for action in range(env.action_space.n):\n",
    "        for prob, next_state, reward, done in env.P[state][action]:\n",
    "            A[action] += prob * (reward + gamma * V[next_state])\n",
    "    return A\n",
    "\n",
    "# Value Iteration Algorithm\n",
    "while True:\n",
    "    delta = 0\n",
    "    for state in range(env.observation_space.n):\n",
    "        A = one_step_lookahead(state, V)\n",
    "        best_action_value = np.max(A)\n",
    "        delta = max(delta, np.abs(best_action_value - V[state]))\n",
    "        V[state] = best_action_value\n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "# Extract the optimal policy\n",
    "policy = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "for state in range(env.observation_space.n):\n",
    "    A = one_step_lookahead(state, V)\n",
    "    best_action = np.argmax(A)\n",
    "    policy[state, best_action] = 1.0\n",
    "\n",
    "# Visualize agent's movement\n",
    "def simulate_agent(env, policy):\n",
    "    state, _ = env.reset()\n",
    "    print(env.render())\n",
    "    time.sleep(1)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.argmax(policy[state])\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        print(env.render())\n",
    "        time.sleep(1)\n",
    "\n",
    "simulate_agent(env, policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy Visualization:\n",
      "Initial State:\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  A  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 0, Action: 2, Reward: -1.0\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  A  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 1, Action: 2, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  A  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 2, Action: 2, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  A  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 3, Action: 2, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  A  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 4, Action: 2, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  A  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 5, Action: 2, Reward: 0.0\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  A  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 6, Action: 2, Reward: 0.0\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  A  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 7, Action: 2, Reward: 0.0\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "A  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 8, Action: 0, Reward: -1.3\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      "A  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 9, Action: 0, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      "A  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 10, Action: 0, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      "A  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 11, Action: 0, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "A  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 12, Action: 0, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      "A  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 13, Action: 0, Reward: 0.0\n",
      "A* H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 14, Action: 3, Reward: -0.8\n",
      "P  A* H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 15, Action: 3, Reward: 0.0\n",
      "P  H  A* .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 16, Action: 1, Reward: -1.3\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  A* .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 17, Action: 1, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  A* .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 18, Action: 1, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  A* .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 19, Action: 1, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  A* .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 20, Action: 1, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  A* .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 21, Action: 1, Reward: 0.0\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  A* .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 22, Action: 1, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  A* .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 23, Action: 1, Reward: 0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  A* S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 24, Action: 3, Reward: -0.30000000000000004\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  A* S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 25, Action: 3, Reward: 0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  A* .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 26, Action: 3, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  A* .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 27, Action: 3, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  A* .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 28, Action: 3, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  A* S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 29, Action: 3, Reward: 0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  A* S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 30, Action: 3, Reward: 0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  A* S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 31, Action: 3, Reward: 0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  A* .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 32, Action: 3, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  A* .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 33, Action: 3, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  A* .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 34, Action: 3, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  A* . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 35, Action: 3, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  A*\n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 36, Action: 1, Reward: -1.3\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  A*\n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 37, Action: 1, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  A*\n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 38, Action: 1, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  A*\n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 39, Action: 1, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  A*\n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 40, Action: 1, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  A*\n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 41, Action: 1, Reward: 499.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  A*\n",
      "\n",
      "Episode finished!\n",
      "Performance Metrics:\n",
      "  Total Reward: 484.0\n",
      "  Steps Taken: 42\n",
      "  Safety Incidents: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class CustomTaxiMDP(gym.Env):\n",
    "    \"\"\"\n",
    "    A customizable taxi MDP environment with:\n",
    "    - Variable grid size\n",
    "    - Custom pickup and drop-off locations\n",
    "    - No explicit pickup/dropoff actions; entering pickup cell picks up passenger, entering dropoff cell completes the ride.\n",
    "    - Safety incidents do not terminate the episode but penalize the agent.\n",
    "    - Road types modify movement rewards.\n",
    "    - Balanced rewards: large final dropoff reward, scenic rewards not exploitable indefinitely.\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = {\"render_modes\": [\"ansi\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_rows=5,\n",
    "        grid_cols=5,\n",
    "        pickup_loc=(0,0),\n",
    "        dropoff_loc=(4,4),\n",
    "        incident_prob=0.001,         # base probability of a safety incident each step\n",
    "        unsafe_squares=[],\n",
    "        unsafe_incident_prob=0.01,   # additional probability if on unsafe square\n",
    "        scenic_squares=[],\n",
    "        scenic_reward=0.5,\n",
    "        step_cost=-1,\n",
    "        incident_cost=-10,           # cost for a safety incident\n",
    "        turn_cost=-0.1,              # penalty for changing direction\n",
    "        straight_reward=0.0,         # reward for going straight\n",
    "        road_types=None,             # {(r,c): \"highway\"/\"rough\"/\"scenic\"/...}\n",
    "        final_dropoff_reward=50.0,   # Large reward for completing dropoff\n",
    "        seed=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.grid_rows = grid_rows\n",
    "        self.grid_cols = grid_cols\n",
    "        self.pickup_loc = pickup_loc\n",
    "        self.dropoff_loc = dropoff_loc\n",
    "        self.incident_prob = incident_prob\n",
    "        self.unsafe_squares = set(unsafe_squares)\n",
    "        self.unsafe_incident_prob = unsafe_incident_prob\n",
    "        self.scenic_squares = set(scenic_squares)\n",
    "        self.scenic_reward = scenic_reward\n",
    "        self.step_cost = step_cost\n",
    "        self.incident_cost = incident_cost\n",
    "        self.turn_cost = turn_cost\n",
    "        self.straight_reward = straight_reward\n",
    "        self.final_dropoff_reward = final_dropoff_reward\n",
    "\n",
    "        if road_types is None:\n",
    "            self.road_types = {}\n",
    "        else:\n",
    "            self.road_types = road_types\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        # State representation: (agent_row, agent_col, passenger_picked_up, last_action)\n",
    "        self.observation_space = spaces.MultiDiscrete([self.grid_rows, self.grid_cols, 2, 5])\n",
    "        \n",
    "        self.np_random, _ = gym.utils.seeding.np_random(seed)\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "        # Build the transition model for planning\n",
    "        self._build_transition_model()\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.np_random, _ = gym.utils.seeding.np_random(seed)\n",
    "        self.agent_row = self.np_random.integers(self.grid_rows)\n",
    "        self.agent_col = self.np_random.integers(self.grid_cols)\n",
    "        \n",
    "        # Ensure not starting at dropoff location\n",
    "        while (self.agent_row, self.agent_col) == self.dropoff_loc:\n",
    "            self.agent_row = self.np_random.integers(self.grid_rows)\n",
    "            self.agent_col = self.np_random.integers(self.grid_cols)\n",
    "        \n",
    "        self.passenger_picked_up = False\n",
    "        self.last_action = -1\n",
    "        self.terminated = False\n",
    "\n",
    "        # Performance metrics\n",
    "        self.total_reward = 0.0\n",
    "        self.steps = 0\n",
    "        self.incidents_count = 0\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return (self.agent_row, self.agent_col, int(self.passenger_picked_up), self.last_action+1)\n",
    "\n",
    "    def _compute_road_type_reward(self, r, c):\n",
    "        rt = self.road_types.get((r, c), None)\n",
    "        # Example logic:\n",
    "        if rt == \"highway\":\n",
    "            return 0.5  # partially offsets the step cost\n",
    "        elif rt == \"rough\":\n",
    "            return -0.5 # makes movement more costly\n",
    "        return 0.0\n",
    "\n",
    "    def step(self, action):\n",
    "        prev_action = self.last_action\n",
    "\n",
    "        # Move\n",
    "        new_row = self.agent_row\n",
    "        new_col = self.agent_col\n",
    "        if action == 0 and self.agent_row > 0:\n",
    "            new_row -= 1\n",
    "        elif action == 1 and self.agent_row < self.grid_rows - 1:\n",
    "            new_row += 1\n",
    "        elif action == 2 and self.agent_col > 0:\n",
    "            new_col -= 1\n",
    "        elif action == 3 and self.agent_col < self.grid_cols - 1:\n",
    "            new_col += 1\n",
    "        \n",
    "        self.agent_row = new_row\n",
    "        self.agent_col = new_col\n",
    "        \n",
    "        # Base reward\n",
    "        reward = self.step_cost + self._compute_road_type_reward(new_row, new_col)\n",
    "\n",
    "        # Turn or straight\n",
    "        if prev_action != -1 and prev_action != action:\n",
    "            reward += self.turn_cost\n",
    "        elif prev_action != -1 and prev_action == action:\n",
    "            reward += self.straight_reward\n",
    "            \n",
    "        # Pickup\n",
    "        if (self.agent_row, self.agent_col) == self.pickup_loc and not self.passenger_picked_up:\n",
    "            self.passenger_picked_up = True\n",
    "        \n",
    "        # Scenic\n",
    "        if (self.agent_row, self.agent_col) in self.scenic_squares:\n",
    "            reward += self.scenic_reward\n",
    "        \n",
    "        # Check dropoff\n",
    "        if (self.agent_row, self.agent_col) == self.dropoff_loc and self.passenger_picked_up:\n",
    "            # Add large final reward\n",
    "            reward += self.final_dropoff_reward\n",
    "            self.terminated = True\n",
    "        \n",
    "        # Incident Probability\n",
    "        local_incident_prob = self.incident_prob\n",
    "        if (self.agent_row, self.agent_col) in self.unsafe_squares:\n",
    "            local_incident_prob += self.unsafe_incident_prob\n",
    "        \n",
    "        # Incident\n",
    "        if self.np_random.random() < local_incident_prob:\n",
    "            reward += self.incident_cost\n",
    "            self.incidents_count += 1\n",
    "        \n",
    "        self.last_action = action\n",
    "        self.total_reward += reward\n",
    "        self.steps += 1\n",
    "\n",
    "        return self._get_obs(), reward, self.terminated, False, {}\n",
    "\n",
    "    def render(self, mode='ansi'):\n",
    "        grid = np.full((self.grid_rows, self.grid_cols), '.', dtype=object)\n",
    "        \n",
    "        for (r,c) in self.unsafe_squares:\n",
    "            grid[r,c] = 'U'\n",
    "        for (r,c) in self.scenic_squares:\n",
    "            if grid[r,c] == 'U':\n",
    "                grid[r,c] = 'US'\n",
    "            else:\n",
    "                grid[r,c] = 'S'\n",
    "        \n",
    "        for (rc, cc), rt in self.road_types.items():\n",
    "            if rt == 'highway':\n",
    "                symbol = 'H'\n",
    "            elif rt == 'rough':\n",
    "                symbol = 'R'\n",
    "            else:\n",
    "                symbol = grid[rc, cc]\n",
    "            if grid[rc,cc] not in ['.', 'H', 'R', 'S', 'U', 'US']:\n",
    "                symbol = grid[rc,cc] + rt[0].upper()\n",
    "            grid[rc, cc] = symbol\n",
    "\n",
    "        pr, pc = self.pickup_loc\n",
    "        grid[pr, pc] = 'P'\n",
    "        dr, dc = self.dropoff_loc\n",
    "        grid[dr, dc] = 'D'\n",
    "        \n",
    "        ar, ac = self.agent_row, self.agent_col\n",
    "        agent_marker = 'A*' if self.passenger_picked_up else 'A'\n",
    "        grid[ar, ac] = agent_marker\n",
    "        \n",
    "        output = ''\n",
    "        for r in range(self.grid_rows):\n",
    "            row_str = ' '.join(f\"{cell:2}\" for cell in grid[r,:])\n",
    "            output += row_str + '\\n'\n",
    "        return output\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def _state_to_tuple(self, s):\n",
    "        return tuple(s)\n",
    "\n",
    "    def _get_all_states(self):\n",
    "        states = []\n",
    "        for r in range(self.grid_rows):\n",
    "            for c in range(self.grid_cols):\n",
    "                for pu in [0,1]:\n",
    "                    for la in range(5):\n",
    "                        states.append((r,c,pu,la))\n",
    "        return states\n",
    "\n",
    "    def _build_transition_model(self):\n",
    "        self.P = {}\n",
    "        all_states = self._get_all_states()\n",
    "        for s in all_states:\n",
    "            self.P[s] = {}\n",
    "            for a in range(self.action_space.n):\n",
    "                obs, reward, done = self._simulate_transition(s, a)\n",
    "                self.P[s][a] = (obs, reward, done)\n",
    "    \n",
    "    def _simulate_transition(self, s, a):\n",
    "        (r,c,pu,la) = s\n",
    "        last_action = la - 1\n",
    "        \n",
    "        new_r, new_c = r,c\n",
    "        if a == 0 and r > 0:\n",
    "            new_r -= 1\n",
    "        elif a == 1 and r < self.grid_rows - 1:\n",
    "            new_r += 1\n",
    "        elif a == 2 and c > 0:\n",
    "            new_c -= 1\n",
    "        elif a == 3 and c < self.grid_cols - 1:\n",
    "            new_c += 1\n",
    "\n",
    "        reward = self.step_cost + self._compute_road_type_reward(new_r, new_c)\n",
    "        \n",
    "        if last_action != -1 and last_action != a:\n",
    "            reward += self.turn_cost\n",
    "        elif last_action != -1 and last_action == a:\n",
    "            reward += self.straight_reward\n",
    "        \n",
    "        passenger_picked = (pu == 1)\n",
    "        \n",
    "        if (new_r, new_c) == self.pickup_loc and not passenger_picked:\n",
    "            passenger_picked = True\n",
    "        \n",
    "        if (new_r, new_c) in self.scenic_squares:\n",
    "            reward += self.scenic_reward\n",
    "        \n",
    "        done = False\n",
    "        if (new_r, new_c) == self.dropoff_loc and passenger_picked:\n",
    "            reward += self.final_dropoff_reward\n",
    "            done = True\n",
    "\n",
    "        local_incident_prob = self.incident_prob\n",
    "        if (new_r, new_c) in self.unsafe_squares:\n",
    "            local_incident_prob += self.unsafe_incident_prob\n",
    "        \n",
    "        reward += local_incident_prob * self.incident_cost\n",
    "        \n",
    "        new_pu = 1 if passenger_picked else 0\n",
    "        new_la = a + 1\n",
    "        next_s = (new_r, new_c, new_pu, new_la)\n",
    "        \n",
    "        return next_s, reward, done\n",
    "\n",
    "    def compute_optimal_policy(self, gamma=0.99, theta=1e-6):\n",
    "        all_states = self._get_all_states()\n",
    "        V = {s:0.0 for s in all_states}\n",
    "\n",
    "        def is_terminal(s):\n",
    "            (r,c,pu,la) = s\n",
    "            if (r,c) == self.dropoff_loc and pu == 1:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in all_states:\n",
    "                if is_terminal(s):\n",
    "                    continue\n",
    "                v = V[s]\n",
    "                q_values = []\n",
    "                for a in range(self.action_space.n):\n",
    "                    (next_s, r, done) = self.P[s][a]\n",
    "                    q_val = r + gamma*(0 if done else V[next_s])\n",
    "                    q_values.append(q_val)\n",
    "                V[s] = max(q_values)\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "        \n",
    "        pi = {}\n",
    "        for s in all_states:\n",
    "            if is_terminal(s):\n",
    "                pi[s] = 0\n",
    "                continue\n",
    "            q_values = []\n",
    "            for a in range(self.action_space.n):\n",
    "                (next_s, r, done) = self.P[s][a]\n",
    "                q_val = r + gamma*(0 if done else V[next_s])\n",
    "                q_values.append(q_val)\n",
    "            pi[s] = np.argmax(q_values)\n",
    "        \n",
    "        return V, pi\n",
    "\n",
    "    def visualize_policy(self, pi, max_steps=50):\n",
    "        obs, info = self.reset()\n",
    "        step_count = 0\n",
    "        print(\"Initial State:\")\n",
    "        print(self.render())\n",
    "        while step_count < max_steps:\n",
    "            s = tuple(obs)\n",
    "            a = pi[s]\n",
    "            obs, r, done, truncated, _ = self.step(a)\n",
    "            print(f\"Step: {step_count}, Action: {a}, Reward: {r}\")\n",
    "            print(self.render())\n",
    "            if done:\n",
    "                print(\"Episode finished!\")\n",
    "                break\n",
    "            step_count += 1\n",
    "        if step_count >= max_steps:\n",
    "            print(\"Reached max steps without termination.\")\n",
    "\n",
    "        print(\"Performance Metrics:\")\n",
    "        print(f\"  Total Reward: {self.total_reward}\")\n",
    "        print(f\"  Steps Taken: {self.steps}\")\n",
    "        print(f\"  Safety Incidents: {self.incidents_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Updated realistic configuration with balanced rewards\n",
    "    env = CustomTaxiMDP(\n",
    "        grid_rows=15,\n",
    "        grid_cols=15,\n",
    "        pickup_loc=(0, 0),\n",
    "        dropoff_loc=(14, 14),\n",
    "        unsafe_squares=[\n",
    "            (2, 5), (2, 6), (2, 7),\n",
    "            (3, 5), (3, 6), (3, 7),\n",
    "            (4, 5), (4, 6), (4, 7)\n",
    "        ],\n",
    "        scenic_squares=[\n",
    "            (0, 11), (0, 12), (0, 13),\n",
    "            (8, 2), (8, 3), (8, 4),\n",
    "            (8, 8), (8, 9), (8, 10)\n",
    "        ],\n",
    "        incident_prob=0.001,\n",
    "        unsafe_incident_prob=0.02,\n",
    "        scenic_reward=1.0,         # Scenic gives +1 but step cost is -1, net 0 if staying put\n",
    "        step_cost=-1,\n",
    "        incident_cost=-20,\n",
    "        turn_cost=-0.3,\n",
    "        straight_reward=0.5,\n",
    "        final_dropoff_reward=500.0, # Large final reward to ensure completing the trip is best\n",
    "        road_types={\n",
    "            # Highways (just some examples)\n",
    "            (0, 0): 'highway', (0, 1): 'highway', (0, 2): 'highway', \n",
    "            (6, 0): 'highway', (6, 1): 'highway', (6, 2): 'highway',\n",
    "            (11, 0): 'highway', (11, 1): 'highway', (11, 2): 'highway',\n",
    "\n",
    "            # Rough roads\n",
    "            (2, 5): 'rough', (2, 6): 'rough', (2, 7): 'rough',\n",
    "            (3, 5): 'rough', (3, 6): 'rough', (3, 7): 'rough',\n",
    "            (4, 5): 'rough', (4, 6): 'rough', (4, 7): 'rough',\n",
    "\n",
    "            # Scenic roads\n",
    "            (0, 11): 'scenic', (0, 12): 'scenic', (0, 13): 'scenic',\n",
    "            (8, 2): 'scenic', (8, 3): 'scenic', (8, 4): 'scenic',\n",
    "            (8, 8): 'scenic', (8, 9): 'scenic', (8, 10): 'scenic'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    V, pi = env.compute_optimal_policy(gamma=0.99, theta=1e-6)\n",
    "    print(\"Optimal Policy Visualization:\")\n",
    "    env.visualize_policy(pi, max_steps=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy Visualization:\n",
      "Initial State:\n",
      "Episode finished!\n",
      "Performance Metrics:\n",
      "  Total Reward: 493.5\n",
      "  Steps Taken: 43\n",
      "  Safety Incidents: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "\n",
    "class CustomTaxiMDP(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Taxi MDP with improved Pygame Visualization.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"ansi\", \"human\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_rows=5,\n",
    "        grid_cols=5,\n",
    "        pickup_loc=(0,0),\n",
    "        dropoff_loc=(4,4),\n",
    "        incident_prob=0.001,\n",
    "        unsafe_squares=[],\n",
    "        unsafe_incident_prob=0.01,\n",
    "        scenic_squares=[],\n",
    "        scenic_reward=0.5,\n",
    "        step_cost=-1,\n",
    "        incident_cost=-10,\n",
    "        turn_cost=-0.1,\n",
    "        straight_reward=0.0,\n",
    "        road_types=None,\n",
    "        final_dropoff_reward=50.0,\n",
    "        render_mode=None,\n",
    "        seed=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.grid_rows = grid_rows\n",
    "        self.grid_cols = grid_cols\n",
    "        self.pickup_loc = pickup_loc\n",
    "        self.dropoff_loc = dropoff_loc\n",
    "        self.incident_prob = incident_prob\n",
    "        self.unsafe_squares = set(unsafe_squares)\n",
    "        self.unsafe_incident_prob = unsafe_incident_prob\n",
    "        self.scenic_squares = set(scenic_squares)\n",
    "        self.scenic_reward = scenic_reward\n",
    "        self.step_cost = step_cost\n",
    "        self.incident_cost = incident_cost\n",
    "        self.turn_cost = turn_cost\n",
    "        self.straight_reward = straight_reward\n",
    "        self.final_dropoff_reward = final_dropoff_reward\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        if road_types is None:\n",
    "            self.road_types = {}\n",
    "        else:\n",
    "            self.road_types = road_types\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.MultiDiscrete([self.grid_rows, self.grid_cols, 2, 5])\n",
    "        \n",
    "        self.np_random, _ = gym.utils.seeding.np_random(seed)\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "        self._build_transition_model()\n",
    "\n",
    "        # Pygame Initialization if needed\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.cell_size = 40  # pixels per cell\n",
    "        self.info_bar_height = 80\n",
    "        self.window_size = (self.grid_cols * self.cell_size, self.grid_rows * self.cell_size + self.info_bar_height)\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.set_caption(\"Custom Taxi MDP\")\n",
    "            self.window = pygame.display.set_mode(self.window_size)\n",
    "            self.clock = pygame.time.Clock()\n",
    "            pygame.font.init()\n",
    "            self.font = pygame.font.SysFont(\"Arial\", 20)\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.np_random, _ = gym.utils.seeding.np_random(seed)\n",
    "        self.agent_row = self.np_random.integers(self.grid_rows)\n",
    "        self.agent_col = self.np_random.integers(self.grid_cols)\n",
    "        \n",
    "        # Ensure not starting at dropoff location\n",
    "        while (self.agent_row, self.agent_col) == self.dropoff_loc:\n",
    "            self.agent_row = self.np_random.integers(self.grid_rows)\n",
    "            self.agent_col = self.np_random.integers(self.grid_cols)\n",
    "        \n",
    "        self.passenger_picked_up = False\n",
    "        self.last_action = -1\n",
    "        self.terminated = False\n",
    "\n",
    "        # Performance metrics\n",
    "        self.total_reward = 0.0\n",
    "        self.steps = 0\n",
    "        self.incidents_count = 0\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return (self.agent_row, self.agent_col, int(self.passenger_picked_up), self.last_action+1)\n",
    "\n",
    "    def _compute_road_type_reward(self, r, c):\n",
    "        rt = self.road_types.get((r, c), None)\n",
    "        if rt == \"highway\":\n",
    "            return 0.5\n",
    "        elif rt == \"rough\":\n",
    "            return -0.5\n",
    "        return 0.0\n",
    "\n",
    "    def step(self, action):\n",
    "        prev_action = self.last_action\n",
    "\n",
    "        # Move\n",
    "        new_row = self.agent_row\n",
    "        new_col = self.agent_col\n",
    "        if action == 0 and self.agent_row > 0:\n",
    "            new_row -= 1\n",
    "        elif action == 1 and self.agent_row < self.grid_rows - 1:\n",
    "            new_row += 1\n",
    "        elif action == 2 and self.agent_col > 0:\n",
    "            new_col -= 1\n",
    "        elif action == 3 and self.agent_col < self.grid_cols - 1:\n",
    "            new_col += 1\n",
    "        \n",
    "        self.agent_row = new_row\n",
    "        self.agent_col = new_col\n",
    "        \n",
    "        # Base reward\n",
    "        reward = self.step_cost + self._compute_road_type_reward(new_row, new_col)\n",
    "\n",
    "        # Turn or straight\n",
    "        if prev_action != -1 and prev_action != action:\n",
    "            reward += self.turn_cost\n",
    "        elif prev_action != -1 and prev_action == action:\n",
    "            reward += self.straight_reward\n",
    "            \n",
    "        # Pickup\n",
    "        if (self.agent_row, self.agent_col) == self.pickup_loc and not self.passenger_picked_up:\n",
    "            self.passenger_picked_up = True\n",
    "        \n",
    "        # Scenic\n",
    "        if (self.agent_row, self.agent_col) in self.scenic_squares:\n",
    "            reward += self.scenic_reward\n",
    "        \n",
    "        # Check dropoff\n",
    "        if (self.agent_row, self.agent_col) == self.dropoff_loc and self.passenger_picked_up:\n",
    "            reward += self.final_dropoff_reward\n",
    "            self.terminated = True\n",
    "        \n",
    "        # Incident Probability\n",
    "        local_incident_prob = self.incident_prob\n",
    "        if (self.agent_row, self.agent_col) in self.unsafe_squares:\n",
    "            local_incident_prob += self.unsafe_incident_prob\n",
    "        \n",
    "        # Incident\n",
    "        if self.np_random.random() < local_incident_prob:\n",
    "            reward += self.incident_cost\n",
    "            self.incidents_count += 1\n",
    "        \n",
    "        self.last_action = action\n",
    "        self.total_reward += reward\n",
    "        self.steps += 1\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        return self._get_obs(), reward, self.terminated, False, {}\n",
    "\n",
    "    def render(self, mode=None):\n",
    "        if mode is None:\n",
    "            mode = self.render_mode\n",
    "\n",
    "        if mode == \"ansi\":\n",
    "            return self._render_text()\n",
    "        elif mode == \"human\":\n",
    "            return self._render_pygame()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported render mode: {}\".format(mode))\n",
    "\n",
    "    def _render_text(self):\n",
    "        grid = np.full((self.grid_rows, self.grid_cols), '.', dtype=object)\n",
    "        \n",
    "        for (r,c) in self.unsafe_squares:\n",
    "            grid[r,c] = 'U'\n",
    "        for (r,c) in self.scenic_squares:\n",
    "            if grid[r,c] == 'U':\n",
    "                grid[r,c] = 'US'\n",
    "            else:\n",
    "                grid[r,c] = 'S'\n",
    "        \n",
    "        for (rc, cc), rt in self.road_types.items():\n",
    "            if rt == 'highway':\n",
    "                symbol = 'H'\n",
    "            elif rt == 'rough':\n",
    "                symbol = 'R'\n",
    "            else:\n",
    "                symbol = grid[rc, cc]\n",
    "            if grid[rc,cc] not in ['.', 'H', 'R', 'S', 'U', 'US']:\n",
    "                symbol = grid[rc,cc] + rt[0].upper()\n",
    "            grid[rc, cc] = symbol\n",
    "\n",
    "        pr, pc = self.pickup_loc\n",
    "        grid[pr, pc] = 'P'\n",
    "        dr, dc = self.dropoff_loc\n",
    "        grid[dr, dc] = 'D'\n",
    "        \n",
    "        ar, ac = self.agent_row, self.agent_col\n",
    "        agent_marker = 'A*' if self.passenger_picked_up else 'A'\n",
    "        grid[ar, ac] = agent_marker\n",
    "        \n",
    "        output = ''\n",
    "        for r in range(self.grid_rows):\n",
    "            row_str = ' '.join(f\"{cell:2}\" for cell in grid[r,:])\n",
    "            output += row_str + '\\n'\n",
    "        return output\n",
    "\n",
    "    def _render_pygame(self):\n",
    "        if self.window is None:\n",
    "            pygame.init()\n",
    "            pygame.display.set_caption(\"Custom Taxi MDP\")\n",
    "            self.window = pygame.display.set_mode(self.window_size)\n",
    "            self.clock = pygame.time.Clock()\n",
    "            pygame.font.init()\n",
    "            self.font = pygame.font.SysFont(\"Arial\", 20)\n",
    "\n",
    "        # Colors\n",
    "        COLOR_BACKGROUND = (230, 230, 230)\n",
    "        COLOR_HIGHWAY = (169,169,169)   # Dark grey for highways\n",
    "        COLOR_ROUGH = (139, 69, 19)     # brown\n",
    "        COLOR_SCENIC = (34, 139, 34)    # green\n",
    "        COLOR_UNSAFE = (255, 0, 0)      # red\n",
    "        COLOR_NORMAL = (200, 200, 200)  # lighter gray for normal roads\n",
    "        COLOR_PICKUP = (255, 255, 0)    # yellow\n",
    "        COLOR_DROPOFF = (0, 255, 255)   # cyan\n",
    "        COLOR_AGENT = (0, 0, 255)       # blue\n",
    "        COLOR_AGENT_WITH_PASSENGER = (0, 0, 128) # darker blue\n",
    "\n",
    "        self.window.fill(COLOR_BACKGROUND)\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "\n",
    "        # Draw the grid and cells\n",
    "        top_offset = self.info_bar_height\n",
    "\n",
    "        for r in range(self.grid_rows):\n",
    "            for c in range(self.grid_cols):\n",
    "                cell_rect = pygame.Rect(c*self.cell_size, r*self.cell_size+top_offset, self.cell_size, self.cell_size)\n",
    "                \n",
    "                rt = self.road_types.get((r,c), None)\n",
    "                base_color = COLOR_NORMAL\n",
    "                if rt == \"highway\":\n",
    "                    base_color = COLOR_HIGHWAY\n",
    "                elif rt == \"rough\":\n",
    "                    base_color = COLOR_ROUGH\n",
    "\n",
    "                if (r,c) in self.scenic_squares:\n",
    "                    base_color = COLOR_SCENIC\n",
    "                if (r,c) in self.unsafe_squares:\n",
    "                    base_color = COLOR_UNSAFE\n",
    "\n",
    "                if (r,c) == self.pickup_loc:\n",
    "                    base_color = COLOR_PICKUP\n",
    "                if (r,c) == self.dropoff_loc:\n",
    "                    base_color = COLOR_DROPOFF\n",
    "\n",
    "                pygame.draw.rect(self.window, base_color, cell_rect)\n",
    "\n",
    "                # Agent\n",
    "                if (r,c) == (self.agent_row, self.agent_col):\n",
    "                    agent_color = COLOR_AGENT_WITH_PASSENGER if self.passenger_picked_up else COLOR_AGENT\n",
    "                    inset = 5\n",
    "                    agent_rect = pygame.Rect(\n",
    "                        c*self.cell_size+inset, r*self.cell_size+inset+top_offset,\n",
    "                        self.cell_size-2*inset, self.cell_size-2*inset\n",
    "                    )\n",
    "                    pygame.draw.rect(self.window, agent_color, agent_rect)\n",
    "\n",
    "        # Draw grid lines for a road-like structure\n",
    "        line_color = (100,100,100)\n",
    "        for x in range(self.grid_cols+1):\n",
    "            pygame.draw.line(self.window, line_color, (x*self.cell_size, top_offset), (x*self.cell_size, self.grid_rows*self.cell_size+top_offset), 1)\n",
    "        for y in range(self.grid_rows+1):\n",
    "            pygame.draw.line(self.window, line_color, (0, y*self.cell_size+top_offset), (self.grid_cols*self.cell_size, y*self.cell_size+top_offset), 1)\n",
    "\n",
    "        # Draw info bar at the top\n",
    "        info_bar_rect = pygame.Rect(0, 0, self.window_size[0], top_offset)\n",
    "        pygame.draw.rect(self.window, (50,50,50), info_bar_rect)\n",
    "\n",
    "        # Show metrics\n",
    "        passenger_status = \"Yes\" if self.passenger_picked_up else \"No\"\n",
    "        info_text = f\"Steps: {self.steps} | Total Reward: {self.total_reward:.2f} | Incidents: {self.incidents_count} | Passenger Picked: {passenger_status}\"\n",
    "        info_surface = self.font.render(info_text, True, (255,255,255))\n",
    "        self.window.blit(info_surface, (10, 10))\n",
    "\n",
    "        pygame.display.update()\n",
    "        self.clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.window = None\n",
    "\n",
    "    def _state_to_tuple(self, s):\n",
    "        return tuple(s)\n",
    "\n",
    "    def _get_all_states(self):\n",
    "        states = []\n",
    "        for r in range(self.grid_rows):\n",
    "            for c in range(self.grid_cols):\n",
    "                for pu in [0,1]:\n",
    "                    for la in range(5):\n",
    "                        states.append((r,c,pu,la))\n",
    "        return states\n",
    "\n",
    "    def _build_transition_model(self):\n",
    "        self.P = {}\n",
    "        all_states = self._get_all_states()\n",
    "        for s in all_states:\n",
    "            self.P[s] = {}\n",
    "            for a in range(self.action_space.n):\n",
    "                obs, reward, done = self._simulate_transition(s, a)\n",
    "                self.P[s][a] = (obs, reward, done)\n",
    "    \n",
    "    def _simulate_transition(self, s, a):\n",
    "        (r,c,pu,la) = s\n",
    "        last_action = la - 1\n",
    "        \n",
    "        new_r, new_c = r,c\n",
    "        if a == 0 and r > 0:\n",
    "            new_r -= 1\n",
    "        elif a == 1 and r < self.grid_rows - 1:\n",
    "            new_r += 1\n",
    "        elif a == 2 and c > 0:\n",
    "            new_c -= 1\n",
    "        elif a == 3 and c < self.grid_cols - 1:\n",
    "            new_c += 1\n",
    "\n",
    "        reward = self.step_cost + self._compute_road_type_reward(new_r, new_c)\n",
    "        \n",
    "        if last_action != -1 and last_action != a:\n",
    "            reward += self.turn_cost\n",
    "        elif last_action != -1 and last_action == a:\n",
    "            reward += self.straight_reward\n",
    "        \n",
    "        passenger_picked = (pu == 1)\n",
    "        \n",
    "        if (new_r, new_c) == self.pickup_loc and not passenger_picked:\n",
    "            passenger_picked = True\n",
    "        \n",
    "        if (new_r, new_c) in self.scenic_squares:\n",
    "            reward += self.scenic_reward\n",
    "        \n",
    "        done = False\n",
    "        if (new_r, new_c) == self.dropoff_loc and passenger_picked:\n",
    "            reward += self.final_dropoff_reward\n",
    "            done = True\n",
    "\n",
    "        local_incident_prob = self.incident_prob\n",
    "        if (new_r, new_c) in self.unsafe_squares:\n",
    "            local_incident_prob += self.unsafe_incident_prob\n",
    "        \n",
    "        reward += local_incident_prob * self.incident_cost\n",
    "        \n",
    "        new_pu = 1 if passenger_picked else 0\n",
    "        new_la = a + 1\n",
    "        next_s = (new_r, new_c, new_pu, new_la)\n",
    "        \n",
    "        return next_s, reward, done\n",
    "\n",
    "    def compute_optimal_policy(self, gamma=0.99, theta=1e-6):\n",
    "        all_states = self._get_all_states()\n",
    "        V = {s:0.0 for s in all_states}\n",
    "\n",
    "        def is_terminal(s):\n",
    "            (r,c,pu,la) = s\n",
    "            return (r,c) == self.dropoff_loc and pu == 1\n",
    "\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in all_states:\n",
    "                if is_terminal(s):\n",
    "                    continue\n",
    "                v = V[s]\n",
    "                q_values = []\n",
    "                for a in range(self.action_space.n):\n",
    "                    (next_s, r, done) = self.P[s][a]\n",
    "                    q_val = r + gamma*(0 if done else V[next_s])\n",
    "                    q_values.append(q_val)\n",
    "                V[s] = max(q_values)\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "        \n",
    "        pi = {}\n",
    "        for s in all_states:\n",
    "            if is_terminal(s):\n",
    "                pi[s] = 0\n",
    "                continue\n",
    "            q_values = []\n",
    "            for a in range(self.action_space.n):\n",
    "                (next_s, r, done) = self.P[s][a]\n",
    "                q_val = r + gamma*(0 if done else V[next_s])\n",
    "                q_values.append(q_val)\n",
    "            pi[s] = np.argmax(q_values)\n",
    "        \n",
    "        return V, pi\n",
    "\n",
    "    def visualize_policy(self, pi, max_steps=50):\n",
    "        obs, info = self.reset()\n",
    "        step_count = 0\n",
    "        print(\"Initial State:\")\n",
    "        if self.render_mode == 'ansi':\n",
    "            print(self.render('ansi'))\n",
    "        while step_count < max_steps:\n",
    "            s = tuple(obs)\n",
    "            a = pi[s]\n",
    "            obs, r, done, truncated, _ = self.step(a)\n",
    "            if self.render_mode == 'ansi':\n",
    "                print(f\"Step: {step_count}, Action: {a}, Reward: {r}\")\n",
    "                print(self.render('ansi'))\n",
    "            if done:\n",
    "                print(\"Episode finished!\")\n",
    "                break\n",
    "            step_count += 1\n",
    "        if step_count >= max_steps:\n",
    "            print(\"Reached max steps without termination.\")\n",
    "\n",
    "        print(\"Performance Metrics:\")\n",
    "        print(f\"  Total Reward: {self.total_reward}\")\n",
    "        print(f\"  Steps Taken: {self.steps}\")\n",
    "        print(f\"  Safety Incidents: {self.incidents_count}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = CustomTaxiMDP(\n",
    "        grid_rows=15,\n",
    "        grid_cols=15,\n",
    "        pickup_loc=(0, 0),  # Pickup location at top-left corner\n",
    "        dropoff_loc=(14, 14),  # Dropoff location at bottom-right corner\n",
    "        unsafe_squares=[\n",
    "            # Unsafe stretches representing high-traffic or industrial zones\n",
    "            (2, 7), (3, 7), (4, 7), (5, 7), (6, 7), (7, 7), (8, 7),\n",
    "            (11, 4), (11, 5), (11, 6), (11, 7), (11, 8)\n",
    "        ],\n",
    "        scenic_squares=[\n",
    "            # Scenic roads on the eastern side\n",
    "            (1, 12), (2, 12), (3, 12), (4, 12), (5, 12), (6, 12),\n",
    "            (12, 13), (12, 14), (13, 13), (13, 14)\n",
    "        ],\n",
    "        road_types={\n",
    "            # Highways form a cross in the center\n",
    "            (7, c): 'highway' for c in range(15)  # Horizontal highway\n",
    "        } | {\n",
    "            (r, 7): 'highway' for r in range(15)  # Vertical highway\n",
    "        } | {\n",
    "            # Additional rough roads\n",
    "            (5, 5): 'rough', (6, 6): 'rough', (8, 8): 'rough'\n",
    "        },\n",
    "        incident_prob=0.002,  # Base incident probability\n",
    "        unsafe_incident_prob=0.05,  # Higher risk in unsafe squares\n",
    "        scenic_reward=2.0,  # Higher reward for scenic roads\n",
    "        step_cost=-1,  # Cost per step\n",
    "        incident_cost=-100,  # Heavy penalty for safety incidents\n",
    "        turn_cost=-0.3,  # Penalty for turns\n",
    "        straight_reward=0.5,  # Reward for going straight\n",
    "        final_dropoff_reward=500.0,  # Large reward for dropoff\n",
    "        render_mode='human'  # Pygame visualization\n",
    "    )\n",
    "\n",
    "    # Compute optimal policy\n",
    "    V, pi = env.compute_optimal_policy(gamma=0.99, theta=1e-6)\n",
    "\n",
    "    # Visualize the policy execution\n",
    "    print(\"Optimal Policy Visualization:\")\n",
    "    env.visualize_policy(pi, max_steps=100)\n",
    "\n",
    "    # Close the environment properly\n",
    "    env.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
