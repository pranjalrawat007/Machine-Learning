{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[42mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| :\u001b[42m_\u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m:\u001b[42m_\u001b[0m| : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "# Initialize the Taxi environment\n",
    "env = gym.make('Taxi-v3', render_mode='ansi')\n",
    "env = env.unwrapped  # Access the underlying environment to get transition probabilities\n",
    "\n",
    "# Initialize value function\n",
    "V = np.zeros(env.observation_space.n)\n",
    "gamma = 0.9  # Discount factor\n",
    "theta = 1e-6  # Convergence threshold\n",
    "\n",
    "def one_step_lookahead(state, V):\n",
    "    \"\"\"\n",
    "    Helper function to calculate the value for all actions in a given state.\n",
    "    \"\"\"\n",
    "    A = np.zeros(env.action_space.n)\n",
    "    for action in range(env.action_space.n):\n",
    "        for prob, next_state, reward, done in env.P[state][action]:\n",
    "            A[action] += prob * (reward + gamma * V[next_state])\n",
    "    return A\n",
    "\n",
    "# Value Iteration Algorithm\n",
    "while True:\n",
    "    delta = 0\n",
    "    for state in range(env.observation_space.n):\n",
    "        A = one_step_lookahead(state, V)\n",
    "        best_action_value = np.max(A)\n",
    "        delta = max(delta, np.abs(best_action_value - V[state]))\n",
    "        V[state] = best_action_value\n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "# Extract the optimal policy\n",
    "policy = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "for state in range(env.observation_space.n):\n",
    "    A = one_step_lookahead(state, V)\n",
    "    best_action = np.argmax(A)\n",
    "    policy[state, best_action] = 1.0\n",
    "\n",
    "# Visualize agent's movement\n",
    "def simulate_agent(env, policy):\n",
    "    state, _ = env.reset()\n",
    "    print(env.render())\n",
    "    time.sleep(1)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.argmax(policy[state])\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        print(env.render())\n",
    "        time.sleep(1)\n",
    "\n",
    "simulate_agent(env, policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy Visualization:\n",
      "Initial State:\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  A  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 0, Action: 0, Reward: -1.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  A  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 1, Action: 0, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  A  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 2, Action: 0, Reward: -0.5\n",
      "P  H  H  .  .  .  .  A  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 3, Action: 2, Reward: -1.3\n",
      "P  H  H  .  .  .  A  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 4, Action: 2, Reward: -0.5\n",
      "P  H  H  .  .  A  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 5, Action: 2, Reward: -0.5\n",
      "P  H  H  .  A  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 6, Action: 2, Reward: -0.5\n",
      "P  H  H  A  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 7, Action: 2, Reward: 0.0\n",
      "P  H  A  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 8, Action: 2, Reward: 0.0\n",
      "P  A  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 9, Action: 2, Reward: 0.0\n",
      "A* H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 10, Action: 3, Reward: -0.8\n",
      "P  A* H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 11, Action: 3, Reward: 0.0\n",
      "P  H  A* .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 12, Action: 1, Reward: -1.3\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  A* .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 13, Action: 1, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  A* .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 14, Action: 1, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  A* .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 15, Action: 1, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  A* .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 16, Action: 1, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  A* .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 17, Action: 1, Reward: 0.0\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  A* .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 18, Action: 1, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  A* .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 19, Action: 1, Reward: 0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  A* S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 20, Action: 3, Reward: -0.30000000000000004\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  A* S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 21, Action: 3, Reward: 0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  A* .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 22, Action: 3, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  A* .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 23, Action: 3, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  A* .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 24, Action: 3, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  A* S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 25, Action: 3, Reward: 0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  A* S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 26, Action: 3, Reward: 0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  A* S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 27, Action: 3, Reward: 0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  A* .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 28, Action: 3, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  A* .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 29, Action: 3, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  A* .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 30, Action: 3, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  A* . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 31, Action: 3, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  A*\n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 32, Action: 1, Reward: -1.3\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  A*\n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 33, Action: 1, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  A*\n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 34, Action: 1, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  A*\n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 35, Action: 1, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  A*\n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 36, Action: 1, Reward: -0.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  A*\n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  D \n",
      "\n",
      "Step: 37, Action: 1, Reward: 499.5\n",
      "P  H  H  .  .  .  .  .  .  .  .  S  S  S  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  R  R  R  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  S  S  S  .  .  .  S  S  S  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      "H  H  H  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  . \n",
      ".  .  .  .  .  .  .  .  .  .  .  .  .  .  A*\n",
      "\n",
      "Episode finished!\n",
      "Performance Metrics:\n",
      "  Total Reward: 485.0\n",
      "  Steps Taken: 38\n",
      "  Safety Incidents: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class CustomTaxiMDP(gym.Env):\n",
    "    \"\"\"\n",
    "    A customizable taxi MDP environment with:\n",
    "    - Variable grid size\n",
    "    - Custom pickup and drop-off locations\n",
    "    - No explicit pickup/dropoff actions; entering pickup cell picks up passenger, entering dropoff cell completes the ride.\n",
    "    - Safety incidents do not terminate the episode but penalize the agent.\n",
    "    - Road types modify movement rewards.\n",
    "    - Balanced rewards: large final dropoff reward, scenic rewards not exploitable indefinitely.\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = {\"render_modes\": [\"ansi\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_rows=4,\n",
    "        grid_cols=5,\n",
    "        pickup_loc=(0,0),\n",
    "        dropoff_loc=(4,4),\n",
    "        incident_prob=0.001,         # base probability of a safety incident each step\n",
    "        unsafe_squares=[],\n",
    "        unsafe_incident_prob=0.01,   # additional probability if on unsafe square\n",
    "        scenic_squares=[],\n",
    "        scenic_reward=0.5,\n",
    "        step_cost=-1,\n",
    "        incident_cost=-10,           # cost for a safety incident\n",
    "        turn_cost=-0.1,              # penalty for changing direction\n",
    "        straight_reward=0.0,         # reward for going straight\n",
    "        road_types=None,             # {(r,c): \"highway\"/\"rough\"/\"scenic\"/...}\n",
    "        final_dropoff_reward=50.0,   # Large reward for completing dropoff\n",
    "        seed=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.grid_rows = grid_rows\n",
    "        self.grid_cols = grid_cols\n",
    "        self.pickup_loc = pickup_loc\n",
    "        self.dropoff_loc = dropoff_loc\n",
    "        self.incident_prob = incident_prob\n",
    "        self.unsafe_squares = set(unsafe_squares)\n",
    "        self.unsafe_incident_prob = unsafe_incident_prob\n",
    "        self.scenic_squares = set(scenic_squares)\n",
    "        self.scenic_reward = scenic_reward\n",
    "        self.step_cost = step_cost\n",
    "        self.incident_cost = incident_cost\n",
    "        self.turn_cost = turn_cost\n",
    "        self.straight_reward = straight_reward\n",
    "        self.final_dropoff_reward = final_dropoff_reward\n",
    "\n",
    "        if road_types is None:\n",
    "            self.road_types = {}\n",
    "        else:\n",
    "            self.road_types = road_types\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        # State representation: (agent_row, agent_col, passenger_picked_up, last_action)\n",
    "        self.observation_space = spaces.MultiDiscrete([self.grid_rows, self.grid_cols, 2, 5])\n",
    "        \n",
    "        self.np_random, _ = gym.utils.seeding.np_random(seed)\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "        # Build the transition model for planning\n",
    "        self._build_transition_model()\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.np_random, _ = gym.utils.seeding.np_random(seed)\n",
    "        self.agent_row = self.np_random.integers(self.grid_rows)\n",
    "        self.agent_col = self.np_random.integers(self.grid_cols)\n",
    "        \n",
    "        # Ensure not starting at dropoff location\n",
    "        while (self.agent_row, self.agent_col) == self.dropoff_loc:\n",
    "            self.agent_row = self.np_random.integers(self.grid_rows)\n",
    "            self.agent_col = self.np_random.integers(self.grid_cols)\n",
    "        \n",
    "        self.passenger_picked_up = False\n",
    "        self.last_action = -1\n",
    "        self.terminated = False\n",
    "\n",
    "        # Performance metrics\n",
    "        self.total_reward = 0.0\n",
    "        self.steps = 0\n",
    "        self.incidents_count = 0\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return (self.agent_row, self.agent_col, int(self.passenger_picked_up), self.last_action+1)\n",
    "\n",
    "    def _compute_road_type_reward(self, r, c):\n",
    "        rt = self.road_types.get((r, c), None)\n",
    "        # Example logic:\n",
    "        if rt == \"highway\":\n",
    "            return 0.5  # partially offsets the step cost\n",
    "        elif rt == \"rough\":\n",
    "            return -0.5 # makes movement more costly\n",
    "        return 0.0\n",
    "\n",
    "    def step(self, action):\n",
    "        prev_action = self.last_action\n",
    "\n",
    "        # Move\n",
    "        new_row = self.agent_row\n",
    "        new_col = self.agent_col\n",
    "        if action == 0 and self.agent_row > 0:\n",
    "            new_row -= 1\n",
    "        elif action == 1 and self.agent_row < self.grid_rows - 1:\n",
    "            new_row += 1\n",
    "        elif action == 2 and self.agent_col > 0:\n",
    "            new_col -= 1\n",
    "        elif action == 3 and self.agent_col < self.grid_cols - 1:\n",
    "            new_col += 1\n",
    "        \n",
    "        self.agent_row = new_row\n",
    "        self.agent_col = new_col\n",
    "        \n",
    "        # Base reward\n",
    "        reward = self.step_cost + self._compute_road_type_reward(new_row, new_col)\n",
    "\n",
    "        # Turn or straight\n",
    "        if prev_action != -1 and prev_action != action:\n",
    "            reward += self.turn_cost\n",
    "        elif prev_action != -1 and prev_action == action:\n",
    "            reward += self.straight_reward\n",
    "            \n",
    "        # Pickup\n",
    "        if (self.agent_row, self.agent_col) == self.pickup_loc and not self.passenger_picked_up:\n",
    "            self.passenger_picked_up = True\n",
    "        \n",
    "        # Scenic\n",
    "        if (self.agent_row, self.agent_col) in self.scenic_squares:\n",
    "            reward += self.scenic_reward\n",
    "        \n",
    "        # Check dropoff\n",
    "        if (self.agent_row, self.agent_col) == self.dropoff_loc and self.passenger_picked_up:\n",
    "            # Add large final reward\n",
    "            reward += self.final_dropoff_reward\n",
    "            self.terminated = True\n",
    "        \n",
    "        # Incident Probability\n",
    "        local_incident_prob = self.incident_prob\n",
    "        if (self.agent_row, self.agent_col) in self.unsafe_squares:\n",
    "            local_incident_prob += self.unsafe_incident_prob\n",
    "        \n",
    "        # Incident\n",
    "        if self.np_random.random() < local_incident_prob:\n",
    "            reward += self.incident_cost\n",
    "            self.incidents_count += 1\n",
    "        \n",
    "        self.last_action = action\n",
    "        self.total_reward += reward\n",
    "        self.steps += 1\n",
    "\n",
    "        return self._get_obs(), reward, self.terminated, False, {}\n",
    "\n",
    "    def render(self, mode='ansi'):\n",
    "        grid = np.full((self.grid_rows, self.grid_cols), '.', dtype=object)\n",
    "        \n",
    "        for (r,c) in self.unsafe_squares:\n",
    "            grid[r,c] = 'U'\n",
    "        for (r,c) in self.scenic_squares:\n",
    "            if grid[r,c] == 'U':\n",
    "                grid[r,c] = 'US'\n",
    "            else:\n",
    "                grid[r,c] = 'S'\n",
    "        \n",
    "        for (rc, cc), rt in self.road_types.items():\n",
    "            if rt == 'highway':\n",
    "                symbol = 'H'\n",
    "            elif rt == 'rough':\n",
    "                symbol = 'R'\n",
    "            else:\n",
    "                symbol = grid[rc, cc]\n",
    "            if grid[rc,cc] not in ['.', 'H', 'R', 'S', 'U', 'US']:\n",
    "                symbol = grid[rc,cc] + rt[0].upper()\n",
    "            grid[rc, cc] = symbol\n",
    "\n",
    "        pr, pc = self.pickup_loc\n",
    "        grid[pr, pc] = 'P'\n",
    "        dr, dc = self.dropoff_loc\n",
    "        grid[dr, dc] = 'D'\n",
    "        \n",
    "        ar, ac = self.agent_row, self.agent_col\n",
    "        agent_marker = 'A*' if self.passenger_picked_up else 'A'\n",
    "        grid[ar, ac] = agent_marker\n",
    "        \n",
    "        output = ''\n",
    "        for r in range(self.grid_rows):\n",
    "            row_str = ' '.join(f\"{cell:2}\" for cell in grid[r,:])\n",
    "            output += row_str + '\\n'\n",
    "        return output\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def _state_to_tuple(self, s):\n",
    "        return tuple(s)\n",
    "\n",
    "    def _get_all_states(self):\n",
    "        states = []\n",
    "        for r in range(self.grid_rows):\n",
    "            for c in range(self.grid_cols):\n",
    "                for pu in [0,1]:\n",
    "                    for la in range(5):\n",
    "                        states.append((r,c,pu,la))\n",
    "        return states\n",
    "\n",
    "    def _build_transition_model(self):\n",
    "        self.P = {}\n",
    "        all_states = self._get_all_states()\n",
    "        for s in all_states:\n",
    "            self.P[s] = {}\n",
    "            for a in range(self.action_space.n):\n",
    "                obs, reward, done = self._simulate_transition(s, a)\n",
    "                self.P[s][a] = (obs, reward, done)\n",
    "    \n",
    "    def _simulate_transition(self, s, a):\n",
    "        (r,c,pu,la) = s\n",
    "        last_action = la - 1\n",
    "        \n",
    "        new_r, new_c = r,c\n",
    "        if a == 0 and r > 0:\n",
    "            new_r -= 1\n",
    "        elif a == 1 and r < self.grid_rows - 1:\n",
    "            new_r += 1\n",
    "        elif a == 2 and c > 0:\n",
    "            new_c -= 1\n",
    "        elif a == 3 and c < self.grid_cols - 1:\n",
    "            new_c += 1\n",
    "\n",
    "        reward = self.step_cost + self._compute_road_type_reward(new_r, new_c)\n",
    "        \n",
    "        if last_action != -1 and last_action != a:\n",
    "            reward += self.turn_cost\n",
    "        elif last_action != -1 and last_action == a:\n",
    "            reward += self.straight_reward\n",
    "        \n",
    "        passenger_picked = (pu == 1)\n",
    "        \n",
    "        if (new_r, new_c) == self.pickup_loc and not passenger_picked:\n",
    "            passenger_picked = True\n",
    "        \n",
    "        if (new_r, new_c) in self.scenic_squares:\n",
    "            reward += self.scenic_reward\n",
    "        \n",
    "        done = False\n",
    "        if (new_r, new_c) == self.dropoff_loc and passenger_picked:\n",
    "            reward += self.final_dropoff_reward\n",
    "            done = True\n",
    "\n",
    "        local_incident_prob = self.incident_prob\n",
    "        if (new_r, new_c) in self.unsafe_squares:\n",
    "            local_incident_prob += self.unsafe_incident_prob\n",
    "        \n",
    "        reward += local_incident_prob * self.incident_cost\n",
    "        \n",
    "        new_pu = 1 if passenger_picked else 0\n",
    "        new_la = a + 1\n",
    "        next_s = (new_r, new_c, new_pu, new_la)\n",
    "        \n",
    "        return next_s, reward, done\n",
    "\n",
    "    def compute_optimal_policy(self, gamma=0.99, theta=1e-6):\n",
    "        all_states = self._get_all_states()\n",
    "        V = {s:0.0 for s in all_states}\n",
    "\n",
    "        def is_terminal(s):\n",
    "            (r,c,pu,la) = s\n",
    "            if (r,c) == self.dropoff_loc and pu == 1:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in all_states:\n",
    "                if is_terminal(s):\n",
    "                    continue\n",
    "                v = V[s]\n",
    "                q_values = []\n",
    "                for a in range(self.action_space.n):\n",
    "                    (next_s, r, done) = self.P[s][a]\n",
    "                    q_val = r + gamma*(0 if done else V[next_s])\n",
    "                    q_values.append(q_val)\n",
    "                V[s] = max(q_values)\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "        \n",
    "        pi = {}\n",
    "        for s in all_states:\n",
    "            if is_terminal(s):\n",
    "                pi[s] = 0\n",
    "                continue\n",
    "            q_values = []\n",
    "            for a in range(self.action_space.n):\n",
    "                (next_s, r, done) = self.P[s][a]\n",
    "                q_val = r + gamma*(0 if done else V[next_s])\n",
    "                q_values.append(q_val)\n",
    "            pi[s] = np.argmax(q_values)\n",
    "        \n",
    "        return V, pi\n",
    "\n",
    "    def visualize_policy(self, pi, max_steps=50):\n",
    "        obs, info = self.reset()\n",
    "        step_count = 0\n",
    "        print(\"Initial State:\")\n",
    "        print(self.render())\n",
    "        while step_count < max_steps:\n",
    "            s = tuple(obs)\n",
    "            a = pi[s]\n",
    "            obs, r, done, truncated, _ = self.step(a)\n",
    "            print(f\"Step: {step_count}, Action: {a}, Reward: {r}\")\n",
    "            print(self.render())\n",
    "            if done:\n",
    "                print(\"Episode finished!\")\n",
    "                break\n",
    "            step_count += 1\n",
    "        if step_count >= max_steps:\n",
    "            print(\"Reached max steps without termination.\")\n",
    "\n",
    "        print(\"Performance Metrics:\")\n",
    "        print(f\"  Total Reward: {self.total_reward}\")\n",
    "        print(f\"  Steps Taken: {self.steps}\")\n",
    "        print(f\"  Safety Incidents: {self.incidents_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Updated realistic configuration with balanced rewards\n",
    "    env = CustomTaxiMDP(\n",
    "        grid_rows=15,\n",
    "        grid_cols=15,\n",
    "        pickup_loc=(0, 0),\n",
    "        dropoff_loc=(14, 14),\n",
    "        unsafe_squares=[\n",
    "            (2, 5), (2, 6), (2, 7),\n",
    "            (3, 5), (3, 6), (3, 7),\n",
    "            (4, 5), (4, 6), (4, 7)\n",
    "        ],\n",
    "        scenic_squares=[\n",
    "            (0, 11), (0, 12), (0, 13),\n",
    "            (8, 2), (8, 3), (8, 4),\n",
    "            (8, 8), (8, 9), (8, 10)\n",
    "        ],\n",
    "        incident_prob=0.001,\n",
    "        unsafe_incident_prob=0.02,\n",
    "        scenic_reward=1.0,         # Scenic gives +1 but step cost is -1, net 0 if staying put\n",
    "        step_cost=-1,\n",
    "        incident_cost=-20,\n",
    "        turn_cost=-0.3,\n",
    "        straight_reward=0.5,\n",
    "        final_dropoff_reward=500.0, # Large final reward to ensure completing the trip is best\n",
    "        road_types={\n",
    "            # Highways (just some examples)\n",
    "            (0, 0): 'highway', (0, 1): 'highway', (0, 2): 'highway', \n",
    "            (6, 0): 'highway', (6, 1): 'highway', (6, 2): 'highway',\n",
    "            (11, 0): 'highway', (11, 1): 'highway', (11, 2): 'highway',\n",
    "\n",
    "            # Rough roads\n",
    "            (2, 5): 'rough', (2, 6): 'rough', (2, 7): 'rough',\n",
    "            (3, 5): 'rough', (3, 6): 'rough', (3, 7): 'rough',\n",
    "            (4, 5): 'rough', (4, 6): 'rough', (4, 7): 'rough',\n",
    "\n",
    "            # Scenic roads\n",
    "            (0, 11): 'scenic', (0, 12): 'scenic', (0, 13): 'scenic',\n",
    "            (8, 2): 'scenic', (8, 3): 'scenic', (8, 4): 'scenic',\n",
    "            (8, 8): 'scenic', (8, 9): 'scenic', (8, 10): 'scenic'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    V, pi = env.compute_optimal_policy(gamma=0.99, theta=1e-6)\n",
    "    print(\"Optimal Policy Visualization:\")\n",
    "    env.visualize_policy(pi, max_steps=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pygame\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class RoadNetworkMDP:\n",
    "    def __init__(self, gamma=0.95, driver_error_prob=0.1, delay_prob=0.1):\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.gamma = gamma\n",
    "        self.driver_error_prob = driver_error_prob\n",
    "        self.delay_prob = delay_prob\n",
    "\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.state_to_idx = {}\n",
    "        self.action_to_idx = {}\n",
    "        self.feature_map = {}\n",
    "\n",
    "    def add_state(self, state_id, features):\n",
    "        self.graph.add_node(state_id, **features)\n",
    "\n",
    "    def add_action(self, from_state, to_state, action):\n",
    "        # Only local connections already ensured outside (no random edges).\n",
    "        self.graph.add_edge(from_state, to_state, action=action)\n",
    "\n",
    "    def set_rewards(self, reward_weights):\n",
    "        for u, v, data in self.graph.edges(data=True):\n",
    "            from_features = self.graph.nodes[u]\n",
    "            reward = sum(reward_weights.get(k,0)*from_features.get(k,0) for k in reward_weights)\n",
    "            data[\"reward\"] = reward\n",
    "\n",
    "    def get_states(self):\n",
    "        return list(self.graph.nodes)\n",
    "\n",
    "    def get_actions(self, s):\n",
    "        return list(self.graph.successors(s))\n",
    "\n",
    "    def is_terminal(self, s, goal):\n",
    "        return s == goal\n",
    "\n",
    "    def transition(self, s, a):\n",
    "        successors = list(self.graph.successors(s))\n",
    "        if self.is_terminal(s, a):\n",
    "            intended_reward = self.graph[s][a][\"reward\"]\n",
    "        else:\n",
    "            intended_reward = self.graph[s][a][\"reward\"] if self.graph.has_edge(s,a) else 0\n",
    "\n",
    "        if len(successors) == 0:\n",
    "            return s, 0.0, True\n",
    "\n",
    "        p_delay = self.delay_prob\n",
    "        p_error = self.driver_error_prob if len(successors) > 1 else 0.0\n",
    "        p_intended = 1.0 - p_delay - p_error\n",
    "        if p_intended < 0:\n",
    "            p_intended = 0.0\n",
    "\n",
    "        rand_val = random.random()\n",
    "        if rand_val < p_delay:\n",
    "            return s, 0.0, self.is_terminal(s, a)\n",
    "        elif rand_val < p_delay + p_error and len(successors) > 1:\n",
    "            possible_errors = [x for x in successors if x != a]\n",
    "            chosen = random.choice(possible_errors)\n",
    "            r = self.graph[s][chosen][\"reward\"] if self.graph.has_edge(s,chosen) else 0\n",
    "            return chosen, r, self.is_terminal(chosen, a)\n",
    "        else:\n",
    "            return a, intended_reward, self.is_terminal(a, a)\n",
    "\n",
    "    # Value iteration code remains, but we won't use the resulting policy.\n",
    "    def value_iteration(self, goal, theta=1e-5):\n",
    "        states = self.get_states()\n",
    "        V = {s:0.0 for s in states}\n",
    "\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in states:\n",
    "                if self.is_terminal(s, goal):\n",
    "                    V[s] = 0.0\n",
    "                    continue\n",
    "                actions = self.get_actions(s)\n",
    "                if not actions:\n",
    "                    V[s] = 0.0\n",
    "                    continue\n",
    "                q_values = []\n",
    "                for a in actions:\n",
    "                    successors = list(self.graph.successors(s))\n",
    "                    p_delay = self.delay_prob\n",
    "                    p_error = self.driver_error_prob if len(successors) > 1 else 0.0\n",
    "                    p_intended = 1.0 - p_delay - p_error\n",
    "                    if p_intended < 0:\n",
    "                        p_intended = 0.0\n",
    "\n",
    "                    r_intended = self.graph[s][a][\"reward\"] if self.graph.has_edge(s,a) else 0\n",
    "                    v_intended = r_intended + self.gamma*V[a]\n",
    "\n",
    "                    v_delay = self.gamma*V[s]\n",
    "\n",
    "                    v_error = 0.0\n",
    "                    if p_error > 0:\n",
    "                        possible_errors = [x for x in successors if x != a]\n",
    "                        if possible_errors:\n",
    "                            error_vals = []\n",
    "                            for e_succ in possible_errors:\n",
    "                                r_err = self.graph[s][e_succ][\"reward\"]\n",
    "                                error_vals.append(r_err + self.gamma*V[e_succ])\n",
    "                            v_error = sum(error_vals)/len(error_vals)\n",
    "                        else:\n",
    "                            v_error = v_intended\n",
    "\n",
    "                    ev = p_delay*v_delay + p_error*v_error + p_intended*v_intended\n",
    "                    q_values.append(ev)\n",
    "\n",
    "                new_val = max(q_values)\n",
    "                delta = max(delta, abs(V[s]-new_val))\n",
    "                V[s] = new_val\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        policy = {}\n",
    "        for s in states:\n",
    "            if self.is_terminal(s, goal) or not self.get_actions(s):\n",
    "                policy[s] = None\n",
    "            else:\n",
    "                actions = self.get_actions(s)\n",
    "                best_act = None\n",
    "                best_val = float('-inf')\n",
    "                for a in actions:\n",
    "                    successors = list(self.graph.successors(s))\n",
    "                    p_delay = self.delay_prob\n",
    "                    p_error = self.driver_error_prob if len(successors) > 1 else 0.0\n",
    "                    p_intended = 1.0 - p_delay - p_error\n",
    "\n",
    "                    r_intended = self.graph[s][a][\"reward\"] if self.graph.has_edge(s,a) else 0\n",
    "                    v_intended = r_intended + self.gamma*V[a]\n",
    "                    v_delay = self.gamma*V[s]\n",
    "\n",
    "                    v_error = 0.0\n",
    "                    if p_error > 0:\n",
    "                        possible_errors = [x for x in successors if x != a]\n",
    "                        if possible_errors:\n",
    "                            error_vals = []\n",
    "                            for e_succ in possible_errors:\n",
    "                                r_err = self.graph[s][e_succ][\"reward\"]\n",
    "                                error_vals.append(r_err + self.gamma*V[e_succ])\n",
    "                            v_error = sum(error_vals)/len(error_vals)\n",
    "                        else:\n",
    "                            v_error = v_intended\n",
    "\n",
    "                    ev = p_delay*v_delay + p_error*v_error + p_intended*v_intended\n",
    "                    if ev > best_val:\n",
    "                        best_val = ev\n",
    "                        best_act = a\n",
    "                policy[s] = best_act\n",
    "        return V, policy\n",
    "\n",
    "    def build_indexing(self):\n",
    "        self.states = self.get_states()\n",
    "        self.state_to_idx = {s:i for i,s in enumerate(self.states)}\n",
    "\n",
    "        edge_actions = []\n",
    "        for s in self.states:\n",
    "            for a in self.get_actions(s):\n",
    "                edge_actions.append((s,a))\n",
    "        edge_actions = list(set(edge_actions))\n",
    "        self.actions = edge_actions\n",
    "        self.action_to_idx = {a:i for i,a in enumerate(self.actions)}\n",
    "\n",
    "    def set_feature_map(self):\n",
    "        for s in self.states:\n",
    "            node_data = self.graph.nodes[s]\n",
    "            f = np.array([node_data['length'], node_data['turn_penalty'], node_data['road_type_val']])\n",
    "            self.feature_map[s] = f\n",
    "\n",
    "    def compute_rewards_from_weights(self, weights):\n",
    "        R_s = np.zeros(len(self.states))\n",
    "        for s in self.states:\n",
    "            s_idx = self.state_to_idx[s]\n",
    "            R_s[s_idx] = np.dot(weights, self.feature_map[s])\n",
    "        return R_s\n",
    "\n",
    "    def to_transition_matrix(self):\n",
    "        N_S = len(self.states)\n",
    "        N_A = len(self.actions)\n",
    "        P_a = np.zeros((N_S, N_S, N_A))\n",
    "        R_sa = np.zeros((N_S, N_A))\n",
    "\n",
    "        for (orig_s, intended_next) in self.actions:\n",
    "            s_idx = self.state_to_idx[orig_s]\n",
    "            a_idx = self.action_to_idx[(orig_s,intended_next)]\n",
    "            successors = list(self.graph.successors(orig_s))\n",
    "\n",
    "            p_delay = self.delay_prob\n",
    "            p_error = self.driver_error_prob if len(successors) > 1 else 0.0\n",
    "            p_intended = 1.0 - p_delay - p_error\n",
    "            if p_intended < 0:\n",
    "                p_intended = 0.0\n",
    "\n",
    "            r_intended = self.graph[orig_s][intended_next][\"reward\"] if self.graph.has_edge(orig_s,intended_next) else 0\n",
    "\n",
    "            if p_delay > 0:\n",
    "                P_a[s_idx, s_idx, a_idx] += p_delay\n",
    "\n",
    "            if p_error > 0:\n",
    "                possible_errors = [x for x in successors if x != intended_next]\n",
    "                if possible_errors:\n",
    "                    p_each_error = p_error / len(possible_errors)\n",
    "                    for es in possible_errors:\n",
    "                        es_idx = self.state_to_idx[es]\n",
    "                        P_a[s_idx, es_idx, a_idx] += p_each_error\n",
    "\n",
    "            if p_intended > 0:\n",
    "                intended_idx = self.state_to_idx[intended_next]\n",
    "                P_a[s_idx, intended_idx, a_idx] += p_intended\n",
    "\n",
    "            r_error = 0.0\n",
    "            if p_error > 0 and possible_errors:\n",
    "                err_vals = []\n",
    "                for es in possible_errors:\n",
    "                    err_r = self.graph[orig_s][es][\"reward\"]\n",
    "                    err_vals.append(err_r)\n",
    "                r_error = sum(err_vals)/len(err_vals)\n",
    "\n",
    "            r_delay = 0.0\n",
    "            R_sa[s_idx, a_idx] = p_delay*r_delay + p_error*r_error + p_intended*r_intended\n",
    "\n",
    "        return P_a, R_sa\n",
    "\n",
    "    def generate_demonstrations(self, policy, n_trajs, max_length, start_state=None):\n",
    "        # not used in final loop, no changes here\n",
    "        demonstrations = []\n",
    "        for _ in range(n_trajs):\n",
    "            if start_state is None:\n",
    "                s = random.choice(self.states)\n",
    "            else:\n",
    "                s = start_state\n",
    "            traj = []\n",
    "            for _ in range(max_length):\n",
    "                if policy[s] is None:\n",
    "                    break\n",
    "                a = policy[s]\n",
    "                successors = list(self.graph.successors(s))\n",
    "                p_delay = self.delay_prob\n",
    "                p_error = self.driver_error_prob if len(successors) > 1 else 0.0\n",
    "                p_intended = 1.0 - p_delay - p_error\n",
    "                if p_intended < 0:\n",
    "                    p_intended = 0.0\n",
    "                r_intended = self.graph[s][a][\"reward\"] if self.graph.has_edge(s,a) else 0\n",
    "\n",
    "                outcomes = []\n",
    "                if p_delay > 0:\n",
    "                    outcomes.append((p_delay, s, 0.0, self.is_terminal(s,a)))\n",
    "                if p_error > 0:\n",
    "                    possible_errors = [x for x in successors if x != a]\n",
    "                    if possible_errors:\n",
    "                        p_err_each = p_error/len(possible_errors)\n",
    "                        for es in possible_errors:\n",
    "                            err_r = self.graph[s][es][\"reward\"] if self.graph.has_edge(s,es) else 0\n",
    "                            outcomes.append((p_err_each, es, err_r, self.is_terminal(es,a)))\n",
    "                    else:\n",
    "                        outcomes.append((p_error, a, r_intended, self.is_terminal(a,a)))\n",
    "                if p_intended > 0:\n",
    "                    outcomes.append((p_intended, a, r_intended, self.is_terminal(a,a)))\n",
    "\n",
    "                total_p = sum(x[0] for x in outcomes)\n",
    "                if total_p > 0:\n",
    "                    outcomes = [(p/total_p, ns, rr, dd) for (p,ns,rr,dd) in outcomes]\n",
    "                else:\n",
    "                    outcomes = [(1.0, s, 0.0, True)]\n",
    "\n",
    "                probs = [x[0] for x in outcomes]\n",
    "                idx = np.random.choice(len(outcomes), p=probs)\n",
    "                p_choice, s_next, r, done = outcomes[idx]\n",
    "\n",
    "                traj.append((s, a, s_next, r))\n",
    "                s = s_next\n",
    "                if done:\n",
    "                    break\n",
    "            demonstrations.append(traj)\n",
    "        return demonstrations\n",
    "\n",
    "    def compute_state_visitation_frequencies(self, policy, horizon=50):\n",
    "        if not self.states:\n",
    "            self.build_indexing()\n",
    "        P_a, R_sa = self.to_transition_matrix()\n",
    "\n",
    "        pi = np.zeros((len(self.states), len(self.actions)))\n",
    "        for s in self.states:\n",
    "            s_idx = self.state_to_idx[s]\n",
    "            chosen_a = policy[s]\n",
    "            if chosen_a is None:\n",
    "                continue\n",
    "            a_idx = self.action_to_idx[(s, chosen_a)]\n",
    "            pi[s_idx, a_idx] = 1.0\n",
    "\n",
    "        N_S = len(self.states)\n",
    "        P_pi = np.zeros((N_S, N_S))\n",
    "        for s_idx in range(N_S):\n",
    "            for a_idx in range(len(self.actions)):\n",
    "                for s_next_idx in range(N_S):\n",
    "                    P_pi[s_next_idx, s_idx] += pi[s_idx,a_idx]*P_a[s_idx,s_next_idx,a_idx]\n",
    "\n",
    "        mu = np.ones(N_S)/N_S\n",
    "        svf = np.zeros(N_S)\n",
    "        for t in range(horizon):\n",
    "            svf += mu\n",
    "            mu = P_pi.T.dot(mu)\n",
    "        svf /= horizon\n",
    "        return svf\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Pygame Visualization and main example usage\n",
    "# ---------------------------------------------\n",
    "pygame.init()\n",
    "WIDTH, HEIGHT = 1000, 800\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"Road Network MDP\")\n",
    "\n",
    "WHITE = (245,245,245)\n",
    "BLACK = (30,30,30)\n",
    "BLUE = (168,216,234)\n",
    "GREEN = (118,200,147)\n",
    "RED = (242,132,130)\n",
    "YELLOW = (246,215,167)\n",
    "GRAY = (200,200,200)\n",
    "PATH_COLOR = (203,170,203)\n",
    "\n",
    "font = pygame.font.SysFont(\"Arial\", 24, bold=False)\n",
    "car_image = pygame.image.load(\"data/car.png\")\n",
    "car_image = pygame.transform.scale(car_image, (30, 15))\n",
    "\n",
    "def draw_nodes(screen, node_positions, graph, start, goal, visited=set(), hover_node=None):\n",
    "    for node, pos in node_positions.items():\n",
    "        if node == start:\n",
    "            color = GREEN\n",
    "        elif node == goal:\n",
    "            color = RED\n",
    "        elif node in visited:\n",
    "            color = YELLOW\n",
    "        else:\n",
    "            color = BLUE\n",
    "        pygame.draw.circle(screen, color, pos, 15, 0)\n",
    "\n",
    "    if hover_node is not None:\n",
    "        features = graph.nodes[hover_node]\n",
    "        info_text = f\"{hover_node}: len={features['length']:.2f}, turn={features['turn_penalty']:.2f}, rt={features['road_type_val']:.2f}\"\n",
    "        info_label = font.render(info_text, True, BLACK)\n",
    "        mouse_x, mouse_y = node_positions[hover_node]\n",
    "        pygame.draw.rect(screen, GRAY, (mouse_x, mouse_y-50, info_label.get_width()+10, info_label.get_height()+10), border_radius=5)\n",
    "        screen.blit(info_label, (mouse_x+5, mouse_y - 50 + 5))\n",
    "\n",
    "def draw_edges(screen, node_positions, graph, threshold=-1, path_edges=set()):\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        start = node_positions[u]\n",
    "        end = node_positions[v]\n",
    "        if (u,v) in path_edges:\n",
    "            edge_color = PATH_COLOR\n",
    "            line_width = 3\n",
    "        else:\n",
    "            edge_color = RED if data[\"reward\"] > threshold else BLACK\n",
    "            line_width = 2\n",
    "\n",
    "        # Just draw a line, no arrow head\n",
    "        pygame.draw.aaline(screen, edge_color, start, end)\n",
    "\n",
    "        # Reward annotation\n",
    "        mid_x = (start[0]+end[0])//2\n",
    "        mid_y = (start[1]+end[1])//2\n",
    "        reward_text = f\"{data['reward']:.1f}\"\n",
    "        rw_label = font.render(reward_text, True, BLACK)\n",
    "        screen.blit(rw_label, (int(mid_x) - rw_label.get_width()//2, int(mid_y) - rw_label.get_height()//2))\n",
    "\n",
    "def run_episode(mdp, start, goal, node_positions):\n",
    "    # Soft policy: choose a random available action at each step\n",
    "    s = start\n",
    "    visited = set()\n",
    "    path = [s]\n",
    "    total_return = 0.0\n",
    "    steps = 100\n",
    "    for _ in range(steps):\n",
    "        actions = mdp.get_actions(s)\n",
    "        if not actions or mdp.is_terminal(s, goal):\n",
    "            break\n",
    "        a = random.choice(actions)\n",
    "        s_next, r, done = mdp.transition(s, a)\n",
    "        total_return += r\n",
    "        path.append(s_next)\n",
    "        s = s_next\n",
    "        if done:\n",
    "            break\n",
    "    return path, total_return\n",
    "\n",
    "def animate_trajectory(screen, path, node_positions, graph, path_edges, total_return):\n",
    "    visited = set()\n",
    "    steps = 100\n",
    "    for i in range(len(path)-1):\n",
    "        start_pos = node_positions[path[i]]\n",
    "        end_pos = node_positions[path[i+1]]\n",
    "        for t in range(steps):\n",
    "            x = start_pos[0] + (end_pos[0]-start_pos[0])*t/steps\n",
    "            y = start_pos[1] + (end_pos[1]-start_pos[1])*t/steps\n",
    "            visited.add(path[i])\n",
    "            screen.fill(WHITE)\n",
    "            draw_nodes(screen, node_positions, graph, path[0], path[-1], visited)\n",
    "            draw_edges(screen, node_positions, graph, path_edges=path_edges)\n",
    "            screen.blit(car_image, (int(x)-15, int(y)-7))\n",
    "            info_label = font.render(\"Animating Trajectory...\", True, BLACK)\n",
    "            screen.blit(info_label, (10,10))\n",
    "            pygame.display.flip()\n",
    "            pygame.time.delay(20)\n",
    "    visited.add(path[-1])\n",
    "    screen.fill(WHITE)\n",
    "    draw_nodes(screen, node_positions, graph, path[0], path[-1], visited)\n",
    "    draw_edges(screen, node_positions, graph, path_edges=path_edges)\n",
    "    final_pos = node_positions[path[-1]]\n",
    "    screen.blit(car_image, (int(final_pos[0])-15, int(final_pos[1])-7))\n",
    "    # Show total return\n",
    "    return_label = font.render(f\"Episode Return: {total_return:.2f}\", True, BLACK)\n",
    "    screen.blit(return_label, (10,40))\n",
    "    pygame.display.flip()\n",
    "    pygame.time.delay(1000)\n",
    "\n",
    "# Construct a grid MDP with only right and down edges\n",
    "mdp = RoadNetworkMDP(gamma=0.95, driver_error_prob=0.1, delay_prob=0.1)\n",
    "\n",
    "rows = 4\n",
    "cols = 5\n",
    "states = []\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "        s = f\"S{r*cols+c}\"\n",
    "        states.append(s)\n",
    "        features = {\n",
    "            \"length\": random.uniform(1,5),\n",
    "            \"turn_penalty\": random.uniform(0,1),\n",
    "            \"road_type_val\": random.choice([0.5,0.0,-0.5])\n",
    "        }\n",
    "        mdp.add_state(s, features)\n",
    "\n",
    "# Only local connections: right and down\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "        s = f\"S{r*cols+c}\"\n",
    "        if c < cols-1:\n",
    "            right = f\"S{r*cols+(c+1)}\"\n",
    "            mdp.add_action(s, right, \"Go straight\")\n",
    "        if r < rows-1:\n",
    "            down = f\"S{(r+1)*cols+c}\"\n",
    "            mdp.add_action(s, down, \"Go straight\")\n",
    "\n",
    "reward_weights = {\"length\": -1.0, \"turn_penalty\": -0.5, \"road_type_val\": 0.2}\n",
    "mdp.set_rewards(reward_weights)\n",
    "\n",
    "# Start and goal at opposite corners\n",
    "# Explicitly compute the start and goal positions\n",
    "start = f\"S0\"  # Top-left corner is always \"S0\"\n",
    "goal = f\"S{(rows-1)*cols + (cols-1)}\"  # Bottom-right corner index\n",
    "\n",
    "mdp.build_indexing()\n",
    "mdp.set_feature_map()\n",
    "\n",
    "spacing_x = WIDTH//(cols+1)\n",
    "spacing_y = HEIGHT//(rows+1)\n",
    "node_positions = {}\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "        s = f\"S{r*cols+c}\"\n",
    "        x = (c+1)*spacing_x\n",
    "        y = (r+1)*spacing_y\n",
    "        node_positions[s] = (x,y)\n",
    "\n",
    "running = True\n",
    "hover_node = None\n",
    "\n",
    "# Infinite loop of episodes\n",
    "while running:\n",
    "    # Run one episode with a random policy\n",
    "    path, total_return = run_episode(mdp, start, goal, node_positions)\n",
    "\n",
    "    # Highlight path edges\n",
    "    path_edges = set()\n",
    "    for i in range(len(path)-1):\n",
    "        path_edges.add((path[i], path[i+1]))\n",
    "\n",
    "    # Animate trajectory\n",
    "    animate_trajectory(screen, path, node_positions, mdp.graph, path_edges, total_return)\n",
    "\n",
    "    # After showing the return, start next episode immediately (no button press)\n",
    "    # If user closes window, break\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "\n",
    "pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import random\n",
    "\n",
    "# Initialize pygame\n",
    "pygame.init()\n",
    "\n",
    "# Set dimensions for debugging visualization\n",
    "WIDTH, HEIGHT = 600, 400\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"Grid Debugging Visualization\")\n",
    "\n",
    "# Colors\n",
    "WHITE = (245, 245, 245)\n",
    "BLUE = (168, 216, 234)\n",
    "GREEN = (118, 200, 147)\n",
    "RED = (242, 132, 130)\n",
    "BLACK = (30, 30, 30)\n",
    "\n",
    "# Font for labels\n",
    "font = pygame.font.SysFont(\"Arial\", 24, bold=False)\n",
    "\n",
    "# Create a grid layout\n",
    "rows, cols = 4, 5\n",
    "spacing_x = WIDTH // (cols + 1)\n",
    "spacing_y = HEIGHT // (rows + 1)\n",
    "\n",
    "# Define node positions\n",
    "node_positions = {}\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "        s = f\"S{r*cols + c}\"\n",
    "        x = (c + 1) * spacing_x\n",
    "        y = (r + 1) * spacing_y\n",
    "        node_positions[s] = (x, y)\n",
    "\n",
    "# Define the diagonally opposite corner pairs\n",
    "diagonal_pairs = [(\"S0\", \"S19\"), (\"S4\", \"S15\")]\n",
    "\n",
    "# Randomly select one diagonal pair\n",
    "start, goal = random.choice(diagonal_pairs)\n",
    "\n",
    "# Debugging visualization loop\n",
    "running = True\n",
    "while running:\n",
    "    screen.fill(WHITE)  # Clear the screen\n",
    "\n",
    "    # Draw nodes\n",
    "    for node, pos in node_positions.items():\n",
    "        if node == start:\n",
    "            color = GREEN  # Start node\n",
    "        elif node == goal:\n",
    "            color = RED  # Goal node\n",
    "        else:\n",
    "            color = BLUE  # Regular node\n",
    "        pygame.draw.circle(screen, color, pos, 15, 0)\n",
    "\n",
    "        # Draw node label\n",
    "        label = font.render(node, True, BLACK)\n",
    "        screen.blit(label, (pos[0] - label.get_width() // 2, pos[1] - label.get_height() // 2))\n",
    "\n",
    "    # Draw edges (right and down connections)\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            current = f\"S{r*cols + c}\"\n",
    "            if c < cols - 1:  # Right connection\n",
    "                right = f\"S{r*cols + (c + 1)}\"\n",
    "                pygame.draw.line(screen, BLACK, node_positions[current], node_positions[right], 2)\n",
    "            if r < rows - 1:  # Down connection\n",
    "                down = f\"S{(r + 1) * cols + c}\"\n",
    "                pygame.draw.line(screen, BLACK, node_positions[current], node_positions[down], 2)\n",
    "\n",
    "    # Refresh the display\n",
    "    pygame.display.flip()\n",
    "\n",
    "    # Event handling for quitting\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "\n",
    "pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node positions:\n",
      "S0: (166, 160)\n",
      "S1: (332, 160)\n",
      "S2: (498, 160)\n",
      "S3: (664, 160)\n",
      "S4: (830, 160)\n",
      "S5: (166, 320)\n",
      "S6: (332, 320)\n",
      "S7: (498, 320)\n",
      "S8: (664, 320)\n",
      "S9: (830, 320)\n",
      "S10: (166, 480)\n",
      "S11: (332, 480)\n",
      "S12: (498, 480)\n",
      "S13: (664, 480)\n",
      "S14: (830, 480)\n",
      "S15: (166, 640)\n",
      "S16: (332, 640)\n",
      "S17: (498, 640)\n",
      "S18: (664, 640)\n",
      "S19: (830, 640)\n"
     ]
    }
   ],
   "source": [
    "print(\"Node positions:\")\n",
    "for state, pos in node_positions.items():\n",
    "    print(f\"{state}: {pos}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
