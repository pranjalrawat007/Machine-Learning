{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main] Ensuring 'results/' directory exists.\n",
      "[main] Starting main execution...\n",
      "[GridMDP] Initializing with size=5, discount_factor=0.95, seed=42\n",
      "[GridMDP] Randomizing initial rewards in [0,1].\n",
      "[GridMDP] Setting start=(0,0) reward to 0.05 and goal=(size-1,size-1) reward to 1.0.\n",
      "[main] Created GridMDP shape=(5, 5), discount=0.95\n",
      "[main] Generating expert data...\n",
      "[generate_trajectories] N=10, max_steps=15, policy_type=soft, seed=42.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_policy] Building soft policy from value function.\n",
      "[generate_trajectories] Generated 10 trajectories (policy_type=soft).\n",
      "[save_trajectories_csv] Saved 10 trajectories to results/expert_data.csv.\n",
      "[main] Estimating NFXP rewards...\n",
      "[estimate_nfxp] Estimating reward via nested fixed-point approach.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[estimate_nfxp] L-BFGS took 28.10s for NxFP estimation.\n",
      "[save_rewards_csv] NFXP estimated rewards saved to results/nfxp_est_rewards.csv.\n",
      "[main] Estimating MaxEnt rewards...\n",
      "[maxent_estimation] Estimating reward via maximum entropy IRL.\n",
      "[compute_expert_feature_expectation] Building expert feature expectations.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[maxent] iter=0, loss=5.9400\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[maxent] iter=10, loss=5.1086\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[maxent] iter=20, loss=4.9411\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[maxent] iter=30, loss=4.9716\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[maxent] iter=40, loss=4.6168\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[maxent_estimation] completed in 8.67s\n",
      "[save_maxent_rewards_csv] MaxEnt estimated rewards saved to results/maxent_est_rewards.csv.\n",
      "[main] Computing evaluation metrics (RMSE, rank corr, policy diff).\n",
      "[standard_value_iteration] Running hard-optimal value iteration.\n",
      "[standard_value_iteration] Running hard-optimal value iteration.\n",
      "[standard_value_iteration] Running hard-optimal value iteration.\n",
      "[standard_value_iteration] Running hard-optimal value iteration.\n",
      "[main] Building route recommendations in unblocked scenario...\n",
      "[get_hard_greedy_path] Computing path with blocked_cells= None\n",
      "[get_hard_greedy_path] Computing path with blocked_cells= None\n",
      "[get_hard_greedy_path] Computing path with blocked_cells= None\n",
      "[main] Building route recommendations in blocked scenario...\n",
      "[get_hard_greedy_path] Computing path with blocked_cells= [(2, 2), (2, 3), (3, 2), (3, 3)]\n",
      "[get_hard_greedy_path] Computing path with blocked_cells= [(2, 2), (2, 3), (3, 2), (3, 3)]\n",
      "[get_hard_greedy_path] Computing path with blocked_cells= [(2, 2), (2, 3), (3, 2), (3, 3)]\n",
      "[main] Creating final policy comparison figure...\n",
      "[standard_value_iteration] Running hard-optimal value iteration.\n",
      "[standard_value_iteration] Running hard-optimal value iteration.\n",
      "[standard_value_iteration] Running hard-optimal value iteration.\n",
      "[main] Saved figure: results/reward_policy_comparison.png\n",
      "[main] Creating recommended paths figure (unblocked vs. blocked).\n",
      "[main] Saved figure: results/recommended_paths.png\n",
      "[main] Preparing final results table and saving to CSV.\n",
      "\n",
      "FINAL RESULTS\n",
      "\n",
      "|------------------|-------------------------|--------|-------------|--|\n",
      "| Hyperparams:     | K=5, N=10, max_steps=15 |        |             |  |\n",
      "| Method           | NFXP                    | MaxEnt | GroundTruth |  |\n",
      "| Time (sec)       | 28.096                  | 8.671  | 0.000       |  |\n",
      "| RMSE(Reward)     | 3.830                   | 3.809  | 0.000       |  |\n",
      "| RankCorr(Reward) | 0.469                   | 0.524  | 1.000       |  |\n",
      "| PolDiff(Reward)  | 0.200                   | 0.240  | 0.000       |  |\n",
      "| --Unblocked--    | NFXP                    | MaxEnt | GroundTruth |  |\n",
      "| Return (Est R)   | 1.084                   | 4.222  | 17.282      |  |\n",
      "| Return (True R)  | 15.816                  | 15.441 | 17.282      |  |\n",
      "| --Blocked--      | NFXP                    | MaxEnt | GroundTruth |  |\n",
      "| Return (Est R)   | 0.770                   | 4.045  | 16.999      |  |\n",
      "| Return (True R)  | 15.636                  | 15.143 | 16.999      |  |\n",
      "[main] Saved final results to results/final_results.csv\n",
      "[main] Done. See results above.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "A single-file Python script demonstrating a gridworld-based IRL experiment,\n",
    "with all outputs saved to a 'results/' folder.\n",
    "\n",
    "Sections (aligned with paper flow):\n",
    "1) Environment & Expert Data Generation\n",
    "2) IRL Algorithms (NFXP, MaxEnt)\n",
    "3) Route Recommendation via Standard (Hard) Value Iteration\n",
    "4) Metrics & Evaluation\n",
    "5) Main Execution\n",
    "\n",
    "Usage:\n",
    "  python main.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import logsumexp\n",
    "from scipy.stats import spearmanr\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Attempt to import tabulate for nicer table printing in final output\n",
    "try:\n",
    "    from tabulate import tabulate\n",
    "    USE_TABULATE = True\n",
    "except ImportError:\n",
    "    USE_TABULATE = False\n",
    "\n",
    "###############################################################################\n",
    "# Make sure a 'results/' folder exists to store outputs\n",
    "###############################################################################\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "print(\"[main] Ensuring 'results/' directory exists.\")\n",
    "\n",
    "###############################################################################\n",
    "# (1) ENVIRONMENT & EXPERT DATA GENERATION\n",
    "###############################################################################\n",
    "class GridMDP:\n",
    "    \"\"\"\n",
    "    A simple Grid MDP:\n",
    "      - Rewards in [0,1] for each cell, except start=0.05 and goal=1.0 (or higher).\n",
    "      - We'll gather 'expert' data from a soft-optimal policy on the TRUE rewards.\n",
    "    \"\"\"\n",
    "    def __init__(self, size=5, discount_factor=0.95, seed=42):\n",
    "        print(f\"[GridMDP] Initializing with size={size}, discount_factor={discount_factor}, seed={seed}\")\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        self.size = size\n",
    "        self.discount_factor = discount_factor\n",
    "        self.true_rewards = np.zeros((size, size), dtype=float)\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self._init_rewards()\n",
    "\n",
    "    def _init_rewards(self):\n",
    "        \"\"\"\n",
    "        Initialize cell-wise rewards in [0,1], with special start & goal cells.\n",
    "        \"\"\"\n",
    "        print(\"[GridMDP] Randomizing initial rewards in [0,1].\")\n",
    "        for r in range(self.size):\n",
    "            for c in range(self.size):\n",
    "                self.true_rewards[r, c] = np.random.rand()\n",
    "        print(\"[GridMDP] Setting start=(0,0) reward to 0.05 and goal=(size-1,size-1) reward to 1.0.\")\n",
    "        self.true_rewards[0, 0] = 0.05\n",
    "        self.true_rewards[self.size - 1, self.size - 1] = 1.0\n",
    "\n",
    "    def step(self, state: Tuple[int, int], action: str) -> Tuple[int, int]:\n",
    "        (r, c) = state\n",
    "        if action == 'up' and r > 0:\n",
    "            r -= 1\n",
    "        elif action == 'down' and r < self.size - 1:\n",
    "            r += 1\n",
    "        elif action == 'left' and c > 0:\n",
    "            c -= 1\n",
    "        elif action == 'right' and c < self.size - 1:\n",
    "            c += 1\n",
    "        return (r, c)\n",
    "\n",
    "    def get_reward(self, state: Tuple[int, int]) -> float:\n",
    "        return self.true_rewards[state]\n",
    "\n",
    "    def random_start_state(self) -> Tuple[int, int]:\n",
    "        r = random.randint(0, self.size - 1)\n",
    "        c = random.randint(0, self.size - 1)\n",
    "        return (r, c)\n",
    "\n",
    "    def shape(self) -> Tuple[int, int]:\n",
    "        return (self.size, self.size)\n",
    "\n",
    "    def valid_states(self) -> List[Tuple[int,int]]:\n",
    "        return [(r, c) for r in range(self.size) for c in range(self.size)]\n",
    "\n",
    "\n",
    "def soft_value_iteration(mdp: GridMDP,\n",
    "                        rewards_2d: np.ndarray,\n",
    "                        discount_factor: float,\n",
    "                        max_iter=200,\n",
    "                        tol=1e-7) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Soft Value Iteration to generate 'soft-optimal' data:\n",
    "      V(s) = log( sum_a exp( R(s') + gamma V(s') ) ).\n",
    "    \"\"\"\n",
    "    states = mdp.valid_states()\n",
    "    V_dict = {s: 0.0 for s in states}\n",
    "    print(f\"[soft_value_iteration] Starting with max_iter={max_iter}, tol={tol}.\")\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        V_old = dict(V_dict)\n",
    "        delta = 0.0\n",
    "        for s in states:\n",
    "            vs = []\n",
    "            for a in mdp.actions:\n",
    "                s_next = mdp.step(s, a)\n",
    "                r_next = rewards_2d[s_next]\n",
    "                vs.append(r_next + discount_factor * V_old[s_next])\n",
    "            new_val = logsumexp(vs)\n",
    "            delta = max(delta, abs(new_val - V_dict[s]))\n",
    "            V_dict[s] = new_val\n",
    "        if delta < tol:\n",
    "            print(f\"[soft_value_iteration] Converged after {it+1} iterations (delta={delta:.3g}).\")\n",
    "            break\n",
    "\n",
    "    V_arr = np.zeros((mdp.size, mdp.size), dtype=float)\n",
    "    for s in states:\n",
    "        V_arr[s] = V_dict[s]\n",
    "    return V_arr\n",
    "\n",
    "\n",
    "def soft_policy(mdp: GridMDP, V_2d: np.ndarray, rewards_2d: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Pi(a|s) ~ exp( R(s') + gamma * V(s') )\n",
    "    \"\"\"\n",
    "    states = mdp.valid_states()\n",
    "    pol = {}\n",
    "    print(\"[soft_policy] Building soft policy from value function.\")\n",
    "    for s in states:\n",
    "        vs = []\n",
    "        for a in mdp.actions:\n",
    "            s_next = mdp.step(s, a)\n",
    "            r_next = rewards_2d[s_next]\n",
    "            vs.append(r_next + mdp.discount_factor * V_2d[s_next])\n",
    "        log_probs = vs - logsumexp(vs)\n",
    "        pol[s] = dict(zip(mdp.actions, np.exp(log_probs)))\n",
    "    return pol\n",
    "\n",
    "\n",
    "def generate_trajectories(mdp: GridMDP,\n",
    "                          N=20,\n",
    "                          max_steps=15,\n",
    "                          policy_type=\"soft\",\n",
    "                          seed=42) -> List[List[Tuple[Tuple[int,int], str, float]]]:\n",
    "    \"\"\"\n",
    "    Creates 'expert' trajectories from either a soft or a deterministic optimal policy.\n",
    "    \"\"\"\n",
    "    print(f\"[generate_trajectories] N={N}, max_steps={max_steps}, policy_type={policy_type}, seed={seed}.\")\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Soft VI on the true reward\n",
    "    V_true = soft_value_iteration(mdp, mdp.true_rewards, mdp.discount_factor)\n",
    "    pi_soft = soft_policy(mdp, V_true, mdp.true_rewards)\n",
    "\n",
    "    # Possibly convert to a deterministic, greedy policy\n",
    "    if policy_type == \"optimal\":\n",
    "        pi_dict = {}\n",
    "        states = mdp.valid_states()\n",
    "        for s in states:\n",
    "            vs = []\n",
    "            for a in mdp.actions:\n",
    "                s_next = mdp.step(s, a)\n",
    "                vs.append(mdp.true_rewards[s_next] + mdp.discount_factor * V_true[s_next])\n",
    "            best_idx = np.argmax(vs)\n",
    "            best_action = mdp.actions[best_idx]\n",
    "            pi_dict[s] = {a: (1.0 if a == best_action else 0.0) for a in mdp.actions}\n",
    "    else:\n",
    "        pi_dict = pi_soft\n",
    "\n",
    "    trajectories = []\n",
    "    for i in range(N):\n",
    "        traj = []\n",
    "        s_current = mdp.random_start_state()\n",
    "        for t in range(max_steps):\n",
    "            acts = list(pi_dict[s_current].keys())\n",
    "            probs = list(pi_dict[s_current].values())\n",
    "            chosen_action = np.random.choice(acts, p=probs)\n",
    "            s_next = mdp.step(s_current, chosen_action)\n",
    "            r_next = mdp.get_reward(s_next)\n",
    "            traj.append((s_current, chosen_action, r_next))\n",
    "            s_current = s_next\n",
    "        trajectories.append(traj)\n",
    "\n",
    "    print(f\"[generate_trajectories] Generated {N} trajectories (policy_type={policy_type}).\")\n",
    "    return trajectories\n",
    "\n",
    "\n",
    "def save_trajectories_csv(trajectories, filename=\"results/expert_data.csv\"):\n",
    "    \"\"\"\n",
    "    Save trajectory data to CSV in 'results/' folder.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for i, traj in enumerate(trajectories):\n",
    "        for t, (s, a, r) in enumerate(traj):\n",
    "            rows.append([i, t, s[0], s[1], a, r])\n",
    "    df = pd.DataFrame(rows, columns=[\"trajectory_id\",\"step\",\"row\",\"col\",\"action\",\"reward\"])\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"[save_trajectories_csv] Saved {len(trajectories)} trajectories to {filename}.\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# (2) IRL ALGORITHMS (NFXP, MAXENT)\n",
    "###############################################################################\n",
    "def reshape_1d_to_2d(vec: np.ndarray, size: int) -> np.ndarray:\n",
    "    return vec.reshape((size, size))\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# NFXP\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "def nfxp_log_likelihood(param_vec: np.ndarray,\n",
    "                        mdp: GridMDP,\n",
    "                        trajectories) -> float:\n",
    "    \"\"\"\n",
    "    Negative log-likelihood for NFXP:\n",
    "      1) Soft VI on param_2d\n",
    "      2) Induce policy\n",
    "      3) Compare log-likelihood to expert data\n",
    "    \"\"\"\n",
    "    size = mdp.size\n",
    "    param_2d = reshape_1d_to_2d(param_vec, size)\n",
    "\n",
    "    V_2d = soft_value_iteration(mdp, param_2d, mdp.discount_factor)\n",
    "    pi_dict = {}\n",
    "    states = mdp.valid_states()\n",
    "    for s in states:\n",
    "        vs = []\n",
    "        for a in mdp.actions:\n",
    "            s_next = mdp.step(s, a)\n",
    "            r_next = param_2d[s_next]\n",
    "            vs.append(r_next + mdp.discount_factor * V_2d[s_next])\n",
    "        log_probs = vs - logsumexp(vs)\n",
    "        pi_dict[s] = np.exp(log_probs)\n",
    "\n",
    "    eps = 1e-12\n",
    "    total_ll = 0\n",
    "    for traj in trajectories:\n",
    "        for (s, a, _) in traj:\n",
    "            a_idx = mdp.actions.index(a)\n",
    "            total_ll += np.log(pi_dict[s][a_idx] + eps)\n",
    "\n",
    "    return -total_ll\n",
    "\n",
    "\n",
    "def estimate_nfxp(mdp: GridMDP,\n",
    "                  trajectories,\n",
    "                  max_iter=300) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Minimizes negative log-likelihood by L-BFGS-B with param in [0,1].\n",
    "    \"\"\"\n",
    "    print(\"[estimate_nfxp] Estimating reward via nested fixed-point approach.\")\n",
    "    size = mdp.size\n",
    "    param_init = np.random.uniform(0.0, 1.0, size*size)\n",
    "    bounds = [(0,1)] * (size*size)\n",
    "\n",
    "    t0 = time.time()\n",
    "    result = minimize(\n",
    "        nfxp_log_likelihood,\n",
    "        param_init,\n",
    "        args=(mdp, trajectories),\n",
    "        method='L-BFGS-B',\n",
    "        bounds=bounds,\n",
    "        options={'maxiter': max_iter, 'disp': False}\n",
    "    )\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"[estimate_nfxp] L-BFGS took {elapsed:.2f}s for NxFP estimation.\")\n",
    "\n",
    "    param_est = reshape_1d_to_2d(result.x, size)\n",
    "    return param_est\n",
    "\n",
    "\n",
    "def save_rewards_csv(reward_2d: np.ndarray, filename=\"results/nfxp_est_rewards.csv\"):\n",
    "    df = pd.DataFrame(reward_2d)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"[save_rewards_csv] NFXP estimated rewards saved to {filename}.\")\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# MAXENT\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "def collect_state_visitation(mdp: GridMDP,\n",
    "                             pi_dict: dict,\n",
    "                             discount_factor: float,\n",
    "                             T=15) -> dict:\n",
    "    \"\"\"\n",
    "    Approx. state visitation frequencies by random rollouts from pi_dict.\n",
    "    \"\"\"\n",
    "    freq = defaultdict(float)\n",
    "    n_roll = 1000\n",
    "    for _ in range(n_roll):\n",
    "        s = mdp.random_start_state()\n",
    "        for t in range(T):\n",
    "            freq[s] += (discount_factor ** t)\n",
    "            pvals = pi_dict[s]\n",
    "            a_idx = np.random.choice(len(pvals), p=pvals)\n",
    "            a = mdp.actions[a_idx]\n",
    "            s = mdp.step(s, a)\n",
    "    for s in freq:\n",
    "        freq[s] /= n_roll\n",
    "    return freq\n",
    "\n",
    "\n",
    "def feature_vector(mdp: GridMDP, s: Tuple[int,int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    1-hot representation for cell s -> dimension = size*size.\n",
    "    \"\"\"\n",
    "    size = mdp.size\n",
    "    vec = np.zeros(size*size)\n",
    "    idx = s[0]*size + s[1]\n",
    "    vec[idx] = 1.0\n",
    "    return vec\n",
    "\n",
    "\n",
    "def compute_expert_feature_expectation(mdp: GridMDP,\n",
    "                                       trajectories,\n",
    "                                       discount_factor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    mu_D = average_{trajectories} sum_{t} gamma^t * one_hot(s_t)\n",
    "    \"\"\"\n",
    "    print(\"[compute_expert_feature_expectation] Building expert feature expectations.\")\n",
    "    size = mdp.size\n",
    "    mu_D = np.zeros(size*size)\n",
    "    N = len(trajectories)\n",
    "    for traj in trajectories:\n",
    "        for t, (s, a, r) in enumerate(traj):\n",
    "            mu_D += (discount_factor ** t) * feature_vector(mdp, s)\n",
    "    mu_D /= float(N)\n",
    "    return mu_D\n",
    "\n",
    "\n",
    "def maxent_estimation(mdp: GridMDP,\n",
    "                      trajectories,\n",
    "                      discount_factor=0.95,\n",
    "                      learning_rate=0.01,\n",
    "                      max_iter=50) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Basic MaxEnt IRL with param in [0,1] for each cell, gradient-based matching of mu_D - mu_pi.\n",
    "    \"\"\"\n",
    "    print(\"[maxent_estimation] Estimating reward via maximum entropy IRL.\")\n",
    "    size = mdp.size\n",
    "    mu_D = compute_expert_feature_expectation(mdp, trajectories, discount_factor)\n",
    "    param = np.random.uniform(0,1,size*size)\n",
    "\n",
    "    t0 = time.time()\n",
    "    for it in range(max_iter):\n",
    "        R_2d = reshape_1d_to_2d(param, size)\n",
    "        V_2d = soft_value_iteration(mdp, R_2d, discount_factor)\n",
    "\n",
    "        pi_dict = {}\n",
    "        for s in mdp.valid_states():\n",
    "            vs = []\n",
    "            for a in mdp.actions:\n",
    "                s_next = mdp.step(s, a)\n",
    "                vs.append(R_2d[s_next] + discount_factor * V_2d[s_next])\n",
    "            log_probs = vs - logsumexp(vs)\n",
    "            pi_dict[s] = np.exp(log_probs)\n",
    "\n",
    "        freq = collect_state_visitation(mdp, pi_dict, discount_factor, T=15)\n",
    "        mu_pi = np.zeros(size*size)\n",
    "        for s, val in freq.items():\n",
    "            idx = s[0]*size + s[1]\n",
    "            mu_pi[idx]+= val\n",
    "\n",
    "        grad = mu_D - mu_pi\n",
    "        param += learning_rate * grad\n",
    "        param = np.clip(param, 0, 1)\n",
    "\n",
    "        if it % 10==0:\n",
    "            loss = np.linalg.norm(grad)\n",
    "            print(f\"[maxent] iter={it}, loss={loss:.4f}\")\n",
    "            if loss < 1e-4:\n",
    "                break\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"[maxent_estimation] completed in {elapsed:.2f}s\")\n",
    "    return reshape_1d_to_2d(param, size)\n",
    "\n",
    "\n",
    "def save_maxent_rewards_csv(reward_2d: np.ndarray, filename=\"results/maxent_est_rewards.csv\"):\n",
    "    df = pd.DataFrame(reward_2d)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"[save_maxent_rewards_csv] MaxEnt estimated rewards saved to {filename}.\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# (3) ROUTE RECOMMENDATION (HARD VALUE ITERATION)\n",
    "###############################################################################\n",
    "def standard_value_iteration(mdp: GridMDP,\n",
    "                             reward_2d: np.ndarray,\n",
    "                             discount_factor: float,\n",
    "                             max_iter=500,\n",
    "                             tol=1e-7) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Hard-optimal value iteration: V(s) = max_a [ R(s') + gamma V(s') ].\n",
    "    \"\"\"\n",
    "    print(\"[standard_value_iteration] Running hard-optimal value iteration.\")\n",
    "    states = mdp.valid_states()\n",
    "    V_dict = {s:0.0 for s in states}\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        V_old = dict(V_dict)\n",
    "        delta = 0.0\n",
    "        for s in states:\n",
    "            best_val=-1e9\n",
    "            for a in mdp.actions:\n",
    "                s_next = mdp.step(s, a)\n",
    "                val = reward_2d[s_next] + discount_factor * V_old[s_next]\n",
    "                if val>best_val:\n",
    "                    best_val=val\n",
    "            delta = max(delta, abs(best_val - V_dict[s]))\n",
    "            V_dict[s] = best_val\n",
    "        if delta<tol:\n",
    "            print(f\"[standard_value_iteration] Converged after {it+1} iterations (delta={delta:.3g}).\")\n",
    "            break\n",
    "\n",
    "    V_arr = np.zeros((mdp.size, mdp.size), dtype=float)\n",
    "    for s in states:\n",
    "        V_arr[s] = V_dict[s]\n",
    "    return V_arr\n",
    "\n",
    "\n",
    "def get_hard_greedy_path(mdp: GridMDP,\n",
    "                         reward_2d: np.ndarray,\n",
    "                         blocked_cells=None,\n",
    "                         max_steps=30,\n",
    "                         discount_factor=None) -> List[Tuple[int,int]]:\n",
    "    \"\"\"\n",
    "    1) Standard value iteration on reward_2d, ignoring transitions to blocked cells.\n",
    "    2) From (0,0), greedily pick next cell for up to max_steps or until goal.\n",
    "    \"\"\"\n",
    "    print(\"[get_hard_greedy_path] Computing path with blocked_cells=\", blocked_cells)\n",
    "    if discount_factor is None:\n",
    "        discount_factor=mdp.discount_factor\n",
    "\n",
    "    # local function to skip blocked cells\n",
    "    def step_blocked(s, a):\n",
    "        (r,c)=s\n",
    "        if blocked_cells and (r,c) in blocked_cells:\n",
    "            return s\n",
    "        if a=='up'and r>0:\n",
    "            nxt=(r-1,c)\n",
    "            if blocked_cells and nxt in blocked_cells:\n",
    "                return s\n",
    "            return nxt\n",
    "        elif a=='down'and r<mdp.size-1:\n",
    "            nxt=(r+1,c)\n",
    "            if blocked_cells and nxt in blocked_cells:\n",
    "                return s\n",
    "            return nxt\n",
    "        elif a=='left'and c>0:\n",
    "            nxt=(r,c-1)\n",
    "            if blocked_cells and nxt in blocked_cells:\n",
    "                return s\n",
    "            return nxt\n",
    "        elif a=='right'and c<mdp.size-1:\n",
    "            nxt=(r,c+1)\n",
    "            if blocked_cells and nxt in blocked_cells:\n",
    "                return s\n",
    "            return nxt\n",
    "        return s\n",
    "\n",
    "    # Value iteration respecting blocked cells\n",
    "    states=mdp.valid_states()\n",
    "    V_dict={s:0.0 for s in states}\n",
    "    for _ in range(200):\n",
    "        V_old=dict(V_dict)\n",
    "        delta=0.0\n",
    "        for s in states:\n",
    "            best_val=-1e9\n",
    "            for a in mdp.actions:\n",
    "                s_next=step_blocked(s,a)\n",
    "                rew=reward_2d[s_next]\n",
    "                val=rew+discount_factor*V_old[s_next]\n",
    "                if val>best_val:\n",
    "                    best_val=val\n",
    "            delta=max(delta,abs(best_val-V_dict[s]))\n",
    "            V_dict[s]=best_val\n",
    "        if delta<1e-7:\n",
    "            break\n",
    "\n",
    "    # Greedily extract a path\n",
    "    path=[]\n",
    "    s_cur=(0,0)\n",
    "    for t in range(max_steps):\n",
    "        path.append(s_cur)\n",
    "        if s_cur==(mdp.size-1,mdp.size-1):\n",
    "            break\n",
    "        best_a=None\n",
    "        best_val=-1e9\n",
    "        for a in mdp.actions:\n",
    "            s_next=step_blocked(s_cur,a)\n",
    "            val=reward_2d[s_next]+discount_factor*V_dict[s_next]\n",
    "            if val>best_val:\n",
    "                best_val=val\n",
    "                best_a=a\n",
    "        s_cur=step_blocked(s_cur,best_a)\n",
    "    return path\n",
    "\n",
    "def get_hard_policy_matrix(mdp: GridMDP, reward_2d: np.ndarray)->np.ndarray:\n",
    "    \"\"\"\n",
    "    Hard-optimal policy matrix from standard_value_iteration for each cell.\n",
    "    \"\"\"\n",
    "    action_map={'up':0,'down':1,'left':2,'right':3}\n",
    "    V_2d = standard_value_iteration(mdp,reward_2d,mdp.discount_factor)\n",
    "    mat=np.zeros((mdp.size,mdp.size),dtype=int)\n",
    "\n",
    "    states=mdp.valid_states()\n",
    "    for s in states:\n",
    "        (r,c)=s\n",
    "        best_val=-1e9\n",
    "        best_a='up'\n",
    "        for a in mdp.actions:\n",
    "            s_next=mdp.step(s,a)\n",
    "            val=reward_2d[s_next]+mdp.discount_factor*V_2d[s_next]\n",
    "            if val>best_val:\n",
    "                best_val=val\n",
    "                best_a=a\n",
    "        mat[r,c]=action_map[best_a]\n",
    "    return mat\n",
    "\n",
    "###############################################################################\n",
    "# (4) METRICS & EVALUATION\n",
    "###############################################################################\n",
    "def compute_true_discounted_return(mdp: GridMDP, path)->float:\n",
    "    \"\"\" sum_{t} gamma^t * R_true(path[t]) \"\"\"\n",
    "    ret=0.0\n",
    "    gamma_t=1.0\n",
    "    for s in path:\n",
    "        ret+= gamma_t * mdp.true_rewards[s]\n",
    "        gamma_t*= mdp.discount_factor\n",
    "    return ret\n",
    "\n",
    "def compute_est_discounted_return(reward_2d: np.ndarray, mdp: GridMDP, path)->float:\n",
    "    \"\"\" sum_{t} gamma^t * R_est(path[t]) \"\"\"\n",
    "    ret=0.0\n",
    "    gamma_t=1.0\n",
    "    for s in path:\n",
    "        ret+= gamma_t*reward_2d[s]\n",
    "        gamma_t*= mdp.discount_factor\n",
    "    return ret\n",
    "\n",
    "def reward_rmse(true_rewards_2d, est_rewards_2d)->float:\n",
    "    return float(np.sqrt(np.mean((true_rewards_2d - est_rewards_2d)**2)))\n",
    "\n",
    "def reward_rank_corr(true_rewards_2d, est_rewards_2d)->float:\n",
    "    t=true_rewards_2d.flatten()\n",
    "    e=est_rewards_2d.flatten()\n",
    "    corr,_= spearmanr(t,e)\n",
    "    return float(corr)\n",
    "\n",
    "def policy_disagreement(mdp:GridMDP,\n",
    "                        R_true_2d: np.ndarray,\n",
    "                        R_est_2d: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compare the hard-optimal policy for R_true_2d vs. R_est_2d\n",
    "    by fraction of states that differ in best action.\n",
    "    \"\"\"\n",
    "    pol_true = get_hard_policy_matrix(mdp, R_true_2d)\n",
    "    pol_est  = get_hard_policy_matrix(mdp, R_est_2d)\n",
    "    states=mdp.valid_states()\n",
    "    diff=0\n",
    "    for s in states:\n",
    "        if pol_true[s[0],s[1]]!= pol_est[s[0],s[1]]:\n",
    "            diff+=1\n",
    "    return diff/len(states)\n",
    "\n",
    "###############################################################################\n",
    "# (5) MAIN EXECUTION\n",
    "###############################################################################\n",
    "def main():\n",
    "    print(\"[main] Starting main execution...\")\n",
    "\n",
    "    # Hyperparameters\n",
    "    K=5\n",
    "    N=10\n",
    "    max_steps=15\n",
    "    seed=42\n",
    "\n",
    "    # 0) Build MDP\n",
    "    mdp=GridMDP(size=K, discount_factor=0.95, seed=seed)\n",
    "    mdp.true_rewards[K-1, K-1] = 20.0  # Increase goal reward for clarity\n",
    "    print(f\"[main] Created GridMDP shape={mdp.shape()}, discount={mdp.discount_factor:.2f}\")\n",
    "\n",
    "    # 1) Gather Expert Data\n",
    "    print(\"[main] Generating expert data...\")\n",
    "    trajectories=generate_trajectories(mdp, N=N, max_steps=max_steps, policy_type=\"soft\", seed=seed)\n",
    "    save_trajectories_csv(trajectories, filename=\"results/expert_data.csv\")\n",
    "\n",
    "    # 2) Estimate Rewards: NFXP\n",
    "    print(\"[main] Estimating NFXP rewards...\")\n",
    "    t0=time.time()\n",
    "    nfxp_est=estimate_nfxp(mdp, trajectories, max_iter=500)\n",
    "    time_nfxp=time.time()-t0\n",
    "    save_rewards_csv(nfxp_est, filename=\"results/nfxp_est_rewards.csv\")\n",
    "\n",
    "    # 3) Estimate Rewards: MaxEnt\n",
    "    print(\"[main] Estimating MaxEnt rewards...\")\n",
    "    t1=time.time()\n",
    "    maxent_est=maxent_estimation(mdp, trajectories, mdp.discount_factor, learning_rate=0.01, max_iter=50)\n",
    "    time_maxent=time.time()-t1\n",
    "    save_maxent_rewards_csv(maxent_est, filename=\"results/maxent_est_rewards.csv\")\n",
    "\n",
    "    # 4) Evaluate reward quality (RMSE, rank correlation, policy difference)\n",
    "    print(\"[main] Computing evaluation metrics (RMSE, rank corr, policy diff).\")\n",
    "    rmse_nfxp  = reward_rmse(mdp.true_rewards, nfxp_est)\n",
    "    rmse_maxent= reward_rmse(mdp.true_rewards, maxent_est)\n",
    "    rcorr_nfxp   = reward_rank_corr(mdp.true_rewards, nfxp_est)\n",
    "    rcorr_maxent = reward_rank_corr(mdp.true_rewards, maxent_est)\n",
    "    pdiff_nfxp   = policy_disagreement(mdp, mdp.true_rewards, nfxp_est)\n",
    "    pdiff_maxent = policy_disagreement(mdp, mdp.true_rewards, maxent_est)\n",
    "\n",
    "    # 5) Evaluate route recommendations in unblocked scenario\n",
    "    print(\"[main] Building route recommendations in unblocked scenario...\")\n",
    "    route_gt    = get_hard_greedy_path(mdp, mdp.true_rewards, None, max_steps)\n",
    "    route_nfxp  = get_hard_greedy_path(mdp, nfxp_est,       None, max_steps)\n",
    "    route_maxent= get_hard_greedy_path(mdp, maxent_est,     None, max_steps)\n",
    "\n",
    "    gt_est_unblocked    = compute_est_discounted_return(mdp.true_rewards, mdp, route_gt)\n",
    "    gt_true_unblocked   = compute_true_discounted_return(mdp, route_gt)\n",
    "    nfxp_est_unblocked  = compute_est_discounted_return(nfxp_est, mdp, route_nfxp)\n",
    "    nfxp_true_unblocked = compute_true_discounted_return(mdp, route_nfxp)\n",
    "    maxent_est_unblocked= compute_est_discounted_return(maxent_est, mdp, route_maxent)\n",
    "    maxent_true_unblocked= compute_true_discounted_return(mdp, route_maxent)\n",
    "\n",
    "    # 6) Evaluate route recommendations in blocked scenario\n",
    "    print(\"[main] Building route recommendations in blocked scenario...\")\n",
    "    blocked_cells=None\n",
    "    if K>=5:\n",
    "        blocked_cells=[(2,2),(2,3),(3,2),(3,3)]\n",
    "    route_gt_block     = get_hard_greedy_path(mdp, mdp.true_rewards, blocked_cells, max_steps)\n",
    "    route_nfxp_block   = get_hard_greedy_path(mdp, nfxp_est,         blocked_cells, max_steps)\n",
    "    route_maxent_block = get_hard_greedy_path(mdp, maxent_est,       blocked_cells, max_steps)\n",
    "\n",
    "    gt_est_block       = compute_est_discounted_return(mdp.true_rewards, mdp, route_gt_block)\n",
    "    gt_true_block      = compute_true_discounted_return(mdp, route_gt_block)\n",
    "    nfxp_est_block     = compute_est_discounted_return(nfxp_est, mdp, route_nfxp_block)\n",
    "    nfxp_true_block    = compute_true_discounted_return(mdp, route_nfxp_block)\n",
    "    maxent_est_block   = compute_est_discounted_return(maxent_est, mdp, route_maxent_block)\n",
    "    maxent_true_block  = compute_true_discounted_return(mdp, route_maxent_block)\n",
    "\n",
    "    # 7) Visualization of final policies\n",
    "    print(\"[main] Creating final policy comparison figure...\")\n",
    "    pol_gt     = get_hard_policy_matrix(mdp, mdp.true_rewards)\n",
    "    pol_nfxp   = get_hard_policy_matrix(mdp, nfxp_est)\n",
    "    pol_maxent = get_hard_policy_matrix(mdp, maxent_est)\n",
    "\n",
    "    fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "    plot_reward_and_policy(axes[0], mdp.true_rewards, pol_gt,     title=\"GroundTruth (Hard Policy)\")\n",
    "    plot_reward_and_policy(axes[1], nfxp_est,         pol_nfxp,   title=\"NFXP (Hard Policy)\")\n",
    "    plot_reward_and_policy(axes[2], maxent_est,       pol_maxent, title=\"MaxEnt (Hard Policy)\")\n",
    "    plt.tight_layout()\n",
    "    out_fig1 = \"results/reward_policy_comparison.png\"\n",
    "    plt.savefig(out_fig1, dpi=150)\n",
    "    print(f\"[main] Saved figure: {out_fig1}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Compare unblocked vs blocked routes for NFXP/MaxEnt\n",
    "    print(\"[main] Creating recommended paths figure (unblocked vs. blocked).\")\n",
    "    fig2, axes2 = plt.subplots(1,2, figsize=(12,5))\n",
    "    plot_two_paths(axes2[0], K, route_nfxp, route_maxent,\n",
    "                   blocked_cells=None,\n",
    "                   title=\"Unblocked: NFXP(blue) vs MaxEnt(red)\")\n",
    "    plot_two_paths(axes2[1], K, route_nfxp_block, route_maxent_block,\n",
    "                   blocked_cells=blocked_cells,\n",
    "                   title=\"Blocked: NFXP(blue) vs MaxEnt(red)\")\n",
    "    plt.tight_layout()\n",
    "    out_fig2 = \"results/recommended_paths.png\"\n",
    "    plt.savefig(out_fig2, dpi=150)\n",
    "    print(f\"[main] Saved figure: {out_fig2}\")\n",
    "    plt.close(fig2)\n",
    "\n",
    "    # 8) Prepare final results table\n",
    "    print(\"[main] Preparing final results table and saving to CSV.\")\n",
    "    time_gt=0.0\n",
    "    rmse_gt=0.0\n",
    "    rcorr_gt=1.0\n",
    "    pdiff_gt=0.0\n",
    "\n",
    "    data_table = [\n",
    "        [\"Hyperparams:\", f\"K={K}, N={N}, max_steps={max_steps}\", \"\", \"\"],\n",
    "        [\"Method\",\"NFXP\",\"MaxEnt\",\"GroundTruth\",\"\"],\n",
    "        [\"Time (sec)\",f\"{time_nfxp:.3f}\",f\"{time_maxent:.3f}\",f\"{time_gt:.3f}\",\"\"],\n",
    "        [\"RMSE(Reward)\",f\"{rmse_nfxp:.3f}\",f\"{rmse_maxent:.3f}\",f\"{rmse_gt:.3f}\",\"\"],\n",
    "        [\"RankCorr(Reward)\",f\"{rcorr_nfxp:.3f}\",f\"{rcorr_maxent:.3f}\",f\"{rcorr_gt:.3f}\",\"\"],\n",
    "        [\"PolDiff(Reward)\",f\"{pdiff_nfxp:.3f}\",f\"{pdiff_maxent:.3f}\",f\"{pdiff_gt:.3f}\",\"\"],\n",
    "        [\"--Unblocked--\",\"NFXP\",\"MaxEnt\",\"GroundTruth\",\"\"],\n",
    "        [\"Return (Est R)\",f\"{nfxp_est_unblocked:.3f}\",f\"{maxent_est_unblocked:.3f}\",f\"{gt_est_unblocked:.3f}\",\"\"],\n",
    "        [\"Return (True R)\",f\"{nfxp_true_unblocked:.3f}\",f\"{maxent_true_unblocked:.3f}\",f\"{gt_true_unblocked:.3f}\",\"\"],\n",
    "        [\"--Blocked--\",\"NFXP\",\"MaxEnt\",\"GroundTruth\",\"\"],\n",
    "        [\"Return (Est R)\",f\"{nfxp_est_block:.3f}\",f\"{maxent_est_block:.3f}\",f\"{gt_est_block:.3f}\",\"\"],\n",
    "        [\"Return (True R)\",f\"{nfxp_true_block:.3f}\",f\"{maxent_true_block:.3f}\",f\"{gt_true_block:.3f}\",\"\"]\n",
    "    ]\n",
    "\n",
    "    # Print table in console\n",
    "    if USE_TABULATE:\n",
    "        print(\"\\nFINAL RESULTS\\n\")\n",
    "        print(tabulate(data_table, tablefmt=\"github\"))\n",
    "    else:\n",
    "        print(\"\\nFINAL RESULTS (Fallback)\\n\")\n",
    "        for row in data_table:\n",
    "            print(\" | \".join(map(str, row)))\n",
    "\n",
    "    # Also save final results as CSV\n",
    "    # We'll assume the first column is a label row, subsequent columns are data\n",
    "    final_csv_path = \"results/final_results.csv\"\n",
    "    all_rows = []\n",
    "    for row in data_table:\n",
    "        # Make sure everything is string\n",
    "        all_rows.append([str(x) for x in row])\n",
    "\n",
    "    df_table = pd.DataFrame(all_rows)\n",
    "    df_table.to_csv(final_csv_path, index=False, header=False)\n",
    "    print(f\"[main] Saved final results to {final_csv_path}\")\n",
    "\n",
    "    print(\"[main] Done. See results above.\")\n",
    "\n",
    "###############################################################################\n",
    "# Additional Helpers for Visualization\n",
    "###############################################################################\n",
    "def plot_reward_and_policy(ax, reward_2d: np.ndarray, policy_mat: np.ndarray, title=\"\"):\n",
    "    \"\"\"\n",
    "    Display the reward grid plus an arrow for each cell's best action.\n",
    "    action_map: 0=up,1=down,2=left,3=right\n",
    "    \"\"\"\n",
    "    size = reward_2d.shape[0]\n",
    "    im = ax.imshow(reward_2d, cmap='viridis', origin='upper')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(range(size))\n",
    "    ax.set_yticks(range(size))\n",
    "    action_arrow = {\n",
    "        0: (0, -0.3),   # up\n",
    "        1: (0,  0.3),   # down\n",
    "        2: (-0.3, 0),   # left\n",
    "        3: (0.3,  0)    # right\n",
    "    }\n",
    "    for r in range(size):\n",
    "        for c in range(size):\n",
    "            a = policy_mat[r,c]\n",
    "            dx, dy = action_arrow[a]\n",
    "            ax.arrow(c, r, dx, dy,\n",
    "                     color='white', head_width=0.1,\n",
    "                     head_length=0.1, length_includes_head=True)\n",
    "\n",
    "def plot_two_paths(ax, size,\n",
    "                   path_nfxp, path_maxent,\n",
    "                   blocked_cells=None,\n",
    "                   title=\"\"):\n",
    "    \"\"\"\n",
    "    Show NFXP path (blue) vs. MaxEnt path (red) in a single plot.\n",
    "    If blocked_cells is given, shade them in gray.\n",
    "    \"\"\"\n",
    "    ax.set_xlim(-0.5, size-0.5)\n",
    "    ax.set_ylim(-0.5, size-0.5)\n",
    "    ax.set_xticks(range(size))\n",
    "    ax.set_yticks(range(size))\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_aspect('equal','box')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Draw grid lines\n",
    "    for i in range(size+1):\n",
    "        ax.axhline(i-0.5,color='black')\n",
    "        ax.axvline(i-0.5,color='black')\n",
    "\n",
    "    if blocked_cells:\n",
    "        for (r,c) in blocked_cells:\n",
    "            ax.fill_between([c-0.5,c+0.5], r-0.5, r+0.5, color='gray', alpha=0.5)\n",
    "\n",
    "    # NFXP path\n",
    "    xs_n = [s[1] for s in path_nfxp]\n",
    "    ys_n = [s[0] for s in path_nfxp]\n",
    "    ax.plot(xs_n, ys_n, 'o-', color='blue', label='NFXP path')\n",
    "\n",
    "    # MaxEnt path\n",
    "    xs_m = [s[1] for s in path_maxent]\n",
    "    ys_m = [s[0] for s in path_maxent]\n",
    "    ax.plot(xs_m, ys_m, 's-', color='red', label='MaxEnt path')\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "###############################################################################\n",
    "# Execute main()\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NxFP IRL (3 rows) ===\n",
      "AnchMode      RankCorr    Corr    RMSE    PolDiff    RecoReward    TimeSec\n",
      "Anch=0            0.41    0.66   0.498        0.2         4.911      21.4\n",
      "Anch=1            0.3     0.65   0.498        0.2         4.911      24.93\n",
      "Anch=2            0.3     0.65   0.499        0.2         4.911      17.46\n",
      "\n",
      "=== MaxEnt IRL (3 rows) ===\n",
      "AnchMode      RankCorr    Corr    RMSE    PolDiff    RecoReward    TimeSec\n",
      "Anch=0            0.58    0.64   0.329       0.24         4.911       5.1\n",
      "Anch=1            0.58    0.64   0.329       0.16         4.391       5.02\n",
      "Anch=2            0.08    0.29   0.417       0.28         4.911       5.08\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import logsumexp\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Attempt to import tabulate for nicer table printing in final output\n",
    "try:\n",
    "    from tabulate import tabulate\n",
    "    USE_TABULATE = True\n",
    "except ImportError:\n",
    "    USE_TABULATE = False\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "###############################################################################\n",
    "# (1) ENVIRONMENT & EXPERT DATA GENERATION\n",
    "###############################################################################\n",
    "class GridMDP:\n",
    "    def __init__(self, size=5, discount_factor=0.95, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        self.size = size\n",
    "        self.discount_factor = discount_factor\n",
    "        self.true_rewards = np.zeros((size, size), dtype=float)\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self._init_rewards()\n",
    "\n",
    "    def _init_rewards(self):\n",
    "        for r in range(self.size):\n",
    "            for c in range(self.size):\n",
    "                self.true_rewards[r,c] = np.random.rand()\n",
    "        self.true_rewards[0,0] = 0.05\n",
    "        self.true_rewards[self.size-1, self.size-1] = 1.0\n",
    "\n",
    "    def step(self, state: Tuple[int,int], action: str) -> Tuple[int,int]:\n",
    "        (r,c) = state\n",
    "        if action=='up'and r>0: r-=1\n",
    "        elif action=='down'and r<self.size-1: r+=1\n",
    "        elif action=='left'and c>0: c-=1\n",
    "        elif action=='right'and c<self.size-1: c+=1\n",
    "        return (r,c)\n",
    "\n",
    "    def get_reward(self, state: Tuple[int,int]) -> float:\n",
    "        return self.true_rewards[state]\n",
    "\n",
    "    def random_start_state(self)->Tuple[int,int]:\n",
    "        return (random.randint(0, self.size-1),\n",
    "                random.randint(0, self.size-1))\n",
    "\n",
    "    def shape(self)->Tuple[int,int]:\n",
    "        return (self.size, self.size)\n",
    "\n",
    "    def valid_states(self)->List[Tuple[int,int]]:\n",
    "        return [(r,c) for r in range(self.size) for c in range(self.size)]\n",
    "\n",
    "def soft_value_iteration(mdp, rewards_2d, discount_factor, max_iter=200, tol=1e-7):\n",
    "    states = mdp.valid_states()\n",
    "    V_dict = {s: 0.0 for s in states}\n",
    "    for _ in range(max_iter):\n",
    "        V_old = dict(V_dict)\n",
    "        delta=0.0\n",
    "        for s in states:\n",
    "            vals=[]\n",
    "            for a in mdp.actions:\n",
    "                s_next = mdp.step(s,a)\n",
    "                vals.append(rewards_2d[s_next] + discount_factor*V_old[s_next])\n",
    "            new_val = logsumexp(vals)\n",
    "            delta = max(delta, abs(V_dict[s]-new_val))\n",
    "            V_dict[s] = new_val\n",
    "        if delta<tol: \n",
    "            break\n",
    "    V_2d = np.zeros_like(rewards_2d)\n",
    "    for s in states:\n",
    "        V_2d[s] = V_dict[s]\n",
    "    return V_2d\n",
    "\n",
    "def soft_policy(mdp, V_2d, R_2d):\n",
    "    states = mdp.valid_states()\n",
    "    pi_dict = {}\n",
    "    for s in states:\n",
    "        vals = []\n",
    "        for a in mdp.actions:\n",
    "            s_next = mdp.step(s, a)\n",
    "            vals.append(R_2d[s_next] + mdp.discount_factor * V_2d[s_next])\n",
    "        log_probs = vals - logsumexp(vals)\n",
    "        probs = np.exp(log_probs)\n",
    "        # store as dict {action: prob}\n",
    "        pi_dict[s] = {mdp.actions[i]: probs[i] for i in range(len(mdp.actions))}\n",
    "    return pi_dict\n",
    "\n",
    "def generate_trajectories(mdp, N=20, max_steps=15, policy_type=\"soft\", seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    V_true = soft_value_iteration(mdp, mdp.true_rewards, mdp.discount_factor)\n",
    "    pi_soft = soft_policy(mdp, V_true, mdp.true_rewards)\n",
    "    if policy_type==\"optimal\":\n",
    "        pi_dict={}\n",
    "        states=mdp.valid_states()\n",
    "        for s in states:\n",
    "            vals=[]\n",
    "            for a in mdp.actions:\n",
    "                s_next = mdp.step(s,a)\n",
    "                vals.append(mdp.true_rewards[s_next]+mdp.discount_factor*V_true[s_next])\n",
    "            best_idx=np.argmax(vals)\n",
    "            best_a=mdp.actions[best_idx]\n",
    "            pi_dict[s]={a:(1.0 if a==best_a else 0.0) for a in mdp.actions}\n",
    "    else:\n",
    "        pi_dict=pi_soft\n",
    "\n",
    "    trajectories=[]\n",
    "    for _ in range(N):\n",
    "        s_cur=mdp.random_start_state()\n",
    "        traj=[]\n",
    "        for __ in range(max_steps):\n",
    "            acts=list(pi_dict[s_cur].keys())\n",
    "            probs=list(pi_dict[s_cur].values())\n",
    "            a=np.random.choice(acts,p=probs)\n",
    "            s_next=mdp.step(s_cur,a)\n",
    "            r_next=mdp.get_reward(s_next)\n",
    "            traj.append((s_cur,a,r_next))\n",
    "            s_cur=s_next\n",
    "        trajectories.append(traj)\n",
    "    return trajectories\n",
    "\n",
    "def save_trajectories_csv(trajectories,filename=\"results/expert_data.csv\"):\n",
    "    rows=[]\n",
    "    for i,traj in enumerate(trajectories):\n",
    "        for t,(s,a,r) in enumerate(traj):\n",
    "            rows.append([i,t,s[0],s[1],a,r])\n",
    "    df=pd.DataFrame(rows,columns=[\"trajectory_id\",\"step\",\"row\",\"col\",\"action\",\"reward\"])\n",
    "    df.to_csv(filename,index=False)\n",
    "\n",
    "###############################################################################\n",
    "# (2) IRL ALGORITHMS WITH ANCHOR MODES\n",
    "###############################################################################\n",
    "def reshape_1d_to_2d(vec: np.ndarray, size: int)->np.ndarray:\n",
    "    return vec.reshape((size,size))\n",
    "\n",
    "def get_anchor_params(p, anchor_mode, anchor_val):\n",
    "    \"\"\"\n",
    "    anchor_mode=0 => no anchor\n",
    "    anchor_mode=1 => param[0,0] fixed=anchor_val\n",
    "    anchor_mode=2 => param[0,0] fixed=anchor_val + L2 reg\n",
    "    We'll handle the reg in objective, but we enforce the anchor here.\n",
    "    \"\"\"\n",
    "    size = int(np.sqrt(p.size + (1 if anchor_mode>=1 else 0)))\n",
    "    if anchor_mode == 0:\n",
    "        return reshape_1d_to_2d(p,size)\n",
    "    else:\n",
    "        # param[0,0] => first element => p[0] is fixed\n",
    "        full = np.zeros((size,size))\n",
    "        full[0,0] = anchor_val\n",
    "        # fill the rest\n",
    "        full_flat = full.flatten()\n",
    "        full_flat[1:] = p\n",
    "        return reshape_1d_to_2d(full_flat, size)\n",
    "\n",
    "###############################################################################\n",
    "# NxFP with anchor\n",
    "###############################################################################\n",
    "def nfxp_objective_anchor(params, anchor_mode, anchor_val, reg, mdp, trajectories):\n",
    "    R_2d = get_anchor_params(params, anchor_mode, anchor_val)\n",
    "    V_2d = soft_value_iteration(mdp,R_2d,mdp.discount_factor)\n",
    "    pi_dict={}\n",
    "    states=mdp.valid_states()\n",
    "    for s in states:\n",
    "        vals=[]\n",
    "        for a in mdp.actions:\n",
    "            s_next=mdp.step(s,a)\n",
    "            vals.append(R_2d[s_next]+mdp.discount_factor*V_2d[s_next])\n",
    "        log_probs=vals - logsumexp(vals)\n",
    "        pi_dict[s]=np.exp(log_probs)\n",
    "\n",
    "    eps=1e-12\n",
    "    total_ll=0.0\n",
    "    for traj in trajectories:\n",
    "        for (s,a,_) in traj:\n",
    "            a_idx=mdp.actions.index(a)\n",
    "            total_ll+=np.log(pi_dict[s][a_idx]+eps)\n",
    "\n",
    "    pen=0.0\n",
    "    if anchor_mode==2:\n",
    "        pen = reg*np.sum(params**2)\n",
    "    return -total_ll + pen\n",
    "\n",
    "def estimate_nfxp_anchor(mdp, trajectories, anchor_mode=0, anchor_val=0.0, reg=0.0):\n",
    "    size=mdp.size\n",
    "    if anchor_mode==0:\n",
    "        # no anchor => param dimension=size*size\n",
    "        x0=np.random.uniform(0,1,size*size)\n",
    "        bnds=[(0,1)]*(size*size)\n",
    "        def wrapper(p):\n",
    "            return nfxp_objective_anchor(p,0,anchor_val,reg,mdp,trajectories)\n",
    "    else:\n",
    "        # anchor => param dimension=(size*size-1)\n",
    "        x0=np.random.uniform(0,1,size*size-1)\n",
    "        bnds=[(0,1)]*(size*size-1)\n",
    "        def wrapper(p):\n",
    "            return nfxp_objective_anchor(p,anchor_mode,anchor_val,reg,mdp,trajectories)\n",
    "\n",
    "    res=minimize(wrapper,x0,method='L-BFGS-B',bounds=bnds,\n",
    "                 options={'maxiter':200,'disp':False})\n",
    "    return get_anchor_params(res.x,anchor_mode,anchor_val)\n",
    "\n",
    "###############################################################################\n",
    "# MaxEnt with anchor\n",
    "###############################################################################\n",
    "def collect_state_visitation(mdp, pi_dict, discount_factor, T=15):\n",
    "    freq=defaultdict(float)\n",
    "    n_roll=500\n",
    "    for _ in range(n_roll):\n",
    "        s=mdp.random_start_state()\n",
    "        for t in range(T):\n",
    "            freq[s]+= (discount_factor**t)\n",
    "            # pi_dict[s] is an array or list of action probs => pick index\n",
    "            # but we stored pi_dict[s] as an array => need to convert it to consistent format if needed\n",
    "            if isinstance(pi_dict[s], list) or isinstance(pi_dict[s], np.ndarray):\n",
    "                a_idx=np.random.choice(len(pi_dict[s]), p=pi_dict[s])\n",
    "            else:\n",
    "                # pi_dict[s] is a dict {action: prob}, pick from keys\n",
    "                acts=list(pi_dict[s].keys())\n",
    "                probs=list(pi_dict[s].values())\n",
    "                a_idx=np.random.choice(len(acts), p=probs)\n",
    "                # map a_idx back to the actual action\n",
    "            a=mdp.actions[a_idx]\n",
    "            s=mdp.step(s,a)\n",
    "    for s_ in freq:\n",
    "        freq[s_]/=n_roll\n",
    "    return freq\n",
    "\n",
    "def feature_vector(mdp,s):\n",
    "    idx=s[0]*mdp.size + s[1]\n",
    "    vec=np.zeros(mdp.size*mdp.size)\n",
    "    vec[idx]=1.0\n",
    "    return vec\n",
    "\n",
    "def compute_expert_feature_expectation(mdp, trajectories):\n",
    "    disc=mdp.discount_factor\n",
    "    size=mdp.size\n",
    "    mu_D=np.zeros(size*size)\n",
    "    for traj in trajectories:\n",
    "        for t,(s,a,r) in enumerate(traj):\n",
    "            mu_D+= (disc**t)*feature_vector(mdp,s)\n",
    "    mu_D/=len(trajectories)\n",
    "    return mu_D\n",
    "\n",
    "def maxent_estimation_anchor(mdp, trajectories, anchor_mode=0, anchor_val=0.0, reg=0.0,\n",
    "                             lr=0.01, max_iter=100):\n",
    "    size=mdp.size\n",
    "    mu_D=compute_expert_feature_expectation(mdp,trajectories)\n",
    "    # param dimension\n",
    "    if anchor_mode==0:\n",
    "        param=np.random.uniform(0,1,size*size)\n",
    "    else:\n",
    "        param=np.random.uniform(0,1,size*size-1)\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # 1) Build full reward from param\n",
    "        R_2d = get_anchor_params(param, anchor_mode, anchor_val)\n",
    "        V_2d=soft_value_iteration(mdp,R_2d,mdp.discount_factor)\n",
    "\n",
    "        # 2) Build policy\n",
    "        pi_dict={}\n",
    "        for s in mdp.valid_states():\n",
    "            vals=[]\n",
    "            for a in mdp.actions:\n",
    "                s_next=mdp.step(s,a)\n",
    "                vals.append(R_2d[s_next]+mdp.discount_factor*V_2d[s_next])\n",
    "            logp=vals-logsumexp(vals)\n",
    "            pi_dict[s]=np.exp(logp)  # array of action probabilities\n",
    "\n",
    "        # 3) Compute state visitation freq\n",
    "        freq=collect_state_visitation(mdp, pi_dict, mdp.discount_factor, T=15)\n",
    "\n",
    "        # 4) mu_pi\n",
    "        mu_pi=np.zeros(size*size)\n",
    "        for s_, val in freq.items():\n",
    "            idx=s_[0]*size+s_[1]\n",
    "            mu_pi[idx]+=val\n",
    "\n",
    "        # 5) Full gradient\n",
    "        grad_full = (mu_D - mu_pi)\n",
    "\n",
    "        # 6) If anchor_mode=2 => L2 reg\n",
    "        if anchor_mode==2:\n",
    "            # param is the free portion => same shape\n",
    "            # grad for free portion only\n",
    "            # but we need to subtract reg * param\n",
    "            # for the free part\n",
    "            pass\n",
    "\n",
    "        # 7) Update only free portion\n",
    "        if anchor_mode==0:\n",
    "            # param is entire 25D vector\n",
    "            if anchor_mode==2:\n",
    "                grad_full -= reg*param\n",
    "            param += lr*grad_full\n",
    "        else:\n",
    "            # anchor => param is 24D\n",
    "            # skip grad_full[0]\n",
    "            grad_free = grad_full[1:]\n",
    "            if anchor_mode==2:\n",
    "                grad_free -= (reg*param)\n",
    "            param += lr*grad_free\n",
    "\n",
    "        # 8) Clip\n",
    "        param=np.clip(param,0,1)\n",
    "\n",
    "    return get_anchor_params(param, anchor_mode, anchor_val)\n",
    "\n",
    "###############################################################################\n",
    "# Hard-optimal VI\n",
    "###############################################################################\n",
    "def standard_value_iteration(mdp, R_2d, discount_factor, max_iter=500,tol=1e-7):\n",
    "    states=mdp.valid_states()\n",
    "    V_dict={s:0.0 for s in states}\n",
    "    for _ in range(max_iter):\n",
    "        V_old=dict(V_dict)\n",
    "        delta=0.0\n",
    "        for s in states:\n",
    "            best=-1e9\n",
    "            for a in mdp.actions:\n",
    "                s_next=mdp.step(s,a)\n",
    "                val=R_2d[s_next]+discount_factor*V_old[s_next]\n",
    "                if val>best: best=val\n",
    "            delta=max(delta,abs(best-V_dict[s]))\n",
    "            V_dict[s]=best\n",
    "        if delta<tol: break\n",
    "    V_2d=np.zeros_like(R_2d)\n",
    "    for s in states: V_2d[s]=V_dict[s]\n",
    "    return V_2d\n",
    "\n",
    "def get_hard_greedy_path(mdp, R_2d, blocked_cells=None, max_steps=30):\n",
    "    discount_factor=mdp.discount_factor\n",
    "    states=mdp.valid_states()\n",
    "\n",
    "    def step_blocked(s,a):\n",
    "        (r,c)=s\n",
    "        if blocked_cells and (r,c)in blocked_cells:\n",
    "            return s\n",
    "        if a=='up'and r>0:\n",
    "            nxt=(r-1,c)\n",
    "            if blocked_cells and nxt in blocked_cells:\n",
    "                return s\n",
    "            return nxt\n",
    "        elif a=='down'and r<mdp.size-1:\n",
    "            nxt=(r+1,c)\n",
    "            if blocked_cells and nxt in blocked_cells:\n",
    "                return s\n",
    "            return nxt\n",
    "        elif a=='left'and c>0:\n",
    "            nxt=(r,c-1)\n",
    "            if blocked_cells and nxt in blocked_cells:\n",
    "                return s\n",
    "            return nxt\n",
    "        elif a=='right'and c<mdp.size-1:\n",
    "            nxt=(r,c+1)\n",
    "            if blocked_cells and nxt in blocked_cells:\n",
    "                return s\n",
    "            return nxt\n",
    "        return s\n",
    "\n",
    "    # Do a standard VI ignoring blocked\n",
    "    V_dict={s:0.0 for s in states}\n",
    "    for _ in range(200):\n",
    "        V_old=dict(V_dict)\n",
    "        delta=0.0\n",
    "        for s in states:\n",
    "            best=-1e9\n",
    "            for a in mdp.actions:\n",
    "                sn=step_blocked(s,a)\n",
    "                val=R_2d[sn]+discount_factor*V_old[sn]\n",
    "                if val>best: best=val\n",
    "            delta=max(delta,abs(best-V_dict[s]))\n",
    "            V_dict[s]=best\n",
    "        if delta<1e-7: break\n",
    "\n",
    "    path=[]\n",
    "    s_cur=(0,0)\n",
    "    for _ in range(max_steps):\n",
    "        path.append(s_cur)\n",
    "        if s_cur==(mdp.size-1,mdp.size-1): break\n",
    "        best_a=None\n",
    "        best_val=-1e9\n",
    "        for a in mdp.actions:\n",
    "            sn=step_blocked(s_cur,a)\n",
    "            val=R_2d[sn]+discount_factor*V_dict[sn]\n",
    "            if val>best_val:\n",
    "                best_val=val\n",
    "                best_a=a\n",
    "        s_cur=step_blocked(s_cur,best_a)\n",
    "    return path\n",
    "\n",
    "###############################################################################\n",
    "# Some utility metrics\n",
    "###############################################################################\n",
    "def compute_true_discounted_return(mdp, path):\n",
    "    ret=0.0\n",
    "    gamma_t=1.0\n",
    "    for s in path:\n",
    "        ret+=gamma_t*mdp.true_rewards[s]\n",
    "        gamma_t*=mdp.discount_factor\n",
    "    return ret\n",
    "\n",
    "def reward_rmse(true_2d, est_2d):\n",
    "    return float(np.sqrt(np.mean((true_2d - est_2d)**2)))\n",
    "\n",
    "def reward_rank_corr(true_2d, est_2d):\n",
    "    t=true_2d.flatten()\n",
    "    e=est_2d.flatten()\n",
    "    corr,_= spearmanr(t,e)\n",
    "    return float(corr)\n",
    "\n",
    "def reward_corr(true_2d, est_2d):\n",
    "    t=true_2d.flatten()\n",
    "    e=est_2d.flatten()\n",
    "    r,_= pearsonr(t,e)\n",
    "    return float(r)\n",
    "\n",
    "def policy_disagreement(mdp, R_true, R_est):\n",
    "    pol_true = get_hard_policy_matrix(mdp,R_true)\n",
    "    pol_est  = get_hard_policy_matrix(mdp,R_est)\n",
    "    states=mdp.valid_states()\n",
    "    diff=0\n",
    "    for s in states:\n",
    "        if pol_true[s[0],s[1]]!=pol_est[s[0],s[1]]:\n",
    "            diff+=1\n",
    "    return diff/len(states)\n",
    "\n",
    "def get_hard_policy_matrix(mdp,R_2d):\n",
    "    V_2d=standard_value_iteration(mdp,R_2d,mdp.discount_factor)\n",
    "    size=R_2d.shape[0]\n",
    "    mat=np.zeros((size,size),dtype=int)\n",
    "    action_map={'up':0,'down':1,'left':2,'right':3}\n",
    "    for r in range(size):\n",
    "        for c in range(size):\n",
    "            best=-1e9\n",
    "            best_a='up'\n",
    "            for a in mdp.actions:\n",
    "                sn=mdp.step((r,c),a)\n",
    "                val=R_2d[sn]+mdp.discount_factor*V_2d[sn]\n",
    "                if val>best:\n",
    "                    best=val\n",
    "                    best_a=a\n",
    "            mat[r,c]=action_map[best_a]\n",
    "    return mat\n",
    "\n",
    "###############################################################################\n",
    "# MAIN: Integrate anchor experiments + routing recommendations\n",
    "###############################################################################\n",
    "def main():\n",
    "    # Basic MDP\n",
    "    K=5\n",
    "    mdp=GridMDP(size=K, discount_factor=0.95, seed=42)\n",
    "    mdp.true_rewards[K-1,K-1]=2.0\n",
    "\n",
    "    # Gather expert data\n",
    "    trajectories=generate_trajectories(mdp, N=10, max_steps=15, policy_type=\"soft\", seed=42)\n",
    "    save_trajectories_csv(trajectories, \"results/expert_data.csv\")\n",
    "\n",
    "    # Anchor experiments: (mode, anchor_val, reg)\n",
    "    anchor_settings=[\n",
    "      (0,0.0,0.0),   # none\n",
    "      (1,0.05,0.0),  # anchor param[0,0] = 0.05\n",
    "      (2,0.05,0.1)   # anchor param[0,0] = 0.05 + reg=0.1\n",
    "    ]\n",
    "\n",
    "    # We'll store NxFP + MaxEnt results\n",
    "    results_nfxp=[]\n",
    "    results_maxent=[]\n",
    "\n",
    "    for (am,aval,regval) in anchor_settings:\n",
    "        # NxFP\n",
    "        t0=time.time()\n",
    "        est_nfxp=estimate_nfxp_anchor(mdp, trajectories,\n",
    "                                      anchor_mode=am,\n",
    "                                      anchor_val=aval,\n",
    "                                      reg=regval)\n",
    "        t_nfxp=time.time()-t0\n",
    "        rmse_n=reward_rmse(mdp.true_rewards,est_nfxp)\n",
    "        rcorr_n=reward_rank_corr(mdp.true_rewards,est_nfxp)\n",
    "        corr_n=reward_corr(mdp.true_rewards,est_nfxp)\n",
    "        pol_n=policy_disagreement(mdp, mdp.true_rewards, est_nfxp)\n",
    "\n",
    "        # recommended route\n",
    "        route_n=get_hard_greedy_path(mdp, est_nfxp, None,30)\n",
    "        reco_r_n=compute_true_discounted_return(mdp, route_n)\n",
    "\n",
    "        results_nfxp.append([\n",
    "          f\"Anch={am}\", f\"{rcorr_n:.2f}\", f\"{corr_n:.2f}\", f\"{rmse_n:.3f}\",\n",
    "          f\"{pol_n:.3f}\", f\"{reco_r_n:.3f}\", f\"{t_nfxp:.2f}\"\n",
    "        ])\n",
    "\n",
    "        # MaxEnt\n",
    "        t1=time.time()\n",
    "        est_mx=maxent_estimation_anchor(mdp, trajectories,\n",
    "                                        anchor_mode=am,\n",
    "                                        anchor_val=aval,\n",
    "                                        reg=regval,\n",
    "                                        lr=0.01, max_iter=50)\n",
    "        t_mx=time.time()-t1\n",
    "        rmse_m=reward_rmse(mdp.true_rewards,est_mx)\n",
    "        rcorr_m=reward_rank_corr(mdp.true_rewards,est_mx)\n",
    "        corr_m=reward_corr(mdp.true_rewards,est_mx)\n",
    "        pol_m=policy_disagreement(mdp, mdp.true_rewards, est_mx)\n",
    "\n",
    "        # recommended route\n",
    "        route_m=get_hard_greedy_path(mdp, est_mx, None,30)\n",
    "        reco_r_m=compute_true_discounted_return(mdp, route_m)\n",
    "\n",
    "        results_maxent.append([\n",
    "          f\"Anch={am}\", f\"{rcorr_m:.2f}\", f\"{corr_m:.2f}\", f\"{rmse_m:.3f}\",\n",
    "          f\"{pol_m:.3f}\", f\"{reco_r_m:.3f}\", f\"{t_mx:.2f}\"\n",
    "        ])\n",
    "\n",
    "    # Print tables with col: anchor_mode, rankCorr, Corr, RMSE, PolDiff, reco_reward, Time\n",
    "    hdr=[\"AnchMode\",\"RankCorr\",\"Corr\",\"RMSE\",\"PolDiff\",\"RecoReward\",\"TimeSec\"]\n",
    "    if USE_TABULATE:\n",
    "        print(\"\\n=== NxFP IRL (3 rows) ===\")\n",
    "        print(tabulate(results_nfxp, headers=hdr,tablefmt=\"plain\"))\n",
    "        print(\"\\n=== MaxEnt IRL (3 rows) ===\")\n",
    "        print(tabulate(results_maxent, headers=hdr,tablefmt=\"plain\"))\n",
    "    else:\n",
    "        print(\"NxFP:\\n\",results_nfxp)\n",
    "        print(\"MaxEnt:\\n\",results_maxent)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
