{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main] Ensuring 'results/' directory exists.\n",
      "[main] Starting main execution...\n",
      "[GridMDP] Initializing with size=5, discount_factor=0.95, seed=42\n",
      "[GridMDP] Randomizing initial rewards in [0,1].\n",
      "[GridMDP] Setting start=(0,0) reward to 0.05 and goal=(size-1,size-1) reward to 1.0.\n",
      "[main] Created GridMDP shape=(5, 5), discount=0.95\n",
      "[main] Generating expert data...\n",
      "[generate_trajectories] N=10, max_steps=15, policy_type=soft, seed=42.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_policy] Building soft policy from value function.\n",
      "[generate_trajectories] Generated 10 trajectories (policy_type=soft).\n",
      "[save_trajectories_csv] Saved 10 trajectories to results/expert_data.csv.\n",
      "[main] Estimating NFXP rewards...\n",
      "[estimate_nfxp] Estimating reward via nested fixed-point approach.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[estimate_nfxp] L-BFGS took 28.10s for NxFP estimation.\n",
      "[save_rewards_csv] NFXP estimated rewards saved to results/nfxp_est_rewards.csv.\n",
      "[main] Estimating MaxEnt rewards...\n",
      "[maxent_estimation] Estimating reward via maximum entropy IRL.\n",
      "[compute_expert_feature_expectation] Building expert feature expectations.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[maxent] iter=0, loss=5.9400\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[maxent] iter=10, loss=5.1086\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[maxent] iter=20, loss=4.9411\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[maxent] iter=30, loss=4.9716\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[maxent] iter=40, loss=4.6168\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[soft_value_iteration] Starting with max_iter=200, tol=1e-07.\n",
      "[maxent_estimation] completed in 8.67s\n",
      "[save_maxent_rewards_csv] MaxEnt estimated rewards saved to results/maxent_est_rewards.csv.\n",
      "[main] Computing evaluation metrics (RMSE, rank corr, policy diff).\n",
      "[standard_value_iteration] Running hard-optimal value iteration.\n",
      "[standard_value_iteration] Running hard-optimal value iteration.\n",
      "[standard_value_iteration] Running hard-optimal value iteration.\n",
      "[standard_value_iteration] Running hard-optimal value iteration.\n",
      "[main] Building route recommendations in unblocked scenario...\n",
      "[get_hard_greedy_path] Computing path with blocked_cells= None\n",
      "[get_hard_greedy_path] Computing path with blocked_cells= None\n",
      "[get_hard_greedy_path] Computing path with blocked_cells= None\n",
      "[main] Building route recommendations in blocked scenario...\n",
      "[get_hard_greedy_path] Computing path with blocked_cells= [(2, 2), (2, 3), (3, 2), (3, 3)]\n",
      "[get_hard_greedy_path] Computing path with blocked_cells= [(2, 2), (2, 3), (3, 2), (3, 3)]\n",
      "[get_hard_greedy_path] Computing path with blocked_cells= [(2, 2), (2, 3), (3, 2), (3, 3)]\n",
      "[main] Creating final policy comparison figure...\n",
      "[standard_value_iteration] Running hard-optimal value iteration.\n",
      "[standard_value_iteration] Running hard-optimal value iteration.\n",
      "[standard_value_iteration] Running hard-optimal value iteration.\n",
      "[main] Saved figure: results/reward_policy_comparison.png\n",
      "[main] Creating recommended paths figure (unblocked vs. blocked).\n",
      "[main] Saved figure: results/recommended_paths.png\n",
      "[main] Preparing final results table and saving to CSV.\n",
      "\n",
      "FINAL RESULTS\n",
      "\n",
      "|------------------|-------------------------|--------|-------------|--|\n",
      "| Hyperparams:     | K=5, N=10, max_steps=15 |        |             |  |\n",
      "| Method           | NFXP                    | MaxEnt | GroundTruth |  |\n",
      "| Time (sec)       | 28.096                  | 8.671  | 0.000       |  |\n",
      "| RMSE(Reward)     | 3.830                   | 3.809  | 0.000       |  |\n",
      "| RankCorr(Reward) | 0.469                   | 0.524  | 1.000       |  |\n",
      "| PolDiff(Reward)  | 0.200                   | 0.240  | 0.000       |  |\n",
      "| --Unblocked--    | NFXP                    | MaxEnt | GroundTruth |  |\n",
      "| Return (Est R)   | 1.084                   | 4.222  | 17.282      |  |\n",
      "| Return (True R)  | 15.816                  | 15.441 | 17.282      |  |\n",
      "| --Blocked--      | NFXP                    | MaxEnt | GroundTruth |  |\n",
      "| Return (Est R)   | 0.770                   | 4.045  | 16.999      |  |\n",
      "| Return (True R)  | 15.636                  | 15.143 | 16.999      |  |\n",
      "[main] Saved final results to results/final_results.csv\n",
      "[main] Done. See results above.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "A single-file Python script demonstrating a gridworld-based IRL experiment,\n",
    "with all outputs saved to a 'results/' folder.\n",
    "\n",
    "Sections (aligned with paper flow):\n",
    "1) Environment & Expert Data Generation\n",
    "2) IRL Algorithms (NFXP, MaxEnt)\n",
    "3) Route Recommendation via Standard (Hard) Value Iteration\n",
    "4) Metrics & Evaluation\n",
    "5) Main Execution\n",
    "\n",
    "Usage:\n",
    "  python main.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import logsumexp\n",
    "from scipy.stats import spearmanr\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Attempt to import tabulate for nicer table printing in final output\n",
    "try:\n",
    "    from tabulate import tabulate\n",
    "    USE_TABULATE = True\n",
    "except ImportError:\n",
    "    USE_TABULATE = False\n",
    "\n",
    "###############################################################################\n",
    "# Make sure a 'results/' folder exists to store outputs\n",
    "###############################################################################\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "print(\"[main] Ensuring 'results/' directory exists.\")\n",
    "\n",
    "###############################################################################\n",
    "# (1) ENVIRONMENT & EXPERT DATA GENERATION\n",
    "###############################################################################\n",
    "class GridMDP:\n",
    "    \"\"\"\n",
    "    A simple Grid MDP:\n",
    "      - Rewards in [0,1] for each cell, except start=0.05 and goal=1.0 (or higher).\n",
    "      - We'll gather 'expert' data from a soft-optimal policy on the TRUE rewards.\n",
    "    \"\"\"\n",
    "    def __init__(self, size=5, discount_factor=0.95, seed=42):\n",
    "        print(f\"[GridMDP] Initializing with size={size}, discount_factor={discount_factor}, seed={seed}\")\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        self.size = size\n",
    "        self.discount_factor = discount_factor\n",
    "        self.true_rewards = np.zeros((size, size), dtype=float)\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self._init_rewards()\n",
    "\n",
    "    def _init_rewards(self):\n",
    "        \"\"\"\n",
    "        Initialize cell-wise rewards in [0,1], with special start & goal cells.\n",
    "        \"\"\"\n",
    "        print(\"[GridMDP] Randomizing initial rewards in [0,1].\")\n",
    "        for r in range(self.size):\n",
    "            for c in range(self.size):\n",
    "                self.true_rewards[r, c] = np.random.rand()\n",
    "        print(\"[GridMDP] Setting start=(0,0) reward to 0.05 and goal=(size-1,size-1) reward to 1.0.\")\n",
    "        self.true_rewards[0, 0] = 0.05\n",
    "        self.true_rewards[self.size - 1, self.size - 1] = 1.0\n",
    "\n",
    "    def step(self, state: Tuple[int, int], action: str) -> Tuple[int, int]:\n",
    "        (r, c) = state\n",
    "        if action == 'up' and r > 0:\n",
    "            r -= 1\n",
    "        elif action == 'down' and r < self.size - 1:\n",
    "            r += 1\n",
    "        elif action == 'left' and c > 0:\n",
    "            c -= 1\n",
    "        elif action == 'right' and c < self.size - 1:\n",
    "            c += 1\n",
    "        return (r, c)\n",
    "\n",
    "    def get_reward(self, state: Tuple[int, int]) -> float:\n",
    "        return self.true_rewards[state]\n",
    "\n",
    "    def random_start_state(self) -> Tuple[int, int]:\n",
    "        r = random.randint(0, self.size - 1)\n",
    "        c = random.randint(0, self.size - 1)\n",
    "        return (r, c)\n",
    "\n",
    "    def shape(self) -> Tuple[int, int]:\n",
    "        return (self.size, self.size)\n",
    "\n",
    "    def valid_states(self) -> List[Tuple[int,int]]:\n",
    "        return [(r, c) for r in range(self.size) for c in range(self.size)]\n",
    "\n",
    "\n",
    "def soft_value_iteration(mdp: GridMDP,\n",
    "                        rewards_2d: np.ndarray,\n",
    "                        discount_factor: float,\n",
    "                        max_iter=200,\n",
    "                        tol=1e-7) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Soft Value Iteration to generate 'soft-optimal' data:\n",
    "      V(s) = log( sum_a exp( R(s') + gamma V(s') ) ).\n",
    "    \"\"\"\n",
    "    states = mdp.valid_states()\n",
    "    V_dict = {s: 0.0 for s in states}\n",
    "    print(f\"[soft_value_iteration] Starting with max_iter={max_iter}, tol={tol}.\")\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        V_old = dict(V_dict)\n",
    "        delta = 0.0\n",
    "        for s in states:\n",
    "            vs = []\n",
    "            for a in mdp.actions:\n",
    "                s_next = mdp.step(s, a)\n",
    "                r_next = rewards_2d[s_next]\n",
    "                vs.append(r_next + discount_factor * V_old[s_next])\n",
    "            new_val = logsumexp(vs)\n",
    "            delta = max(delta, abs(new_val - V_dict[s]))\n",
    "            V_dict[s] = new_val\n",
    "        if delta < tol:\n",
    "            print(f\"[soft_value_iteration] Converged after {it+1} iterations (delta={delta:.3g}).\")\n",
    "            break\n",
    "\n",
    "    V_arr = np.zeros((mdp.size, mdp.size), dtype=float)\n",
    "    for s in states:\n",
    "        V_arr[s] = V_dict[s]\n",
    "    return V_arr\n",
    "\n",
    "\n",
    "def soft_policy(mdp: GridMDP, V_2d: np.ndarray, rewards_2d: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Pi(a|s) ~ exp( R(s') + gamma * V(s') )\n",
    "    \"\"\"\n",
    "    states = mdp.valid_states()\n",
    "    pol = {}\n",
    "    print(\"[soft_policy] Building soft policy from value function.\")\n",
    "    for s in states:\n",
    "        vs = []\n",
    "        for a in mdp.actions:\n",
    "            s_next = mdp.step(s, a)\n",
    "            r_next = rewards_2d[s_next]\n",
    "            vs.append(r_next + mdp.discount_factor * V_2d[s_next])\n",
    "        log_probs = vs - logsumexp(vs)\n",
    "        pol[s] = dict(zip(mdp.actions, np.exp(log_probs)))\n",
    "    return pol\n",
    "\n",
    "\n",
    "def generate_trajectories(mdp: GridMDP,\n",
    "                          N=20,\n",
    "                          max_steps=15,\n",
    "                          policy_type=\"soft\",\n",
    "                          seed=42) -> List[List[Tuple[Tuple[int,int], str, float]]]:\n",
    "    \"\"\"\n",
    "    Creates 'expert' trajectories from either a soft or a deterministic optimal policy.\n",
    "    \"\"\"\n",
    "    print(f\"[generate_trajectories] N={N}, max_steps={max_steps}, policy_type={policy_type}, seed={seed}.\")\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Soft VI on the true reward\n",
    "    V_true = soft_value_iteration(mdp, mdp.true_rewards, mdp.discount_factor)\n",
    "    pi_soft = soft_policy(mdp, V_true, mdp.true_rewards)\n",
    "\n",
    "    # Possibly convert to a deterministic, greedy policy\n",
    "    if policy_type == \"optimal\":\n",
    "        pi_dict = {}\n",
    "        states = mdp.valid_states()\n",
    "        for s in states:\n",
    "            vs = []\n",
    "            for a in mdp.actions:\n",
    "                s_next = mdp.step(s, a)\n",
    "                vs.append(mdp.true_rewards[s_next] + mdp.discount_factor * V_true[s_next])\n",
    "            best_idx = np.argmax(vs)\n",
    "            best_action = mdp.actions[best_idx]\n",
    "            pi_dict[s] = {a: (1.0 if a == best_action else 0.0) for a in mdp.actions}\n",
    "    else:\n",
    "        pi_dict = pi_soft\n",
    "\n",
    "    trajectories = []\n",
    "    for i in range(N):\n",
    "        traj = []\n",
    "        s_current = mdp.random_start_state()\n",
    "        for t in range(max_steps):\n",
    "            acts = list(pi_dict[s_current].keys())\n",
    "            probs = list(pi_dict[s_current].values())\n",
    "            chosen_action = np.random.choice(acts, p=probs)\n",
    "            s_next = mdp.step(s_current, chosen_action)\n",
    "            r_next = mdp.get_reward(s_next)\n",
    "            traj.append((s_current, chosen_action, r_next))\n",
    "            s_current = s_next\n",
    "        trajectories.append(traj)\n",
    "\n",
    "    print(f\"[generate_trajectories] Generated {N} trajectories (policy_type={policy_type}).\")\n",
    "    return trajectories\n",
    "\n",
    "\n",
    "def save_trajectories_csv(trajectories, filename=\"results/expert_data.csv\"):\n",
    "    \"\"\"\n",
    "    Save trajectory data to CSV in 'results/' folder.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for i, traj in enumerate(trajectories):\n",
    "        for t, (s, a, r) in enumerate(traj):\n",
    "            rows.append([i, t, s[0], s[1], a, r])\n",
    "    df = pd.DataFrame(rows, columns=[\"trajectory_id\",\"step\",\"row\",\"col\",\"action\",\"reward\"])\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"[save_trajectories_csv] Saved {len(trajectories)} trajectories to {filename}.\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# (2) IRL ALGORITHMS (NFXP, MAXENT)\n",
    "###############################################################################\n",
    "def reshape_1d_to_2d(vec: np.ndarray, size: int) -> np.ndarray:\n",
    "    return vec.reshape((size, size))\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# NFXP\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "def nfxp_log_likelihood(param_vec: np.ndarray,\n",
    "                        mdp: GridMDP,\n",
    "                        trajectories) -> float:\n",
    "    \"\"\"\n",
    "    Negative log-likelihood for NFXP:\n",
    "      1) Soft VI on param_2d\n",
    "      2) Induce policy\n",
    "      3) Compare log-likelihood to expert data\n",
    "    \"\"\"\n",
    "    size = mdp.size\n",
    "    param_2d = reshape_1d_to_2d(param_vec, size)\n",
    "\n",
    "    V_2d = soft_value_iteration(mdp, param_2d, mdp.discount_factor)\n",
    "    pi_dict = {}\n",
    "    states = mdp.valid_states()\n",
    "    for s in states:\n",
    "        vs = []\n",
    "        for a in mdp.actions:\n",
    "            s_next = mdp.step(s, a)\n",
    "            r_next = param_2d[s_next]\n",
    "            vs.append(r_next + mdp.discount_factor * V_2d[s_next])\n",
    "        log_probs = vs - logsumexp(vs)\n",
    "        pi_dict[s] = np.exp(log_probs)\n",
    "\n",
    "    eps = 1e-12\n",
    "    total_ll = 0\n",
    "    for traj in trajectories:\n",
    "        for (s, a, _) in traj:\n",
    "            a_idx = mdp.actions.index(a)\n",
    "            total_ll += np.log(pi_dict[s][a_idx] + eps)\n",
    "\n",
    "    return -total_ll\n",
    "\n",
    "\n",
    "def estimate_nfxp(mdp: GridMDP,\n",
    "                  trajectories,\n",
    "                  max_iter=300) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Minimizes negative log-likelihood by L-BFGS-B with param in [0,1].\n",
    "    \"\"\"\n",
    "    print(\"[estimate_nfxp] Estimating reward via nested fixed-point approach.\")\n",
    "    size = mdp.size\n",
    "    param_init = np.random.uniform(0.0, 1.0, size*size)\n",
    "    bounds = [(0,1)] * (size*size)\n",
    "\n",
    "    t0 = time.time()\n",
    "    result = minimize(\n",
    "        nfxp_log_likelihood,\n",
    "        param_init,\n",
    "        args=(mdp, trajectories),\n",
    "        method='L-BFGS-B',\n",
    "        bounds=bounds,\n",
    "        options={'maxiter': max_iter, 'disp': False}\n",
    "    )\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"[estimate_nfxp] L-BFGS took {elapsed:.2f}s for NxFP estimation.\")\n",
    "\n",
    "    param_est = reshape_1d_to_2d(result.x, size)\n",
    "    return param_est\n",
    "\n",
    "\n",
    "def save_rewards_csv(reward_2d: np.ndarray, filename=\"results/nfxp_est_rewards.csv\"):\n",
    "    df = pd.DataFrame(reward_2d)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"[save_rewards_csv] NFXP estimated rewards saved to {filename}.\")\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# MAXENT\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "def collect_state_visitation(mdp: GridMDP,\n",
    "                             pi_dict: dict,\n",
    "                             discount_factor: float,\n",
    "                             T=15) -> dict:\n",
    "    \"\"\"\n",
    "    Approx. state visitation frequencies by random rollouts from pi_dict.\n",
    "    \"\"\"\n",
    "    freq = defaultdict(float)\n",
    "    n_roll = 1000\n",
    "    for _ in range(n_roll):\n",
    "        s = mdp.random_start_state()\n",
    "        for t in range(T):\n",
    "            freq[s] += (discount_factor ** t)\n",
    "            pvals = pi_dict[s]\n",
    "            a_idx = np.random.choice(len(pvals), p=pvals)\n",
    "            a = mdp.actions[a_idx]\n",
    "            s = mdp.step(s, a)\n",
    "    for s in freq:\n",
    "        freq[s] /= n_roll\n",
    "    return freq\n",
    "\n",
    "\n",
    "def feature_vector(mdp: GridMDP, s: Tuple[int,int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    1-hot representation for cell s -> dimension = size*size.\n",
    "    \"\"\"\n",
    "    size = mdp.size\n",
    "    vec = np.zeros(size*size)\n",
    "    idx = s[0]*size + s[1]\n",
    "    vec[idx] = 1.0\n",
    "    return vec\n",
    "\n",
    "\n",
    "def compute_expert_feature_expectation(mdp: GridMDP,\n",
    "                                       trajectories,\n",
    "                                       discount_factor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    mu_D = average_{trajectories} sum_{t} gamma^t * one_hot(s_t)\n",
    "    \"\"\"\n",
    "    print(\"[compute_expert_feature_expectation] Building expert feature expectations.\")\n",
    "    size = mdp.size\n",
    "    mu_D = np.zeros(size*size)\n",
    "    N = len(trajectories)\n",
    "    for traj in trajectories:\n",
    "        for t, (s, a, r) in enumerate(traj):\n",
    "            mu_D += (discount_factor ** t) * feature_vector(mdp, s)\n",
    "    mu_D /= float(N)\n",
    "    return mu_D\n",
    "\n",
    "\n",
    "def maxent_estimation(mdp: GridMDP,\n",
    "                      trajectories,\n",
    "                      discount_factor=0.95,\n",
    "                      learning_rate=0.01,\n",
    "                      max_iter=50) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Basic MaxEnt IRL with param in [0,1] for each cell, gradient-based matching of mu_D - mu_pi.\n",
    "    \"\"\"\n",
    "    print(\"[maxent_estimation] Estimating reward via maximum entropy IRL.\")\n",
    "    size = mdp.size\n",
    "    mu_D = compute_expert_feature_expectation(mdp, trajectories, discount_factor)\n",
    "    param = np.random.uniform(0,1,size*size)\n",
    "\n",
    "    t0 = time.time()\n",
    "    for it in range(max_iter):\n",
    "        R_2d = reshape_1d_to_2d(param, size)\n",
    "        V_2d = soft_value_iteration(mdp, R_2d, discount_factor)\n",
    "\n",
    "        pi_dict = {}\n",
    "        for s in mdp.valid_states():\n",
    "            vs = []\n",
    "            for a in mdp.actions:\n",
    "                s_next = mdp.step(s, a)\n",
    "                vs.append(R_2d[s_next] + discount_factor * V_2d[s_next])\n",
    "            log_probs = vs - logsumexp(vs)\n",
    "            pi_dict[s] = np.exp(log_probs)\n",
    "\n",
    "        freq = collect_state_visitation(mdp, pi_dict, discount_factor, T=15)\n",
    "        mu_pi = np.zeros(size*size)\n",
    "        for s, val in freq.items():\n",
    "            idx = s[0]*size + s[1]\n",
    "            mu_pi[idx]+= val\n",
    "\n",
    "        grad = mu_D - mu_pi\n",
    "        param += learning_rate * grad\n",
    "        param = np.clip(param, 0, 1)\n",
    "\n",
    "        if it % 10==0:\n",
    "            loss = np.linalg.norm(grad)\n",
    "            print(f\"[maxent] iter={it}, loss={loss:.4f}\")\n",
    "            if loss < 1e-4:\n",
    "                break\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"[maxent_estimation] completed in {elapsed:.2f}s\")\n",
    "    return reshape_1d_to_2d(param, size)\n",
    "\n",
    "\n",
    "def save_maxent_rewards_csv(reward_2d: np.ndarray, filename=\"results/maxent_est_rewards.csv\"):\n",
    "    df = pd.DataFrame(reward_2d)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"[save_maxent_rewards_csv] MaxEnt estimated rewards saved to {filename}.\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# (3) ROUTE RECOMMENDATION (HARD VALUE ITERATION)\n",
    "###############################################################################\n",
    "def standard_value_iteration(mdp: GridMDP,\n",
    "                             reward_2d: np.ndarray,\n",
    "                             discount_factor: float,\n",
    "                             max_iter=500,\n",
    "                             tol=1e-7) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Hard-optimal value iteration: V(s) = max_a [ R(s') + gamma V(s') ].\n",
    "    \"\"\"\n",
    "    print(\"[standard_value_iteration] Running hard-optimal value iteration.\")\n",
    "    states = mdp.valid_states()\n",
    "    V_dict = {s:0.0 for s in states}\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        V_old = dict(V_dict)\n",
    "        delta = 0.0\n",
    "        for s in states:\n",
    "            best_val=-1e9\n",
    "            for a in mdp.actions:\n",
    "                s_next = mdp.step(s, a)\n",
    "                val = reward_2d[s_next] + discount_factor * V_old[s_next]\n",
    "                if val>best_val:\n",
    "                    best_val=val\n",
    "            delta = max(delta, abs(best_val - V_dict[s]))\n",
    "            V_dict[s] = best_val\n",
    "        if delta<tol:\n",
    "            print(f\"[standard_value_iteration] Converged after {it+1} iterations (delta={delta:.3g}).\")\n",
    "            break\n",
    "\n",
    "    V_arr = np.zeros((mdp.size, mdp.size), dtype=float)\n",
    "    for s in states:\n",
    "        V_arr[s] = V_dict[s]\n",
    "    return V_arr\n",
    "\n",
    "\n",
    "def get_hard_greedy_path(mdp: GridMDP,\n",
    "                         reward_2d: np.ndarray,\n",
    "                         blocked_cells=None,\n",
    "                         max_steps=30,\n",
    "                         discount_factor=None) -> List[Tuple[int,int]]:\n",
    "    \"\"\"\n",
    "    1) Standard value iteration on reward_2d, ignoring transitions to blocked cells.\n",
    "    2) From (0,0), greedily pick next cell for up to max_steps or until goal.\n",
    "    \"\"\"\n",
    "    print(\"[get_hard_greedy_path] Computing path with blocked_cells=\", blocked_cells)\n",
    "    if discount_factor is None:\n",
    "        discount_factor=mdp.discount_factor\n",
    "\n",
    "    # local function to skip blocked cells\n",
    "    def step_blocked(s, a):\n",
    "        (r,c)=s\n",
    "        if blocked_cells and (r,c) in blocked_cells:\n",
    "            return s\n",
    "        if a=='up'and r>0:\n",
    "            nxt=(r-1,c)\n",
    "            if blocked_cells and nxt in blocked_cells:\n",
    "                return s\n",
    "            return nxt\n",
    "        elif a=='down'and r<mdp.size-1:\n",
    "            nxt=(r+1,c)\n",
    "            if blocked_cells and nxt in blocked_cells:\n",
    "                return s\n",
    "            return nxt\n",
    "        elif a=='left'and c>0:\n",
    "            nxt=(r,c-1)\n",
    "            if blocked_cells and nxt in blocked_cells:\n",
    "                return s\n",
    "            return nxt\n",
    "        elif a=='right'and c<mdp.size-1:\n",
    "            nxt=(r,c+1)\n",
    "            if blocked_cells and nxt in blocked_cells:\n",
    "                return s\n",
    "            return nxt\n",
    "        return s\n",
    "\n",
    "    # Value iteration respecting blocked cells\n",
    "    states=mdp.valid_states()\n",
    "    V_dict={s:0.0 for s in states}\n",
    "    for _ in range(200):\n",
    "        V_old=dict(V_dict)\n",
    "        delta=0.0\n",
    "        for s in states:\n",
    "            best_val=-1e9\n",
    "            for a in mdp.actions:\n",
    "                s_next=step_blocked(s,a)\n",
    "                rew=reward_2d[s_next]\n",
    "                val=rew+discount_factor*V_old[s_next]\n",
    "                if val>best_val:\n",
    "                    best_val=val\n",
    "            delta=max(delta,abs(best_val-V_dict[s]))\n",
    "            V_dict[s]=best_val\n",
    "        if delta<1e-7:\n",
    "            break\n",
    "\n",
    "    # Greedily extract a path\n",
    "    path=[]\n",
    "    s_cur=(0,0)\n",
    "    for t in range(max_steps):\n",
    "        path.append(s_cur)\n",
    "        if s_cur==(mdp.size-1,mdp.size-1):\n",
    "            break\n",
    "        best_a=None\n",
    "        best_val=-1e9\n",
    "        for a in mdp.actions:\n",
    "            s_next=step_blocked(s_cur,a)\n",
    "            val=reward_2d[s_next]+discount_factor*V_dict[s_next]\n",
    "            if val>best_val:\n",
    "                best_val=val\n",
    "                best_a=a\n",
    "        s_cur=step_blocked(s_cur,best_a)\n",
    "    return path\n",
    "\n",
    "def get_hard_policy_matrix(mdp: GridMDP, reward_2d: np.ndarray)->np.ndarray:\n",
    "    \"\"\"\n",
    "    Hard-optimal policy matrix from standard_value_iteration for each cell.\n",
    "    \"\"\"\n",
    "    action_map={'up':0,'down':1,'left':2,'right':3}\n",
    "    V_2d = standard_value_iteration(mdp,reward_2d,mdp.discount_factor)\n",
    "    mat=np.zeros((mdp.size,mdp.size),dtype=int)\n",
    "\n",
    "    states=mdp.valid_states()\n",
    "    for s in states:\n",
    "        (r,c)=s\n",
    "        best_val=-1e9\n",
    "        best_a='up'\n",
    "        for a in mdp.actions:\n",
    "            s_next=mdp.step(s,a)\n",
    "            val=reward_2d[s_next]+mdp.discount_factor*V_2d[s_next]\n",
    "            if val>best_val:\n",
    "                best_val=val\n",
    "                best_a=a\n",
    "        mat[r,c]=action_map[best_a]\n",
    "    return mat\n",
    "\n",
    "###############################################################################\n",
    "# (4) METRICS & EVALUATION\n",
    "###############################################################################\n",
    "def compute_true_discounted_return(mdp: GridMDP, path)->float:\n",
    "    \"\"\" sum_{t} gamma^t * R_true(path[t]) \"\"\"\n",
    "    ret=0.0\n",
    "    gamma_t=1.0\n",
    "    for s in path:\n",
    "        ret+= gamma_t * mdp.true_rewards[s]\n",
    "        gamma_t*= mdp.discount_factor\n",
    "    return ret\n",
    "\n",
    "def compute_est_discounted_return(reward_2d: np.ndarray, mdp: GridMDP, path)->float:\n",
    "    \"\"\" sum_{t} gamma^t * R_est(path[t]) \"\"\"\n",
    "    ret=0.0\n",
    "    gamma_t=1.0\n",
    "    for s in path:\n",
    "        ret+= gamma_t*reward_2d[s]\n",
    "        gamma_t*= mdp.discount_factor\n",
    "    return ret\n",
    "\n",
    "def reward_rmse(true_rewards_2d, est_rewards_2d)->float:\n",
    "    return float(np.sqrt(np.mean((true_rewards_2d - est_rewards_2d)**2)))\n",
    "\n",
    "def reward_rank_corr(true_rewards_2d, est_rewards_2d)->float:\n",
    "    t=true_rewards_2d.flatten()\n",
    "    e=est_rewards_2d.flatten()\n",
    "    corr,_= spearmanr(t,e)\n",
    "    return float(corr)\n",
    "\n",
    "def policy_disagreement(mdp:GridMDP,\n",
    "                        R_true_2d: np.ndarray,\n",
    "                        R_est_2d: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compare the hard-optimal policy for R_true_2d vs. R_est_2d\n",
    "    by fraction of states that differ in best action.\n",
    "    \"\"\"\n",
    "    pol_true = get_hard_policy_matrix(mdp, R_true_2d)\n",
    "    pol_est  = get_hard_policy_matrix(mdp, R_est_2d)\n",
    "    states=mdp.valid_states()\n",
    "    diff=0\n",
    "    for s in states:\n",
    "        if pol_true[s[0],s[1]]!= pol_est[s[0],s[1]]:\n",
    "            diff+=1\n",
    "    return diff/len(states)\n",
    "\n",
    "###############################################################################\n",
    "# (5) MAIN EXECUTION\n",
    "###############################################################################\n",
    "def main():\n",
    "    print(\"[main] Starting main execution...\")\n",
    "\n",
    "    # Hyperparameters\n",
    "    K=5\n",
    "    N=10\n",
    "    max_steps=15\n",
    "    seed=42\n",
    "\n",
    "    # 0) Build MDP\n",
    "    mdp=GridMDP(size=K, discount_factor=0.95, seed=seed)\n",
    "    mdp.true_rewards[K-1, K-1] = 20.0  # Increase goal reward for clarity\n",
    "    print(f\"[main] Created GridMDP shape={mdp.shape()}, discount={mdp.discount_factor:.2f}\")\n",
    "\n",
    "    # 1) Gather Expert Data\n",
    "    print(\"[main] Generating expert data...\")\n",
    "    trajectories=generate_trajectories(mdp, N=N, max_steps=max_steps, policy_type=\"soft\", seed=seed)\n",
    "    save_trajectories_csv(trajectories, filename=\"results/expert_data.csv\")\n",
    "\n",
    "    # 2) Estimate Rewards: NFXP\n",
    "    print(\"[main] Estimating NFXP rewards...\")\n",
    "    t0=time.time()\n",
    "    nfxp_est=estimate_nfxp(mdp, trajectories, max_iter=500)\n",
    "    time_nfxp=time.time()-t0\n",
    "    save_rewards_csv(nfxp_est, filename=\"results/nfxp_est_rewards.csv\")\n",
    "\n",
    "    # 3) Estimate Rewards: MaxEnt\n",
    "    print(\"[main] Estimating MaxEnt rewards...\")\n",
    "    t1=time.time()\n",
    "    maxent_est=maxent_estimation(mdp, trajectories, mdp.discount_factor, learning_rate=0.01, max_iter=50)\n",
    "    time_maxent=time.time()-t1\n",
    "    save_maxent_rewards_csv(maxent_est, filename=\"results/maxent_est_rewards.csv\")\n",
    "\n",
    "    # 4) Evaluate reward quality (RMSE, rank correlation, policy difference)\n",
    "    print(\"[main] Computing evaluation metrics (RMSE, rank corr, policy diff).\")\n",
    "    rmse_nfxp  = reward_rmse(mdp.true_rewards, nfxp_est)\n",
    "    rmse_maxent= reward_rmse(mdp.true_rewards, maxent_est)\n",
    "    rcorr_nfxp   = reward_rank_corr(mdp.true_rewards, nfxp_est)\n",
    "    rcorr_maxent = reward_rank_corr(mdp.true_rewards, maxent_est)\n",
    "    pdiff_nfxp   = policy_disagreement(mdp, mdp.true_rewards, nfxp_est)\n",
    "    pdiff_maxent = policy_disagreement(mdp, mdp.true_rewards, maxent_est)\n",
    "\n",
    "    # 5) Evaluate route recommendations in unblocked scenario\n",
    "    print(\"[main] Building route recommendations in unblocked scenario...\")\n",
    "    route_gt    = get_hard_greedy_path(mdp, mdp.true_rewards, None, max_steps)\n",
    "    route_nfxp  = get_hard_greedy_path(mdp, nfxp_est,       None, max_steps)\n",
    "    route_maxent= get_hard_greedy_path(mdp, maxent_est,     None, max_steps)\n",
    "\n",
    "    gt_est_unblocked    = compute_est_discounted_return(mdp.true_rewards, mdp, route_gt)\n",
    "    gt_true_unblocked   = compute_true_discounted_return(mdp, route_gt)\n",
    "    nfxp_est_unblocked  = compute_est_discounted_return(nfxp_est, mdp, route_nfxp)\n",
    "    nfxp_true_unblocked = compute_true_discounted_return(mdp, route_nfxp)\n",
    "    maxent_est_unblocked= compute_est_discounted_return(maxent_est, mdp, route_maxent)\n",
    "    maxent_true_unblocked= compute_true_discounted_return(mdp, route_maxent)\n",
    "\n",
    "    # 6) Evaluate route recommendations in blocked scenario\n",
    "    print(\"[main] Building route recommendations in blocked scenario...\")\n",
    "    blocked_cells=None\n",
    "    if K>=5:\n",
    "        blocked_cells=[(2,2),(2,3),(3,2),(3,3)]\n",
    "    route_gt_block     = get_hard_greedy_path(mdp, mdp.true_rewards, blocked_cells, max_steps)\n",
    "    route_nfxp_block   = get_hard_greedy_path(mdp, nfxp_est,         blocked_cells, max_steps)\n",
    "    route_maxent_block = get_hard_greedy_path(mdp, maxent_est,       blocked_cells, max_steps)\n",
    "\n",
    "    gt_est_block       = compute_est_discounted_return(mdp.true_rewards, mdp, route_gt_block)\n",
    "    gt_true_block      = compute_true_discounted_return(mdp, route_gt_block)\n",
    "    nfxp_est_block     = compute_est_discounted_return(nfxp_est, mdp, route_nfxp_block)\n",
    "    nfxp_true_block    = compute_true_discounted_return(mdp, route_nfxp_block)\n",
    "    maxent_est_block   = compute_est_discounted_return(maxent_est, mdp, route_maxent_block)\n",
    "    maxent_true_block  = compute_true_discounted_return(mdp, route_maxent_block)\n",
    "\n",
    "    # 7) Visualization of final policies\n",
    "    print(\"[main] Creating final policy comparison figure...\")\n",
    "    pol_gt     = get_hard_policy_matrix(mdp, mdp.true_rewards)\n",
    "    pol_nfxp   = get_hard_policy_matrix(mdp, nfxp_est)\n",
    "    pol_maxent = get_hard_policy_matrix(mdp, maxent_est)\n",
    "\n",
    "    fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "    plot_reward_and_policy(axes[0], mdp.true_rewards, pol_gt,     title=\"GroundTruth (Hard Policy)\")\n",
    "    plot_reward_and_policy(axes[1], nfxp_est,         pol_nfxp,   title=\"NFXP (Hard Policy)\")\n",
    "    plot_reward_and_policy(axes[2], maxent_est,       pol_maxent, title=\"MaxEnt (Hard Policy)\")\n",
    "    plt.tight_layout()\n",
    "    out_fig1 = \"results/reward_policy_comparison.png\"\n",
    "    plt.savefig(out_fig1, dpi=150)\n",
    "    print(f\"[main] Saved figure: {out_fig1}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Compare unblocked vs blocked routes for NFXP/MaxEnt\n",
    "    print(\"[main] Creating recommended paths figure (unblocked vs. blocked).\")\n",
    "    fig2, axes2 = plt.subplots(1,2, figsize=(12,5))\n",
    "    plot_two_paths(axes2[0], K, route_nfxp, route_maxent,\n",
    "                   blocked_cells=None,\n",
    "                   title=\"Unblocked: NFXP(blue) vs MaxEnt(red)\")\n",
    "    plot_two_paths(axes2[1], K, route_nfxp_block, route_maxent_block,\n",
    "                   blocked_cells=blocked_cells,\n",
    "                   title=\"Blocked: NFXP(blue) vs MaxEnt(red)\")\n",
    "    plt.tight_layout()\n",
    "    out_fig2 = \"results/recommended_paths.png\"\n",
    "    plt.savefig(out_fig2, dpi=150)\n",
    "    print(f\"[main] Saved figure: {out_fig2}\")\n",
    "    plt.close(fig2)\n",
    "\n",
    "    # 8) Prepare final results table\n",
    "    print(\"[main] Preparing final results table and saving to CSV.\")\n",
    "    time_gt=0.0\n",
    "    rmse_gt=0.0\n",
    "    rcorr_gt=1.0\n",
    "    pdiff_gt=0.0\n",
    "\n",
    "    data_table = [\n",
    "        [\"Hyperparams:\", f\"K={K}, N={N}, max_steps={max_steps}\", \"\", \"\"],\n",
    "        [\"Method\",\"NFXP\",\"MaxEnt\",\"GroundTruth\",\"\"],\n",
    "        [\"Time (sec)\",f\"{time_nfxp:.3f}\",f\"{time_maxent:.3f}\",f\"{time_gt:.3f}\",\"\"],\n",
    "        [\"RMSE(Reward)\",f\"{rmse_nfxp:.3f}\",f\"{rmse_maxent:.3f}\",f\"{rmse_gt:.3f}\",\"\"],\n",
    "        [\"RankCorr(Reward)\",f\"{rcorr_nfxp:.3f}\",f\"{rcorr_maxent:.3f}\",f\"{rcorr_gt:.3f}\",\"\"],\n",
    "        [\"PolDiff(Reward)\",f\"{pdiff_nfxp:.3f}\",f\"{pdiff_maxent:.3f}\",f\"{pdiff_gt:.3f}\",\"\"],\n",
    "        [\"--Unblocked--\",\"NFXP\",\"MaxEnt\",\"GroundTruth\",\"\"],\n",
    "        [\"Return (Est R)\",f\"{nfxp_est_unblocked:.3f}\",f\"{maxent_est_unblocked:.3f}\",f\"{gt_est_unblocked:.3f}\",\"\"],\n",
    "        [\"Return (True R)\",f\"{nfxp_true_unblocked:.3f}\",f\"{maxent_true_unblocked:.3f}\",f\"{gt_true_unblocked:.3f}\",\"\"],\n",
    "        [\"--Blocked--\",\"NFXP\",\"MaxEnt\",\"GroundTruth\",\"\"],\n",
    "        [\"Return (Est R)\",f\"{nfxp_est_block:.3f}\",f\"{maxent_est_block:.3f}\",f\"{gt_est_block:.3f}\",\"\"],\n",
    "        [\"Return (True R)\",f\"{nfxp_true_block:.3f}\",f\"{maxent_true_block:.3f}\",f\"{gt_true_block:.3f}\",\"\"]\n",
    "    ]\n",
    "\n",
    "    # Print table in console\n",
    "    if USE_TABULATE:\n",
    "        print(\"\\nFINAL RESULTS\\n\")\n",
    "        print(tabulate(data_table, tablefmt=\"github\"))\n",
    "    else:\n",
    "        print(\"\\nFINAL RESULTS (Fallback)\\n\")\n",
    "        for row in data_table:\n",
    "            print(\" | \".join(map(str, row)))\n",
    "\n",
    "    # Also save final results as CSV\n",
    "    # We'll assume the first column is a label row, subsequent columns are data\n",
    "    final_csv_path = \"results/final_results.csv\"\n",
    "    all_rows = []\n",
    "    for row in data_table:\n",
    "        # Make sure everything is string\n",
    "        all_rows.append([str(x) for x in row])\n",
    "\n",
    "    df_table = pd.DataFrame(all_rows)\n",
    "    df_table.to_csv(final_csv_path, index=False, header=False)\n",
    "    print(f\"[main] Saved final results to {final_csv_path}\")\n",
    "\n",
    "    print(\"[main] Done. See results above.\")\n",
    "\n",
    "###############################################################################\n",
    "# Additional Helpers for Visualization\n",
    "###############################################################################\n",
    "def plot_reward_and_policy(ax, reward_2d: np.ndarray, policy_mat: np.ndarray, title=\"\"):\n",
    "    \"\"\"\n",
    "    Display the reward grid plus an arrow for each cell's best action.\n",
    "    action_map: 0=up,1=down,2=left,3=right\n",
    "    \"\"\"\n",
    "    size = reward_2d.shape[0]\n",
    "    im = ax.imshow(reward_2d, cmap='viridis', origin='upper')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(range(size))\n",
    "    ax.set_yticks(range(size))\n",
    "    action_arrow = {\n",
    "        0: (0, -0.3),   # up\n",
    "        1: (0,  0.3),   # down\n",
    "        2: (-0.3, 0),   # left\n",
    "        3: (0.3,  0)    # right\n",
    "    }\n",
    "    for r in range(size):\n",
    "        for c in range(size):\n",
    "            a = policy_mat[r,c]\n",
    "            dx, dy = action_arrow[a]\n",
    "            ax.arrow(c, r, dx, dy,\n",
    "                     color='white', head_width=0.1,\n",
    "                     head_length=0.1, length_includes_head=True)\n",
    "\n",
    "def plot_two_paths(ax, size,\n",
    "                   path_nfxp, path_maxent,\n",
    "                   blocked_cells=None,\n",
    "                   title=\"\"):\n",
    "    \"\"\"\n",
    "    Show NFXP path (blue) vs. MaxEnt path (red) in a single plot.\n",
    "    If blocked_cells is given, shade them in gray.\n",
    "    \"\"\"\n",
    "    ax.set_xlim(-0.5, size-0.5)\n",
    "    ax.set_ylim(-0.5, size-0.5)\n",
    "    ax.set_xticks(range(size))\n",
    "    ax.set_yticks(range(size))\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_aspect('equal','box')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Draw grid lines\n",
    "    for i in range(size+1):\n",
    "        ax.axhline(i-0.5,color='black')\n",
    "        ax.axvline(i-0.5,color='black')\n",
    "\n",
    "    if blocked_cells:\n",
    "        for (r,c) in blocked_cells:\n",
    "            ax.fill_between([c-0.5,c+0.5], r-0.5, r+0.5, color='gray', alpha=0.5)\n",
    "\n",
    "    # NFXP path\n",
    "    xs_n = [s[1] for s in path_nfxp]\n",
    "    ys_n = [s[0] for s in path_nfxp]\n",
    "    ax.plot(xs_n, ys_n, 'o-', color='blue', label='NFXP path')\n",
    "\n",
    "    # MaxEnt path\n",
    "    xs_m = [s[1] for s in path_maxent]\n",
    "    ys_m = [s[0] for s in path_maxent]\n",
    "    ax.plot(xs_m, ys_m, 's-', color='red', label='MaxEnt path')\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "###############################################################################\n",
    "# Execute main()\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
