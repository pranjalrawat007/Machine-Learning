{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=0000, Loss=5618.3013, NLL=5493.1523, Bellman=12.5149, L2=0.0000\n",
      "  R=[ 0.5  -0.05 -0.05], RewDiff=0.4500\n",
      "  PolDiff=0.3829, ValDiff=38.5110\n",
      "\n",
      "Iter=0100, Loss=5456.2183, NLL=5432.2593, Bellman=2.3951, L2=7.6653\n",
      "  R=[ 0.5       -1.3857818 -1.475164 ], RewDiff=3.2109\n",
      "  PolDiff=0.0947, ValDiff=38.4013\n",
      "\n",
      "Iter=0200, Loss=5455.5454, NLL=5432.2168, Bellman=2.3321, L2=7.6924\n",
      "  R=[ 0.5       -1.3609862 -1.4494882], RewDiff=3.1605\n",
      "  PolDiff=0.0931, ValDiff=37.8794\n",
      "\n",
      "Iter=0300, Loss=5454.6792, NLL=5432.1943, Bellman=2.2476, L2=8.6416\n",
      "  R=[ 0.5       -1.3328481 -1.4214159], RewDiff=3.1043\n",
      "  PolDiff=0.0926, ValDiff=37.2007\n",
      "\n",
      "Iter=0400, Loss=5453.6729, NLL=5432.1694, Bellman=2.1493, L2=10.9694\n",
      "  R=[ 0.5       -1.2993728 -1.3880069], RewDiff=3.0374\n",
      "  PolDiff=0.0919, ValDiff=36.3942\n",
      "\n",
      "Iter=0500, Loss=5452.5601, NLL=5432.1421, Bellman=2.0403, L2=15.1909\n",
      "  R=[ 0.5       -1.2613928 -1.3501029], RewDiff=2.9615\n",
      "  PolDiff=0.0912, ValDiff=35.4791\n",
      "\n",
      "Iter=0600, Loss=5451.4160, NLL=5432.1602, Bellman=1.9234, L2=21.7929\n",
      "  R=[ 0.5       -1.2195212 -1.3083168], RewDiff=2.8778\n",
      "  PolDiff=0.0904, ValDiff=34.4702\n",
      "\n",
      "Iter=0700, Loss=5450.2163, NLL=5432.1758, Bellman=1.8009, L2=31.2235\n",
      "  R=[ 0.5       -1.1742779 -1.2631643], RewDiff=2.7874\n",
      "  PolDiff=0.0895, ValDiff=33.3802\n",
      "\n",
      "Iter=0800, Loss=5448.9443, NLL=5432.1499, Bellman=1.6750, L2=43.8808\n",
      "  R=[ 0.5       -1.1261275 -1.2151151], RewDiff=2.6912\n",
      "  PolDiff=0.0887, ValDiff=32.2201\n",
      "\n",
      "Iter=0900, Loss=5447.6328, NLL=5432.0967, Bellman=1.5476, L2=60.1018\n",
      "  R=[ 0.5       -1.0755124 -1.1646018], RewDiff=2.5901\n",
      "  PolDiff=0.0880, ValDiff=31.0006\n",
      "\n",
      "Iter=1000, Loss=5446.3433, NLL=5432.0601, Bellman=1.4203, L2=80.1529\n",
      "  R=[ 0.5       -1.0228449 -1.1120421], RewDiff=2.4849\n",
      "  PolDiff=0.0872, ValDiff=29.7316\n",
      "\n",
      "==== Final Results ====\n",
      "Learned Rewards = [ 0.5       -1.0228449 -1.1120421]\n",
      "True Rewards    = [0.5  0.25 0.1 ]\n",
      "Learned Q =\n",
      "[[3.4635828 3.5315838 3.8327434]\n",
      " [2.8147655 2.431752  2.5127413]\n",
      " [2.4352224 2.712356  2.3441136]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "############################################################################\n",
    "# 1) Triangular MDP Setup\n",
    "############################################################################\n",
    "n_states = 3\n",
    "n_actions = 3\n",
    "gamma = 0.9\n",
    "eps = 0.05\n",
    "\n",
    "def build_transition_matrix(eps=0.05):\n",
    "    \"\"\"\n",
    "    P[s, a, s'] = probability of going to s' from s when taking action a.\n",
    "    Actions: 0=left, 1=right, 2=stay.\n",
    "    \"\"\"\n",
    "    P = np.zeros((n_states, n_actions, n_states))\n",
    "\n",
    "    def dist(target):\n",
    "        d = np.ones(n_states) * (eps / n_states)\n",
    "        d[target] += (1.0 - eps)\n",
    "        return d\n",
    "    \n",
    "    for s in range(n_states):\n",
    "        if s == 0:\n",
    "            P[s, 0] = dist(2)  # left from 0 -> 2\n",
    "            P[s, 1] = dist(1)  # right from 0 -> 1\n",
    "            P[s, 2] = dist(0)  # stay in 0\n",
    "        elif s == 1:\n",
    "            P[s, 0] = dist(0)  # left from 1 -> 0\n",
    "            P[s, 1] = dist(2)  # right from 1 -> 2\n",
    "            P[s, 2] = dist(1)  # stay in 1\n",
    "        else:  # s == 2\n",
    "            P[s, 0] = dist(1)  # left from 2 -> 1\n",
    "            P[s, 1] = dist(0)  # right from 2 -> 0\n",
    "            P[s, 2] = dist(2)  # stay in 2\n",
    "\n",
    "    return P\n",
    "\n",
    "P_np = build_transition_matrix(eps)\n",
    "P = torch.tensor(P_np, dtype=torch.float)\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# 2) True Rewards & Generate Expert Data\n",
    "############################################################################\n",
    "R_true_full = torch.tensor([0.5, 0.25, 0.1])  # one reward per state\n",
    "\n",
    "def compute_softmax_policy(Q):\n",
    "    \"\"\"\n",
    "    pi[s,a] = softmax(Q[s,a]) along actions\n",
    "    \"\"\"\n",
    "    shifted = Q - Q.max(dim=1, keepdim=True).values  # stable\n",
    "    expQ = shifted.exp()\n",
    "    denom = expQ.sum(dim=1, keepdim=True)\n",
    "    return expQ / denom\n",
    "\n",
    "def generate_expert_trajectories(Q_true, n_trajectories=100, traj_len=5):\n",
    "    \"\"\"\n",
    "    Sample states/actions from the policy induced by Q_true.\n",
    "    Return list of (s,a) pairs for all trajectories.\n",
    "    \"\"\"\n",
    "    policy = compute_softmax_policy(Q_true)\n",
    "    transitions = []\n",
    "    for _ in range(n_trajectories):\n",
    "        s = np.random.choice(n_states)  # random start\n",
    "        for _step in range(traj_len):\n",
    "            a = np.random.choice(n_actions, p=policy[s].detach().numpy())\n",
    "            transitions.append((s, a))\n",
    "            # next state\n",
    "            s_next = np.random.choice(n_states, p=P_np[s,a])\n",
    "            s = s_next\n",
    "    return transitions\n",
    "\n",
    "# We'll do a quick approximate \"soft\" Q-iteration to get Q_true for demonstration:\n",
    "Q_true_full = torch.zeros(n_states, n_actions)\n",
    "for _ in range(200):\n",
    "    Qnew = Q_true_full.clone()\n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            sum_sprime = 0.0\n",
    "            for s_next in range(n_states):\n",
    "                # V(s_next) = logsumexp(Q[s_next,:])\n",
    "                V_snext = torch.logsumexp(Q_true_full[s_next], dim=0)\n",
    "                sum_sprime += P[s,a,s_next] * V_snext\n",
    "            Qnew[s,a] = R_true_full[s] + gamma * sum_sprime\n",
    "    if torch.max(torch.abs(Qnew - Q_true_full)) < 1e-6:\n",
    "        break\n",
    "    Q_true_full = Qnew\n",
    "\n",
    "expert_data = generate_expert_trajectories(Q_true_full, n_trajectories=500, traj_len=10)\n",
    "policy_true = compute_softmax_policy(Q_true_full)\n",
    "V_true = torch.logsumexp(Q_true_full, dim=1)  # V(s)=logsumexp(Q(s,a))\n",
    "\n",
    "############################################################################\n",
    "# 3) Single-Loop IRL: Fix R(0)=0.5, L2 Regularization\n",
    "############################################################################\n",
    "\n",
    "# -------------------\n",
    "# (a) Setup Learnable Parameters\n",
    "# -------------------\n",
    "# We fix the first state's reward = 0.5 (the true value),\n",
    "# and only learn the latter two states' rewards.\n",
    "R_param = torch.zeros(2, requires_grad=True)  # for states 1,2\n",
    "Q = torch.zeros(n_states, n_actions, requires_grad=True)  \n",
    "\n",
    "optimizer = torch.optim.Adam([R_param, Q], lr=0.05)\n",
    "alpha_bellman = 10.0   # weight on Bellman error\n",
    "alpha_l2      = 0.001  # L2 regularization strength\n",
    "max_iters     = 1001\n",
    "print_every   = 100\n",
    "\n",
    "def full_reward_vector(R_param):\n",
    "    \"\"\"\n",
    "    Reconstruct R(s) where R(0)=0.5, R(1)=R_param[0], R(2)=R_param[1].\n",
    "    \"\"\"\n",
    "    # First state is pinned at 0.5\n",
    "    return torch.cat([torch.tensor([0.5]), R_param], dim=0)\n",
    "\n",
    "for it in range(max_iters):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 1) Reconstruct the full reward vector\n",
    "    R_learn = full_reward_vector(R_param)  # shape [3]\n",
    "\n",
    "    # 2) Compute policy from current Q\n",
    "    policy = compute_softmax_policy(Q)\n",
    "\n",
    "    # 3) Negative Log-Likelihood of expert data\n",
    "    nll = 0.0\n",
    "    for (s_exp, a_exp) in expert_data:\n",
    "        nll = nll - torch.log(policy[s_exp, a_exp] + 1e-10)\n",
    "\n",
    "    # 4) Soft Bellman Consistency: Q(s,a) ~ R(s) + gamma * Î£ P(s,a,s') * logsumexp(Q[s',:])\n",
    "    bellman_error = 0.0\n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            sum_sprime = 0.0\n",
    "            for s_next in range(n_states):\n",
    "                V_snext = torch.logsumexp(Q[s_next], dim=0)\n",
    "                sum_sprime += P[s,a,s_next] * V_snext\n",
    "            target = R_learn[s] + gamma * sum_sprime\n",
    "            bellman_error += (Q[s,a] - target)**2\n",
    "\n",
    "    # 5) L2 regularization on Q and learnable R\n",
    "    l2_reg = Q.pow(2).sum() + R_param.pow(2).sum()\n",
    "\n",
    "    # 6) Total Loss\n",
    "    loss = nll + alpha_bellman * bellman_error + alpha_l2 * l2_reg\n",
    "\n",
    "    # 7) Backprop + Update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # ----------------\n",
    "    # Debugging Info\n",
    "    # ----------------\n",
    "    if it % print_every == 0:\n",
    "        with torch.no_grad():\n",
    "            R_current = full_reward_vector(R_param)\n",
    "            policy_diff = (policy - policy_true).abs().sum().item()\n",
    "            V_current = torch.logsumexp(Q, dim=1)\n",
    "            val_diff = (V_current - V_true).abs().sum().item()\n",
    "            rew_diff = (R_current - R_true_full).abs().sum().item()\n",
    "\n",
    "            print(f\"Iter={it:04d}, \"\n",
    "                  f\"Loss={loss.item():.4f}, \"\n",
    "                  f\"NLL={nll.item():.4f}, \"\n",
    "                  f\"Bellman={bellman_error.item():.4f}, \"\n",
    "                  f\"L2={l2_reg.item():.4f}\")\n",
    "            print(f\"  R={R_current.detach().numpy()}, RewDiff={rew_diff:.4f}\")\n",
    "            print(f\"  PolDiff={policy_diff:.4f}, ValDiff={val_diff:.4f}\")\n",
    "            print(\"\")\n",
    "\n",
    "# ----------------\n",
    "# Final Results\n",
    "# ----------------\n",
    "R_final = full_reward_vector(R_param).detach().numpy()\n",
    "print(\"==== Final Results ====\")\n",
    "print(f\"Learned Rewards = {R_final}\")\n",
    "print(f\"True Rewards    = {R_true_full.numpy()}\")\n",
    "print(\"Learned Q =\")\n",
    "print(Q.detach().numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
