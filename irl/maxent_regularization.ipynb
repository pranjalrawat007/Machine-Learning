{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 01000 | Loss(SVF): 0.0076 | GradNorm: 0.0076 | PolDiff: 0.9582 | ValDiff: 11.8977 | RewDiff: 1.5524\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3938 0.3171 0.2891]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5041 16.6004 16.0799]\n",
      "    s=1, Q=[16.6887 17.1129 17.2093]\n",
      "    s=2, Q=[17.0966 16.576  17.0002]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.3629 0.3996 0.2375]\n",
      "    s=1, π=[0.2375 0.3629 0.3996]\n",
      "    s=2, π=[0.3996 0.2375 0.3629]\n",
      "  Reward: [0.2974 0.9063 0.7935]\n",
      "\n",
      "Iter 02000 | Loss(SVF): 0.0008 | GradNorm: 0.0008 | PolDiff: 0.9798 | ValDiff: 11.9721 | RewDiff: 1.5823\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3911 0.3227 0.2863]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5035 16.6286 16.0745]\n",
      "    s=1, Q=[16.7225 17.1515 17.2766]\n",
      "    s=2, Q=[17.1303 16.5762 17.0052]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.3591 0.407  0.2339]\n",
      "    s=1, π=[0.2339 0.3591 0.407 ]\n",
      "    s=2, π=[0.407  0.2339 0.3591]\n",
      "  Reward: [0.2825 0.9305 0.7842]\n",
      "\n",
      "Iter 03000 | Loss(SVF): 0.0001 | GradNorm: 0.0001 | PolDiff: 0.9832 | ValDiff: 11.9831 | RewDiff: 1.5872\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3906 0.3232 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5043 16.6316 16.0732]\n",
      "    s=1, Q=[16.7263 17.1573 17.2846]\n",
      "    s=2, Q=[17.1357 16.5774 17.0084]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4077 0.2333]\n",
      "    s=1, π=[0.2333 0.359  0.4077]\n",
      "    s=2, π=[0.4077 0.2333 0.359 ]\n",
      "  Reward: [0.28   0.9331 0.7841]\n",
      "\n",
      "Iter 04000 | Loss(SVF): 0.0000 | GradNorm: 0.0000 | PolDiff: 0.9838 | ValDiff: 11.9849 | RewDiff: 1.5880\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3906 0.3233 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5045 16.6319 16.073 ]\n",
      "    s=1, Q=[16.7267 17.1582 17.2857]\n",
      "    s=2, Q=[17.1366 16.5776 17.0091]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4078 0.2332]\n",
      "    s=1, π=[0.2332 0.359  0.4078]\n",
      "    s=2, π=[0.4078 0.2332 0.359 ]\n",
      "  Reward: [0.2796 0.9334 0.7843]\n",
      "\n",
      "Iter 05000 | Loss(SVF): 0.0000 | GradNorm: 0.0000 | PolDiff: 0.9839 | ValDiff: 11.9851 | RewDiff: 1.5882\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3905 0.3233 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5045 16.632  16.0729]\n",
      "    s=1, Q=[16.7268 17.1584 17.2859]\n",
      "    s=2, Q=[17.1367 16.5777 17.0093]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4078 0.2332]\n",
      "    s=1, π=[0.2332 0.359  0.4078]\n",
      "    s=2, π=[0.4078 0.2332 0.359 ]\n",
      "  Reward: [0.2795 0.9334 0.7843]\n",
      "\n",
      "Iter 06000 | Loss(SVF): 0.0000 | GradNorm: 0.0000 | PolDiff: 0.9839 | ValDiff: 11.9852 | RewDiff: 1.5882\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3905 0.3233 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5045 16.632  16.0729]\n",
      "    s=1, Q=[16.7268 17.1584 17.2859]\n",
      "    s=2, Q=[17.1368 16.5777 17.0093]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4078 0.2332]\n",
      "    s=1, π=[0.2332 0.359  0.4078]\n",
      "    s=2, π=[0.4078 0.2332 0.359 ]\n",
      "  Reward: [0.2795 0.9334 0.7843]\n",
      "\n",
      "Iter 07000 | Loss(SVF): 0.0000 | GradNorm: 0.0000 | PolDiff: 0.9839 | ValDiff: 11.9852 | RewDiff: 1.5882\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3905 0.3233 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5045 16.632  16.0729]\n",
      "    s=1, Q=[16.7268 17.1584 17.2859]\n",
      "    s=2, Q=[17.1368 16.5777 17.0093]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4078 0.2332]\n",
      "    s=1, π=[0.2332 0.359  0.4078]\n",
      "    s=2, π=[0.4078 0.2332 0.359 ]\n",
      "  Reward: [0.2795 0.9334 0.7843]\n",
      "\n",
      "Iter 08000 | Loss(SVF): 0.0000 | GradNorm: 0.0000 | PolDiff: 0.9839 | ValDiff: 11.9852 | RewDiff: 1.5882\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3905 0.3233 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5045 16.632  16.0729]\n",
      "    s=1, Q=[16.7268 17.1584 17.2859]\n",
      "    s=2, Q=[17.1368 16.5777 17.0093]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4078 0.2332]\n",
      "    s=1, π=[0.2332 0.359  0.4078]\n",
      "    s=2, π=[0.4078 0.2332 0.359 ]\n",
      "  Reward: [0.2795 0.9334 0.7843]\n",
      "\n",
      "Iter 09000 | Loss(SVF): 0.0000 | GradNorm: 0.0000 | PolDiff: 0.9839 | ValDiff: 11.9852 | RewDiff: 1.5882\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3905 0.3233 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5045 16.632  16.0729]\n",
      "    s=1, Q=[16.7268 17.1584 17.2859]\n",
      "    s=2, Q=[17.1368 16.5777 17.0093]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4078 0.2332]\n",
      "    s=1, π=[0.2332 0.359  0.4078]\n",
      "    s=2, π=[0.4078 0.2332 0.359 ]\n",
      "  Reward: [0.2795 0.9334 0.7843]\n",
      "\n",
      "Iter 10000 | Loss(SVF): 0.0000 | GradNorm: 0.0000 | PolDiff: 0.9839 | ValDiff: 11.9852 | RewDiff: 1.5882\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3905 0.3233 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5045 16.632  16.0729]\n",
      "    s=1, Q=[16.7268 17.1584 17.2859]\n",
      "    s=2, Q=[17.1368 16.5777 17.0093]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4078 0.2332]\n",
      "    s=1, π=[0.2332 0.359  0.4078]\n",
      "    s=2, π=[0.4078 0.2332 0.359 ]\n",
      "  Reward: [0.2795 0.9334 0.7843]\n",
      "\n",
      "\n",
      "Final Results:\n",
      "True Rewards:       [0.5  0.25 0.1 ]\n",
      "Estimated Rewards:  [0.2795 0.9334 0.7843]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Triangular MDP Setup\n",
    "# ---------------------------\n",
    "# States: 0,1,2 (think of them arranged in a triangle)\n",
    "# Actions: 0=left, 1=right, 2=stay\n",
    "# We'll define transitions with a small noise probability eps.\n",
    "eps = 0.05\n",
    "n_states = 3\n",
    "n_actions = 3\n",
    "gamma = 0.9\n",
    "\n",
    "def build_transition_matrix(eps=0.05):\n",
    "    \"\"\"\n",
    "    Triangular 3-state MDP with 3 actions.\n",
    "    P[(s, a)] = probability distribution over next states [p0, p1, p2].\n",
    "    \n",
    "    Action 0 = 'left' \n",
    "    Action 1 = 'right'\n",
    "    Action 2 = 'stay'\n",
    "    \"\"\"\n",
    "    # Initialize dictionary\n",
    "    P = {}\n",
    "    # Helper to set distribution with main target and small noise to others\n",
    "    def dist(target):\n",
    "        # With prob (1 - eps) go to the target, with prob eps the rest is evenly distributed\n",
    "        d = np.ones(n_states) * (eps / (n_states))\n",
    "        d[target] += (1 - eps)\n",
    "        return d\n",
    "    \n",
    "    # For each state s, define transitions for each action\n",
    "    for s in range(n_states):\n",
    "        # Action 'left'\n",
    "        if s == 0:\n",
    "            P[(0, 0)] = dist(target=2)  # from 0, left goes to 2 ideally\n",
    "        elif s == 1:\n",
    "            P[(1, 0)] = dist(target=0)  # from 1, left goes to 0\n",
    "        else: # s==2\n",
    "            P[(2, 0)] = dist(target=1)  # from 2, left goes to 1\n",
    "        \n",
    "        # Action 'right'\n",
    "        if s == 0:\n",
    "            P[(0, 1)] = dist(target=1)  # from 0, right -> 1\n",
    "        elif s == 1:\n",
    "            P[(1, 1)] = dist(target=2)  # from 1, right -> 2\n",
    "        else: # s==2\n",
    "            P[(2, 1)] = dist(target=0)  # from 2, right -> 0\n",
    "        \n",
    "        # Action 'stay'\n",
    "        P[(s, 2)] = dist(target=s)     # from s, stay in s\n",
    "    return P\n",
    "\n",
    "# Construct the transitions\n",
    "P = build_transition_matrix(eps)\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Features & True Rewards\n",
    "# ---------------------------\n",
    "# We'll use one-hot features for each state: phi(s) = e_s\n",
    "features = np.eye(n_states)\n",
    "\n",
    "# Set a \"true\" reward for demonstration. \n",
    "# The user can choose any arbitrary vector.\n",
    "true_rewards = np.array([0.5, 0.25, 0.10])\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Soft Value Iteration\n",
    "# ---------------------------\n",
    "def soft_value_iteration(reward, P, tol=1e-6, max_iter=200):\n",
    "    \"\"\"\n",
    "    V(s) <- log sum_a exp( R(s) + gamma * sum_{s'} P(s'|s,a)*V(s') )\n",
    "    \"\"\"\n",
    "    V = np.zeros(n_states)\n",
    "    for _ in range(max_iter):\n",
    "        V_prev = V.copy()\n",
    "        for s in range(n_states):\n",
    "            Q_sa = []\n",
    "            for a in range(n_actions):\n",
    "                Q_sa.append(reward[s] + gamma * np.dot(P[(s,a)], V_prev))\n",
    "            # log-sum-exp\n",
    "            V[s] = np.log(np.sum(np.exp(Q_sa)))\n",
    "        if np.max(np.abs(V - V_prev)) < tol:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Compute Policy\n",
    "# ---------------------------\n",
    "def compute_policy(V, reward, P):\n",
    "    \"\"\"\n",
    "    pi(a|s) = exp(Q(s,a)) / sum_{a'} exp(Q(s,a'))\n",
    "    Q(s,a) = R(s) + gamma * sum_{s'} P(s'|s,a]*V(s')\n",
    "    \"\"\"\n",
    "    policy = np.zeros((n_states, n_actions))\n",
    "    Q_values = np.zeros((n_states, n_actions))  # for debug printing\n",
    "    for s in range(n_states):\n",
    "        Q_sa = []\n",
    "        for a in range(n_actions):\n",
    "            q = reward[s] + gamma * np.dot(P[(s,a)], V)\n",
    "            Q_sa.append(q)\n",
    "        Q_sa = np.array(Q_sa)\n",
    "        # For debugging\n",
    "        Q_values[s] = Q_sa\n",
    "        # Stable softmax\n",
    "        shift = Q_sa - np.max(Q_sa)\n",
    "        policy[s] = np.exp(shift) / np.sum(np.exp(shift))\n",
    "    return policy, Q_values\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Compute State Visitation Frequencies\n",
    "# ---------------------------\n",
    "def compute_svf(policy, P, start_state=0, trajectory_length=5):\n",
    "    \"\"\"\n",
    "    Accumulate visitation frequencies over 'trajectory_length' steps,\n",
    "    starting with a single initial state (prob=1).\n",
    "    \"\"\"\n",
    "    d_t = np.zeros(n_states)\n",
    "    d_t[start_state] = 1.0\n",
    "\n",
    "    svf = np.zeros(n_states)\n",
    "    for _ in range(trajectory_length):\n",
    "        svf += d_t\n",
    "        next_d_t = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                next_d_t += d_t[s] * policy[s, a] * P[(s,a)]\n",
    "        d_t = next_d_t\n",
    "    return svf\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Generate \"Expert\" Trajectories\n",
    "# ---------------------------\n",
    "def generate_soft_optimal_trajectories(policy, P, n_trajectories=100, trajectory_length=5):\n",
    "    \"\"\"\n",
    "    Sample states by following 'policy'. \n",
    "    Each trajectory has length 'trajectory_length'.\n",
    "    \"\"\"\n",
    "    trajectories = []\n",
    "    for _ in range(n_trajectories):\n",
    "        traj = []\n",
    "        # Start from a random state\n",
    "        state = np.random.choice(n_states)\n",
    "        for _ in range(trajectory_length):\n",
    "            action = np.random.choice(n_actions, p=policy[state])\n",
    "            next_state = np.random.choice(n_states, p=P[(state, action)])\n",
    "            traj.append(state)\n",
    "            state = next_state\n",
    "        trajectories.append(traj)\n",
    "    return trajectories\n",
    "\n",
    "# ---------------------------\n",
    "# 7) MaxEnt IRL\n",
    "# ---------------------------\n",
    "def maxent_irl(features, expert_trajectories,\n",
    "               P,  # transition model\n",
    "               true_rewards=None, true_policy=None, true_value=None,\n",
    "               gamma=0.9, lr=0.01, n_iters=10000, print_every=1000):\n",
    "    \"\"\"\n",
    "    Gradient-based MaxEnt IRL: R(s) = theta^T phi(s).\n",
    "    Additional debugging prints added.\n",
    "    \"\"\"\n",
    "    n_states, d_features = features.shape\n",
    "    # Initialize random weights\n",
    "    reward_weights = np.random.uniform(size=d_features)\n",
    "\n",
    "    # Compute expert state visitation frequency (normalized)\n",
    "    expert_svf = np.zeros(n_states)\n",
    "    total_steps = 0\n",
    "    for traj in expert_trajectories:\n",
    "        for s in traj:\n",
    "            expert_svf[s] += 1\n",
    "        total_steps += len(traj)\n",
    "    expert_svf /= total_steps\n",
    "\n",
    "    for it in range(n_iters):\n",
    "        # 1) Current reward\n",
    "        reward_est = features @ reward_weights\n",
    "        \n",
    "        # 2) Soft Value Iteration\n",
    "        V_est = soft_value_iteration(reward_est, P)\n",
    "        \n",
    "        # 3) Compute policy & Q-values\n",
    "        policy_est, Q_values = compute_policy(V_est, reward_est, P)\n",
    "        \n",
    "        # 4) Predicted SVF\n",
    "        svf_est = compute_svf(policy_est, P, start_state=0, trajectory_length=5)\n",
    "        svf_est /= np.sum(svf_est)\n",
    "\n",
    "        # 5) Gradient step\n",
    "        grad = expert_svf - svf_est\n",
    "        reward_weights += lr * features.T @ grad\n",
    "\n",
    "        # 6) Debug prints\n",
    "        if (it+1) % print_every == 0:\n",
    "            loss_svf = np.linalg.norm(expert_svf - svf_est)  # L2\n",
    "            grad_norm = np.linalg.norm(grad)\n",
    "            msg = f\"Iter {it+1:05d} | Loss(SVF): {loss_svf:.4f} | GradNorm: {grad_norm:.4f}\"\n",
    "            \n",
    "            if true_policy is not None:\n",
    "                # L1 difference across all states, all actions\n",
    "                pol_diff = np.sum(np.abs(policy_est - true_policy))\n",
    "                msg += f\" | PolDiff: {pol_diff:.4f}\"\n",
    "            if true_value is not None:\n",
    "                # L1 difference in value\n",
    "                val_diff = np.sum(np.abs(V_est - true_value))\n",
    "                msg += f\" | ValDiff: {val_diff:.4f}\"\n",
    "            if true_rewards is not None:\n",
    "                # L1 difference in reward vector\n",
    "                rew_diff = np.sum(np.abs(reward_est - true_rewards))\n",
    "                msg += f\" | RewDiff: {rew_diff:.4f}\"\n",
    "\n",
    "            print(msg)\n",
    "            # Print SVF side by side\n",
    "            print(f\"  Expert SVF: {expert_svf}\")\n",
    "            print(f\"  Pred   SVF: {svf_est}\")\n",
    "            # Print Q-values\n",
    "            print(\"  Q-values (s x a):\")\n",
    "            for s in range(n_states):\n",
    "                print(f\"    s={s}, Q={Q_values[s]}\")\n",
    "            # Print policy\n",
    "            print(\"  Policy (s x a):\")\n",
    "            for s in range(n_states):\n",
    "                print(f\"    s={s}, π={policy_est[s]}\")\n",
    "            # Print reward\n",
    "            print(f\"  Reward: {reward_est}\")\n",
    "            print(\"\")\n",
    "\n",
    "    return features @ reward_weights\n",
    "\n",
    "# ---------------------------\n",
    "# 8) Main Demo\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(0)  # for reproducibility, if desired\n",
    "\n",
    "    # 1) Compute the \"true\" V, policy from the known reward\n",
    "    V_true = soft_value_iteration(true_rewards, P)\n",
    "    policy_true, _ = compute_policy(V_true, true_rewards, P)\n",
    "\n",
    "    # 2) Generate \"expert\" data from the \"true\" policy\n",
    "    n_sample_trajectories = 500\n",
    "    expert_trajectories = generate_soft_optimal_trajectories(\n",
    "        policy_true, P, n_trajectories=n_sample_trajectories, trajectory_length=100\n",
    "    )\n",
    "\n",
    "    # 3) Run MaxEnt IRL with extra prints\n",
    "    estimated_rewards = maxent_irl(\n",
    "        features,\n",
    "        expert_trajectories,\n",
    "        P,\n",
    "        true_rewards=true_rewards,\n",
    "        true_policy=policy_true,\n",
    "        true_value=V_true,\n",
    "        lr=0.01,\n",
    "        n_iters=10000,\n",
    "        print_every=1000\n",
    "    )\n",
    "\n",
    "    # Final results\n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(\"True Rewards:      \", true_rewards)\n",
    "    print(\"Estimated Rewards: \", estimated_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Base Experiment Info ===\n",
      "States:        3\n",
      "Actions:       3\n",
      "eps (noise):   0.05\n",
      "gamma:         0.9\n",
      "True Rewards:  [0.   0.25 0.9 ]\n",
      "NumTraj:       500\n",
      "TrajLen:       50\n",
      "============================\n",
      "\n",
      "\n",
      "=== Final Results Table (multiple reg_lambdas & anchor_modes) ===\n",
      "AnchorMode | RegLambda  | R(0)    R(1)    R(2)    | RewDiff\n",
      "--------------------------------------------------------------\n",
      "FixNone      | 0.00      | -1.7464  0.5624  1.1849 | 2.3437\n",
      "FixNone      | 0.05      | -1.1259  0.2844  0.8415 | 1.2188\n",
      "FixNone      | 0.10      | -0.8586  0.1801  0.6785 | 1.1500\n",
      "FixNone      | 0.20      | -0.6029  0.0991  0.5038 | 1.1500\n",
      "FixNone      | 0.70      | -0.2574  0.0291  0.2284 | 1.1500\n",
      "FixFirst     | 0.00      | 0.0000  1.3468  2.0132 | 2.2099\n",
      "FixFirst     | 0.05      | 0.0000  0.6975  1.2958 | 0.8433\n",
      "FixFirst     | 0.10      | 0.0000  0.4287  0.9609 | 0.2396\n",
      "FixFirst     | 0.20      | 0.0000  0.2189  0.6442 | 0.2870\n",
      "FixFirst     | 0.70      | 0.0000  0.0498  0.2519 | 0.8482\n",
      "FixFirstSecond | 0.00      | 0.0000  0.2500  1.4513 | 0.5513\n",
      "FixFirstSecond | 0.05      | 0.0000  0.2500  1.1116 | 0.2116\n",
      "FixFirstSecond | 0.10      | 0.0000  0.2500  0.9008 | 0.0008\n",
      "FixFirstSecond | 0.20      | 0.0000  0.2500  0.6519 | 0.2481\n",
      "FixFirstSecond | 0.70      | 0.0000  0.2500  0.2709 | 0.6291\n",
      "==============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1) Triangular MDP Setup\n",
    "# -------------------------------------------------\n",
    "eps = 0.05\n",
    "n_states = 3\n",
    "n_actions = 3\n",
    "gamma = 0.9\n",
    "\n",
    "def build_transition_matrix(eps=0.05):\n",
    "    \"\"\"\n",
    "    Triangular 3-state MDP with 3 actions.\n",
    "    P[(s, a)] = probability distribution over next states [p0, p1, p2].\n",
    "    \n",
    "    Action 0=left, 1=right, 2=stay\n",
    "    \"\"\"\n",
    "    P = {}\n",
    "    def dist(target):\n",
    "        d = np.ones(n_states) * (eps / n_states)\n",
    "        d[target] += (1 - eps)\n",
    "        return d\n",
    "    \n",
    "    for s in range(n_states):\n",
    "        if s == 0:\n",
    "            P[(0,0)] = dist(target=2)  # left\n",
    "            P[(0,1)] = dist(target=1)  # right\n",
    "            P[(0,2)] = dist(target=0)  # stay\n",
    "        elif s == 1:\n",
    "            P[(1,0)] = dist(target=0)\n",
    "            P[(1,1)] = dist(target=2)\n",
    "            P[(1,2)] = dist(target=1)\n",
    "        else:  # s=2\n",
    "            P[(2,0)] = dist(target=1)\n",
    "            P[(2,1)] = dist(target=0)\n",
    "            P[(2,2)] = dist(target=2)\n",
    "    return P\n",
    "\n",
    "P = build_transition_matrix(eps)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) Features & True Rewards\n",
    "# -------------------------------------------------\n",
    "features = np.eye(n_states)\n",
    "true_rewards = np.array([0.0, 0.25, 0.9])\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3) Soft Value Iteration\n",
    "# -------------------------------------------------\n",
    "def soft_value_iteration(reward, P, tol=1e-6, max_iter=200):\n",
    "    V = np.zeros(n_states)\n",
    "    for _ in range(max_iter):\n",
    "        V_prev = V.copy()\n",
    "        for s in range(n_states):\n",
    "            Q_sa = [reward[s] + gamma * np.dot(P[(s,a)], V_prev)\n",
    "                    for a in range(n_actions)]\n",
    "            # log-sum-exp\n",
    "            V[s] = np.log(np.sum(np.exp(Q_sa)))\n",
    "        if np.max(np.abs(V - V_prev)) < tol:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4) Compute Policy\n",
    "# -------------------------------------------------\n",
    "def compute_policy(V, reward, P):\n",
    "    policy = np.zeros((n_states, n_actions))\n",
    "    Q_values = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        Q_sa = np.array([reward[s] + gamma * np.dot(P[(s,a)], V) for a in range(n_actions)])\n",
    "        Q_values[s] = Q_sa\n",
    "        shift = Q_sa - np.max(Q_sa)  # stable softmax\n",
    "        policy[s] = np.exp(shift) / np.sum(np.exp(shift))\n",
    "    return policy, Q_values\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5) Compute State Visitation Frequencies\n",
    "# -------------------------------------------------\n",
    "def compute_svf(policy, P, start_state=0, trajectory_length=5):\n",
    "    d_t = np.zeros(n_states)\n",
    "    d_t[start_state] = 1.0\n",
    "    svf = np.zeros(n_states)\n",
    "    for _ in range(trajectory_length):\n",
    "        svf += d_t\n",
    "        next_d_t = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                next_d_t += d_t[s] * policy[s,a] * P[(s,a)]\n",
    "        d_t = next_d_t\n",
    "    return svf\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6) Generate \"Expert\" Trajectories\n",
    "# -------------------------------------------------\n",
    "def generate_soft_optimal_trajectories(policy, P, n_trajectories=100, trajectory_length=5):\n",
    "    trajectories = []\n",
    "    for _ in range(n_trajectories):\n",
    "        traj = []\n",
    "        state = np.random.choice(n_states)\n",
    "        for __ in range(trajectory_length):\n",
    "            action = np.random.choice(n_actions, p=policy[state])\n",
    "            next_state = np.random.choice(n_states, p=P[(state, action)])\n",
    "            traj.append(state)\n",
    "            state = next_state\n",
    "        trajectories.append(traj)\n",
    "    return trajectories\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 7) MaxEnt IRL with Anchors + L2 + Verbosity\n",
    "# -------------------------------------------------\n",
    "def maxent_irl(features,\n",
    "               expert_trajectories,\n",
    "               P,\n",
    "               anchor_mode=0,           # 0=fix none, 1=fix first, 2=fix first&second\n",
    "               anchor_values=None,      # e.g. [0.0], or [0.0, 0.25], or None\n",
    "               reg_lambda=0.0,          # L2 penalty\n",
    "               gamma=0.9,\n",
    "               lr=0.01,\n",
    "               n_iters=10000,\n",
    "               print_every=1000,\n",
    "               verbose=True,            # verbosity flag\n",
    "               true_rewards=None,\n",
    "               true_policy=None,\n",
    "               true_value=None):\n",
    "    \"\"\"\n",
    "    anchor_mode:\n",
    "      0 -> no anchoring\n",
    "      1 -> fix R(0)=anchor_values[0]\n",
    "      2 -> fix R(0)=anchor_values[0], R(1)=anchor_values[1]\n",
    "\n",
    "    reg_lambda: L2 penalty coefficient\n",
    "    verbose: if False, no intermediate prints\n",
    "    \"\"\"\n",
    "    n_states, d_features = features.shape\n",
    "    reward_weights = np.random.uniform(low=-1e-3, high=1e-3, size=d_features)\n",
    "\n",
    "    # Initialize anchored states if needed\n",
    "    if anchor_mode >= 1:\n",
    "        reward_weights[0] = anchor_values[0]\n",
    "    if anchor_mode == 2:\n",
    "        reward_weights[1] = anchor_values[1]\n",
    "\n",
    "    # Compute expert SVF\n",
    "    expert_svf = np.zeros(n_states)\n",
    "    total_steps = 0\n",
    "    for traj in expert_trajectories:\n",
    "        for s in traj:\n",
    "            expert_svf[s] += 1\n",
    "        total_steps += len(traj)\n",
    "    expert_svf /= total_steps\n",
    "\n",
    "    for it in range(n_iters):\n",
    "        reward_est = features @ reward_weights\n",
    "        V_est = soft_value_iteration(reward_est, P)\n",
    "        policy_est, Q_values = compute_policy(V_est, reward_est, P)\n",
    "\n",
    "        svf_est = compute_svf(policy_est, P, start_state=0, trajectory_length=5)\n",
    "        svf_est /= np.sum(svf_est)\n",
    "\n",
    "        # IRL gradient\n",
    "        grad_main = expert_svf - svf_est\n",
    "        grad_for_weights = features.T @ grad_main\n",
    "\n",
    "        # L2 penalty gradient\n",
    "        if reg_lambda > 0:\n",
    "            grad_for_weights -= reg_lambda * reward_weights\n",
    "\n",
    "        # Apply gradient\n",
    "        reward_weights += lr * grad_for_weights\n",
    "\n",
    "        # Re-pin anchored states\n",
    "        if anchor_mode >= 1:\n",
    "            reward_weights[0] = anchor_values[0]\n",
    "        if anchor_mode == 2:\n",
    "            reward_weights[1] = anchor_values[1]\n",
    "\n",
    "        # Possibly print\n",
    "        if verbose and (it+1) % print_every == 0:\n",
    "            loss_svf = np.linalg.norm(grad_main)\n",
    "            msg = f\"Iter {it+1:05d} | Loss(SVF): {loss_svf:.4f}\"\n",
    "            if true_rewards is not None:\n",
    "                rew_diff = np.sum(np.abs(reward_est - true_rewards))\n",
    "                msg += f\" | RewDiff: {rew_diff:.4f}\"\n",
    "            print(msg)\n",
    "\n",
    "    return features @ reward_weights\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 8) Main Demo\n",
    "# -------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Print Base Experiment Info\n",
    "    # ------------------------------\n",
    "    print(\"=== Base Experiment Info ===\")\n",
    "    print(f\"States:        {n_states}\")\n",
    "    print(f\"Actions:       {n_actions}\")\n",
    "    print(f\"eps (noise):   {eps}\")\n",
    "    print(f\"gamma:         {gamma}\")\n",
    "    print(f\"True Rewards:  {true_rewards}\")\n",
    "    n_sample_trajectories = 500\n",
    "    trajectory_length = 50\n",
    "    print(f\"NumTraj:       {n_sample_trajectories}\")\n",
    "    print(f\"TrajLen:       {trajectory_length}\")\n",
    "    print(\"============================\\n\")\n",
    "\n",
    "    # Build transitions, compute \"true\" policy\n",
    "    V_true = soft_value_iteration(true_rewards, P)\n",
    "    policy_true, _ = compute_policy(V_true, true_rewards, P)\n",
    "\n",
    "    # Generate expert data\n",
    "    expert_trajectories = generate_soft_optimal_trajectories(\n",
    "        policy_true, P,\n",
    "        n_trajectories=n_sample_trajectories,\n",
    "        trajectory_length=trajectory_length\n",
    "    )\n",
    "\n",
    "    # We'll test anchor_mode in [0,1,2] and reg_lambda in [0,0.05,0.1,0.2].\n",
    "    # Store final estimates in a dict: results[(anchor_mode, reg_lambda)].\n",
    "    anchor_modes = [0, 1, 2]\n",
    "    reg_lambdas = [0.0, 0.05, 0.1, 0.2, 0.7]\n",
    "    results = {}\n",
    "\n",
    "    for am in anchor_modes:\n",
    "        # If am=1 => fix R(0)=0.0\n",
    "        # If am=2 => fix R(0)=0.0, R(1)=0.25\n",
    "        # If am=0 => no anchor => anchor_values=None\n",
    "        if am == 0:\n",
    "            anc_vals = None\n",
    "        elif am == 1:\n",
    "            anc_vals = [0.0]\n",
    "        else:  # am=2\n",
    "            anc_vals = [0.0, 0.25]\n",
    "\n",
    "        for rl in reg_lambdas:\n",
    "            est_rew = maxent_irl(\n",
    "                features,\n",
    "                expert_trajectories,\n",
    "                P,\n",
    "                anchor_mode=am,\n",
    "                anchor_values=anc_vals,\n",
    "                reg_lambda=rl,\n",
    "                gamma=gamma,\n",
    "                lr=0.01,\n",
    "                n_iters=3000,      # short run for demo\n",
    "                print_every=1000,  # could set verbose=False to hide logs\n",
    "                verbose=False,\n",
    "                true_rewards=true_rewards,\n",
    "                true_policy=policy_true,\n",
    "                true_value=V_true\n",
    "            )\n",
    "            results[(am, rl)] = est_rew\n",
    "\n",
    "    # Now print a final table of results.\n",
    "    # We'll have one row for each (anchor_mode, reg_lambda).\n",
    "    print(\"\\n=== Final Results Table (multiple reg_lambdas & anchor_modes) ===\")\n",
    "    print(\"AnchorMode | RegLambda  | R(0)    R(1)    R(2)    | RewDiff\")\n",
    "    print(\"--------------------------------------------------------------\")\n",
    "\n",
    "    for am in anchor_modes:\n",
    "        for rl in reg_lambdas:\n",
    "            est_rew = results[(am, rl)]\n",
    "            rew_diff = np.sum(np.abs(est_rew - true_rewards))\n",
    "            # format anchor mode\n",
    "            if am == 0:\n",
    "                am_str = \"FixNone\"\n",
    "            elif am == 1:\n",
    "                am_str = \"FixFirst\"\n",
    "            else:\n",
    "                am_str = \"FixFirstSecond\"\n",
    "            print(f\"{am_str:<12} | {rl:<9.2f} | \"\n",
    "                  f\"{est_rew[0]:.4f}  {est_rew[1]:.4f}  {est_rew[2]:.4f} | \"\n",
    "                  f\"{rew_diff:.4f}\")\n",
    "    print(\"==============================================================\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
