{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 01000 | Loss(SVF): 0.0076 | GradNorm: 0.0076 | PolDiff: 0.9582 | ValDiff: 11.8977 | RewDiff: 1.5524\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3938 0.3171 0.2891]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5041 16.6004 16.0799]\n",
      "    s=1, Q=[16.6887 17.1129 17.2093]\n",
      "    s=2, Q=[17.0966 16.576  17.0002]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.3629 0.3996 0.2375]\n",
      "    s=1, π=[0.2375 0.3629 0.3996]\n",
      "    s=2, π=[0.3996 0.2375 0.3629]\n",
      "  Reward: [0.2974 0.9063 0.7935]\n",
      "\n",
      "Iter 02000 | Loss(SVF): 0.0008 | GradNorm: 0.0008 | PolDiff: 0.9798 | ValDiff: 11.9721 | RewDiff: 1.5823\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3911 0.3227 0.2863]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5035 16.6286 16.0745]\n",
      "    s=1, Q=[16.7225 17.1515 17.2766]\n",
      "    s=2, Q=[17.1303 16.5762 17.0052]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.3591 0.407  0.2339]\n",
      "    s=1, π=[0.2339 0.3591 0.407 ]\n",
      "    s=2, π=[0.407  0.2339 0.3591]\n",
      "  Reward: [0.2825 0.9305 0.7842]\n",
      "\n",
      "Iter 03000 | Loss(SVF): 0.0001 | GradNorm: 0.0001 | PolDiff: 0.9832 | ValDiff: 11.9831 | RewDiff: 1.5872\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3906 0.3232 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5043 16.6316 16.0732]\n",
      "    s=1, Q=[16.7263 17.1573 17.2846]\n",
      "    s=2, Q=[17.1357 16.5774 17.0084]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4077 0.2333]\n",
      "    s=1, π=[0.2333 0.359  0.4077]\n",
      "    s=2, π=[0.4077 0.2333 0.359 ]\n",
      "  Reward: [0.28   0.9331 0.7841]\n",
      "\n",
      "Iter 04000 | Loss(SVF): 0.0000 | GradNorm: 0.0000 | PolDiff: 0.9838 | ValDiff: 11.9849 | RewDiff: 1.5880\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3906 0.3233 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5045 16.6319 16.073 ]\n",
      "    s=1, Q=[16.7267 17.1582 17.2857]\n",
      "    s=2, Q=[17.1366 16.5776 17.0091]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4078 0.2332]\n",
      "    s=1, π=[0.2332 0.359  0.4078]\n",
      "    s=2, π=[0.4078 0.2332 0.359 ]\n",
      "  Reward: [0.2796 0.9334 0.7843]\n",
      "\n",
      "Iter 05000 | Loss(SVF): 0.0000 | GradNorm: 0.0000 | PolDiff: 0.9839 | ValDiff: 11.9851 | RewDiff: 1.5882\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3905 0.3233 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5045 16.632  16.0729]\n",
      "    s=1, Q=[16.7268 17.1584 17.2859]\n",
      "    s=2, Q=[17.1367 16.5777 17.0093]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4078 0.2332]\n",
      "    s=1, π=[0.2332 0.359  0.4078]\n",
      "    s=2, π=[0.4078 0.2332 0.359 ]\n",
      "  Reward: [0.2795 0.9334 0.7843]\n",
      "\n",
      "Iter 06000 | Loss(SVF): 0.0000 | GradNorm: 0.0000 | PolDiff: 0.9839 | ValDiff: 11.9852 | RewDiff: 1.5882\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3905 0.3233 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5045 16.632  16.0729]\n",
      "    s=1, Q=[16.7268 17.1584 17.2859]\n",
      "    s=2, Q=[17.1368 16.5777 17.0093]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4078 0.2332]\n",
      "    s=1, π=[0.2332 0.359  0.4078]\n",
      "    s=2, π=[0.4078 0.2332 0.359 ]\n",
      "  Reward: [0.2795 0.9334 0.7843]\n",
      "\n",
      "Iter 07000 | Loss(SVF): 0.0000 | GradNorm: 0.0000 | PolDiff: 0.9839 | ValDiff: 11.9852 | RewDiff: 1.5882\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3905 0.3233 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5045 16.632  16.0729]\n",
      "    s=1, Q=[16.7268 17.1584 17.2859]\n",
      "    s=2, Q=[17.1368 16.5777 17.0093]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4078 0.2332]\n",
      "    s=1, π=[0.2332 0.359  0.4078]\n",
      "    s=2, π=[0.4078 0.2332 0.359 ]\n",
      "  Reward: [0.2795 0.9334 0.7843]\n",
      "\n",
      "Iter 08000 | Loss(SVF): 0.0000 | GradNorm: 0.0000 | PolDiff: 0.9839 | ValDiff: 11.9852 | RewDiff: 1.5882\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3905 0.3233 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5045 16.632  16.0729]\n",
      "    s=1, Q=[16.7268 17.1584 17.2859]\n",
      "    s=2, Q=[17.1368 16.5777 17.0093]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4078 0.2332]\n",
      "    s=1, π=[0.2332 0.359  0.4078]\n",
      "    s=2, π=[0.4078 0.2332 0.359 ]\n",
      "  Reward: [0.2795 0.9334 0.7843]\n",
      "\n",
      "Iter 09000 | Loss(SVF): 0.0000 | GradNorm: 0.0000 | PolDiff: 0.9839 | ValDiff: 11.9852 | RewDiff: 1.5882\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3905 0.3233 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5045 16.632  16.0729]\n",
      "    s=1, Q=[16.7268 17.1584 17.2859]\n",
      "    s=2, Q=[17.1368 16.5777 17.0093]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4078 0.2332]\n",
      "    s=1, π=[0.2332 0.359  0.4078]\n",
      "    s=2, π=[0.4078 0.2332 0.359 ]\n",
      "  Reward: [0.2795 0.9334 0.7843]\n",
      "\n",
      "Iter 10000 | Loss(SVF): 0.0000 | GradNorm: 0.0000 | PolDiff: 0.9839 | ValDiff: 11.9852 | RewDiff: 1.5882\n",
      "  Expert SVF: [0.3905 0.3233 0.2862]\n",
      "  Pred   SVF: [0.3905 0.3233 0.2862]\n",
      "  Q-values (s x a):\n",
      "    s=0, Q=[16.5045 16.632  16.0729]\n",
      "    s=1, Q=[16.7268 17.1584 17.2859]\n",
      "    s=2, Q=[17.1368 16.5777 17.0093]\n",
      "  Policy (s x a):\n",
      "    s=0, π=[0.359  0.4078 0.2332]\n",
      "    s=1, π=[0.2332 0.359  0.4078]\n",
      "    s=2, π=[0.4078 0.2332 0.359 ]\n",
      "  Reward: [0.2795 0.9334 0.7843]\n",
      "\n",
      "\n",
      "Final Results:\n",
      "True Rewards:       [0.5  0.25 0.1 ]\n",
      "Estimated Rewards:  [0.2795 0.9334 0.7843]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Triangular MDP Setup\n",
    "# ---------------------------\n",
    "# States: 0,1,2 (think of them arranged in a triangle)\n",
    "# Actions: 0=left, 1=right, 2=stay\n",
    "# We'll define transitions with a small noise probability eps.\n",
    "eps = 0.05\n",
    "n_states = 3\n",
    "n_actions = 3\n",
    "gamma = 0.9\n",
    "\n",
    "def build_transition_matrix(eps=0.05):\n",
    "    \"\"\"\n",
    "    Triangular 3-state MDP with 3 actions.\n",
    "    P[(s, a)] = probability distribution over next states [p0, p1, p2].\n",
    "    \n",
    "    Action 0 = 'left' \n",
    "    Action 1 = 'right'\n",
    "    Action 2 = 'stay'\n",
    "    \"\"\"\n",
    "    # Initialize dictionary\n",
    "    P = {}\n",
    "    # Helper to set distribution with main target and small noise to others\n",
    "    def dist(target):\n",
    "        # With prob (1 - eps) go to the target, with prob eps the rest is evenly distributed\n",
    "        d = np.ones(n_states) * (eps / (n_states))\n",
    "        d[target] += (1 - eps)\n",
    "        return d\n",
    "    \n",
    "    # For each state s, define transitions for each action\n",
    "    for s in range(n_states):\n",
    "        # Action 'left'\n",
    "        if s == 0:\n",
    "            P[(0, 0)] = dist(target=2)  # from 0, left goes to 2 ideally\n",
    "        elif s == 1:\n",
    "            P[(1, 0)] = dist(target=0)  # from 1, left goes to 0\n",
    "        else: # s==2\n",
    "            P[(2, 0)] = dist(target=1)  # from 2, left goes to 1\n",
    "        \n",
    "        # Action 'right'\n",
    "        if s == 0:\n",
    "            P[(0, 1)] = dist(target=1)  # from 0, right -> 1\n",
    "        elif s == 1:\n",
    "            P[(1, 1)] = dist(target=2)  # from 1, right -> 2\n",
    "        else: # s==2\n",
    "            P[(2, 1)] = dist(target=0)  # from 2, right -> 0\n",
    "        \n",
    "        # Action 'stay'\n",
    "        P[(s, 2)] = dist(target=s)     # from s, stay in s\n",
    "    return P\n",
    "\n",
    "# Construct the transitions\n",
    "P = build_transition_matrix(eps)\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Features & True Rewards\n",
    "# ---------------------------\n",
    "# We'll use one-hot features for each state: phi(s) = e_s\n",
    "features = np.eye(n_states)\n",
    "\n",
    "# Set a \"true\" reward for demonstration. \n",
    "# The user can choose any arbitrary vector.\n",
    "true_rewards = np.array([0.5, 0.25, 0.10])\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Soft Value Iteration\n",
    "# ---------------------------\n",
    "def soft_value_iteration(reward, P, tol=1e-6, max_iter=200):\n",
    "    \"\"\"\n",
    "    V(s) <- log sum_a exp( R(s) + gamma * sum_{s'} P(s'|s,a)*V(s') )\n",
    "    \"\"\"\n",
    "    V = np.zeros(n_states)\n",
    "    for _ in range(max_iter):\n",
    "        V_prev = V.copy()\n",
    "        for s in range(n_states):\n",
    "            Q_sa = []\n",
    "            for a in range(n_actions):\n",
    "                Q_sa.append(reward[s] + gamma * np.dot(P[(s,a)], V_prev))\n",
    "            # log-sum-exp\n",
    "            V[s] = np.log(np.sum(np.exp(Q_sa)))\n",
    "        if np.max(np.abs(V - V_prev)) < tol:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Compute Policy\n",
    "# ---------------------------\n",
    "def compute_policy(V, reward, P):\n",
    "    \"\"\"\n",
    "    pi(a|s) = exp(Q(s,a)) / sum_{a'} exp(Q(s,a'))\n",
    "    Q(s,a) = R(s) + gamma * sum_{s'} P(s'|s,a]*V(s')\n",
    "    \"\"\"\n",
    "    policy = np.zeros((n_states, n_actions))\n",
    "    Q_values = np.zeros((n_states, n_actions))  # for debug printing\n",
    "    for s in range(n_states):\n",
    "        Q_sa = []\n",
    "        for a in range(n_actions):\n",
    "            q = reward[s] + gamma * np.dot(P[(s,a)], V)\n",
    "            Q_sa.append(q)\n",
    "        Q_sa = np.array(Q_sa)\n",
    "        # For debugging\n",
    "        Q_values[s] = Q_sa\n",
    "        # Stable softmax\n",
    "        shift = Q_sa - np.max(Q_sa)\n",
    "        policy[s] = np.exp(shift) / np.sum(np.exp(shift))\n",
    "    return policy, Q_values\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Compute State Visitation Frequencies\n",
    "# ---------------------------\n",
    "def compute_svf(policy, P, start_state=0, trajectory_length=5):\n",
    "    \"\"\"\n",
    "    Accumulate visitation frequencies over 'trajectory_length' steps,\n",
    "    starting with a single initial state (prob=1).\n",
    "    \"\"\"\n",
    "    d_t = np.zeros(n_states)\n",
    "    d_t[start_state] = 1.0\n",
    "\n",
    "    svf = np.zeros(n_states)\n",
    "    for _ in range(trajectory_length):\n",
    "        svf += d_t\n",
    "        next_d_t = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                next_d_t += d_t[s] * policy[s, a] * P[(s,a)]\n",
    "        d_t = next_d_t\n",
    "    return svf\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Generate \"Expert\" Trajectories\n",
    "# ---------------------------\n",
    "def generate_soft_optimal_trajectories(policy, P, n_trajectories=100, trajectory_length=5):\n",
    "    \"\"\"\n",
    "    Sample states by following 'policy'. \n",
    "    Each trajectory has length 'trajectory_length'.\n",
    "    \"\"\"\n",
    "    trajectories = []\n",
    "    for _ in range(n_trajectories):\n",
    "        traj = []\n",
    "        # Start from a random state\n",
    "        state = np.random.choice(n_states)\n",
    "        for _ in range(trajectory_length):\n",
    "            action = np.random.choice(n_actions, p=policy[state])\n",
    "            next_state = np.random.choice(n_states, p=P[(state, action)])\n",
    "            traj.append(state)\n",
    "            state = next_state\n",
    "        trajectories.append(traj)\n",
    "    return trajectories\n",
    "\n",
    "# ---------------------------\n",
    "# 7) MaxEnt IRL\n",
    "# ---------------------------\n",
    "def maxent_irl(features, expert_trajectories,\n",
    "               P,  # transition model\n",
    "               true_rewards=None, true_policy=None, true_value=None,\n",
    "               gamma=0.9, lr=0.01, n_iters=10000, print_every=1000):\n",
    "    \"\"\"\n",
    "    Gradient-based MaxEnt IRL: R(s) = theta^T phi(s).\n",
    "    Additional debugging prints added.\n",
    "    \"\"\"\n",
    "    n_states, d_features = features.shape\n",
    "    # Initialize random weights\n",
    "    reward_weights = np.random.uniform(size=d_features)\n",
    "\n",
    "    # Compute expert state visitation frequency (normalized)\n",
    "    expert_svf = np.zeros(n_states)\n",
    "    total_steps = 0\n",
    "    for traj in expert_trajectories:\n",
    "        for s in traj:\n",
    "            expert_svf[s] += 1\n",
    "        total_steps += len(traj)\n",
    "    expert_svf /= total_steps\n",
    "\n",
    "    for it in range(n_iters):\n",
    "        # 1) Current reward\n",
    "        reward_est = features @ reward_weights\n",
    "        \n",
    "        # 2) Soft Value Iteration\n",
    "        V_est = soft_value_iteration(reward_est, P)\n",
    "        \n",
    "        # 3) Compute policy & Q-values\n",
    "        policy_est, Q_values = compute_policy(V_est, reward_est, P)\n",
    "        \n",
    "        # 4) Predicted SVF\n",
    "        svf_est = compute_svf(policy_est, P, start_state=0, trajectory_length=5)\n",
    "        svf_est /= np.sum(svf_est)\n",
    "\n",
    "        # 5) Gradient step\n",
    "        grad = expert_svf - svf_est\n",
    "        reward_weights += lr * features.T @ grad\n",
    "\n",
    "        # 6) Debug prints\n",
    "        if (it+1) % print_every == 0:\n",
    "            loss_svf = np.linalg.norm(expert_svf - svf_est)  # L2\n",
    "            grad_norm = np.linalg.norm(grad)\n",
    "            msg = f\"Iter {it+1:05d} | Loss(SVF): {loss_svf:.4f} | GradNorm: {grad_norm:.4f}\"\n",
    "            \n",
    "            if true_policy is not None:\n",
    "                # L1 difference across all states, all actions\n",
    "                pol_diff = np.sum(np.abs(policy_est - true_policy))\n",
    "                msg += f\" | PolDiff: {pol_diff:.4f}\"\n",
    "            if true_value is not None:\n",
    "                # L1 difference in value\n",
    "                val_diff = np.sum(np.abs(V_est - true_value))\n",
    "                msg += f\" | ValDiff: {val_diff:.4f}\"\n",
    "            if true_rewards is not None:\n",
    "                # L1 difference in reward vector\n",
    "                rew_diff = np.sum(np.abs(reward_est - true_rewards))\n",
    "                msg += f\" | RewDiff: {rew_diff:.4f}\"\n",
    "\n",
    "            print(msg)\n",
    "            # Print SVF side by side\n",
    "            print(f\"  Expert SVF: {expert_svf}\")\n",
    "            print(f\"  Pred   SVF: {svf_est}\")\n",
    "            # Print Q-values\n",
    "            print(\"  Q-values (s x a):\")\n",
    "            for s in range(n_states):\n",
    "                print(f\"    s={s}, Q={Q_values[s]}\")\n",
    "            # Print policy\n",
    "            print(\"  Policy (s x a):\")\n",
    "            for s in range(n_states):\n",
    "                print(f\"    s={s}, π={policy_est[s]}\")\n",
    "            # Print reward\n",
    "            print(f\"  Reward: {reward_est}\")\n",
    "            print(\"\")\n",
    "\n",
    "    return features @ reward_weights\n",
    "\n",
    "# ---------------------------\n",
    "# 8) Main Demo\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(0)  # for reproducibility, if desired\n",
    "\n",
    "    # 1) Compute the \"true\" V, policy from the known reward\n",
    "    V_true = soft_value_iteration(true_rewards, P)\n",
    "    policy_true, _ = compute_policy(V_true, true_rewards, P)\n",
    "\n",
    "    # 2) Generate \"expert\" data from the \"true\" policy\n",
    "    n_sample_trajectories = 500\n",
    "    expert_trajectories = generate_soft_optimal_trajectories(\n",
    "        policy_true, P, n_trajectories=n_sample_trajectories, trajectory_length=100\n",
    "    )\n",
    "\n",
    "    # 3) Run MaxEnt IRL with extra prints\n",
    "    estimated_rewards = maxent_irl(\n",
    "        features,\n",
    "        expert_trajectories,\n",
    "        P,\n",
    "        true_rewards=true_rewards,\n",
    "        true_policy=policy_true,\n",
    "        true_value=V_true,\n",
    "        lr=0.01,\n",
    "        n_iters=10000,\n",
    "        print_every=1000\n",
    "    )\n",
    "\n",
    "    # Final results\n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(\"True Rewards:      \", true_rewards)\n",
    "    print(\"Estimated Rewards: \", estimated_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Base Experiment Info ===\n",
      "States:        3\n",
      "Actions:       3\n",
      "eps (noise):   0.05\n",
      "gamma:         0.9\n",
      "True Rewards:  [0.   0.25 0.9 ]\n",
      "NumTraj:       500\n",
      "TrajLen:       50\n",
      "============================\n",
      "\n",
      "\n",
      "=== Final Results Table ===\n",
      "AnchorMode       | reg_lambda |   R(0)    R(1)    R(2)   | RewDiff | PolDiff | GradNorm | ValDiff | SvfDiff | LogLik\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "FixNone          | 0.00      | -2.1748  0.7848  1.3909 | 3.2005  | 1.1914  | 0.0021  | 5.0764  | 0.0035  | -37579.8135\n",
      "FixNone          | 0.05      | -1.1384  0.2913  0.8471 | 1.2327  | 0.7558  | 0.0724  | 6.6523  | 0.1138  | -31970.5889\n",
      "FixNone          | 0.10      | -0.8596  0.1807  0.6789 | 1.1500  | 0.5266  | 0.1110  | 9.0881  | 0.1719  | -31019.4487\n",
      "FixNone          | 0.20      | -0.6029  0.0991  0.5038 | 1.1500  | 0.3358  | 0.1584  | 10.9757  | 0.2412  | -30526.8096\n",
      "FixNone          | 0.70      | -0.2574  0.0291  0.2284 | 1.1500  | 0.5490  | 0.2417  | 12.7077  | 0.3604  | -30709.3660\n",
      "FixFirst         | 0.00      |  0.0000  2.3865  3.0045 | 4.2410  | 1.0886  | 0.0181  | 54.1033  | 0.0295  | -35285.5688\n",
      "FixFirst         | 0.05      |  0.0000  0.7542  1.3466 | 0.9508  | 0.3758  | 0.1304  | 11.0431  | 0.2101  | -30631.1218\n",
      "FixFirst         | 0.10      |  0.0000  0.4356  0.9669 | 0.2525  | 0.1484  | 0.1758  | 2.6010  | 0.2805  | -30344.2743\n",
      "FixFirst         | 0.20      |  0.0000  0.2190  0.6443 | 0.2867  | 0.3044  | 0.2199  | 3.7022  | 0.3453  | -30428.6472\n",
      "FixFirst         | 0.70      |  0.0000  0.0498  0.2519 | 0.8482  | 0.6794  | 0.2774  | 9.9866  | 0.4225  | -30993.1513\n",
      "FixFirstSecond   | 0.00      |  0.0000  0.2500  1.4644 | 0.5644  | 0.7130  | 0.1588  | 8.7931  | 0.2246  | -31077.6609\n",
      "FixFirstSecond   | 0.05      |  0.0000  0.2500  1.1135 | 0.2135  | 0.2734  | 0.1690  | 3.1230  | 0.2744  | -30433.4002\n",
      "FixFirstSecond   | 0.10      |  0.0000  0.2500  0.9012 | 0.0012  | 0.0015  | 0.1879  | 0.0165  | 0.3051  | -30313.7891\n",
      "FixFirstSecond   | 0.20      |  0.0000  0.2500  0.6519 | 0.2481  | 0.3163  | 0.2184  | 3.3161  | 0.3411  | -30435.6874\n",
      "FixFirstSecond   | 0.70      |  0.0000  0.2500  0.2709 | 0.6291  | 0.7840  | 0.2735  | 7.7663  | 0.3940  | -31157.9991\n",
      "==================================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1) Triangular MDP Setup\n",
    "# -------------------------------------------------\n",
    "eps = 0.05\n",
    "n_states = 3\n",
    "n_actions = 3\n",
    "gamma = 0.9\n",
    "\n",
    "def build_transition_matrix(eps=0.05):\n",
    "    \"\"\"\n",
    "    Triangular 3-state MDP with 3 actions.\n",
    "    P[(s, a)] = probability distribution over next states [p0, p1, p2].\n",
    "    \n",
    "    Action 0=left, 1=right, 2=stay\n",
    "    \"\"\"\n",
    "    P = {}\n",
    "    def dist(target):\n",
    "        d = np.ones(n_states) * (eps / n_states)\n",
    "        d[target] += (1 - eps)\n",
    "        return d\n",
    "    \n",
    "    for s in range(n_states):\n",
    "        if s == 0:\n",
    "            P[(0,0)] = dist(target=2)  # left\n",
    "            P[(0,1)] = dist(target=1)  # right\n",
    "            P[(0,2)] = dist(target=0)  # stay\n",
    "        elif s == 1:\n",
    "            P[(1,0)] = dist(target=0)\n",
    "            P[(1,1)] = dist(target=2)\n",
    "            P[(1,2)] = dist(target=1)\n",
    "        else:  # s=2\n",
    "            P[(2,0)] = dist(target=1)\n",
    "            P[(2,1)] = dist(target=0)\n",
    "            P[(2,2)] = dist(target=2)\n",
    "    return P\n",
    "\n",
    "P = build_transition_matrix(eps)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) Features & True Rewards\n",
    "# -------------------------------------------------\n",
    "features = np.eye(n_states)\n",
    "true_rewards = np.array([0.0, 0.25, 0.9])\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3) Soft Value Iteration\n",
    "# -------------------------------------------------\n",
    "def soft_value_iteration(reward, P, tol=1e-6, max_iter=200):\n",
    "    \"\"\"\n",
    "    Returns the value function V(s) under the \"soft\" (MaxEnt) Bellman update:\n",
    "    V(s) = log sum_a exp( R(s) + gamma * sum_{s'} P(s'|s,a)*V(s') ).\n",
    "    \"\"\"\n",
    "    V = np.zeros(n_states)\n",
    "    for _ in range(max_iter):\n",
    "        V_prev = V.copy()\n",
    "        for s in range(n_states):\n",
    "            Q_sa = [\n",
    "                reward[s] + gamma * np.dot(P[(s,a)], V_prev)\n",
    "                for a in range(n_actions)\n",
    "            ]\n",
    "            # log-sum-exp\n",
    "            V[s] = np.log(np.sum(np.exp(Q_sa)))\n",
    "        if np.max(np.abs(V - V_prev)) < tol:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4) Compute Policy\n",
    "# -------------------------------------------------\n",
    "def compute_policy(V, reward, P):\n",
    "    \"\"\"\n",
    "    Derives a softmax policy from value function V(s) and reward R(s).\n",
    "    Returns (policy, Q_values).\n",
    "    \"\"\"\n",
    "    policy = np.zeros((n_states, n_actions))\n",
    "    Q_values = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        Q_sa = np.array([\n",
    "            reward[s] + gamma * np.dot(P[(s,a)], V)\n",
    "            for a in range(n_actions)\n",
    "        ])\n",
    "        Q_values[s] = Q_sa\n",
    "        # stable softmax\n",
    "        shift = Q_sa - np.max(Q_sa)\n",
    "        policy[s] = np.exp(shift) / np.sum(np.exp(shift))\n",
    "    return policy, Q_values\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5) Compute State Visitation Frequencies\n",
    "# -------------------------------------------------\n",
    "def compute_svf(policy, P, start_state=0, trajectory_length=5):\n",
    "    \"\"\"\n",
    "    Returns the unnormalized visitation distribution from a single start state,\n",
    "    across 'trajectory_length' steps. We then sum over time and optionally normalize.\n",
    "    \"\"\"\n",
    "    d_t = np.zeros(n_states)\n",
    "    d_t[start_state] = 1.0\n",
    "    svf = np.zeros(n_states)\n",
    "    for _ in range(trajectory_length):\n",
    "        svf += d_t\n",
    "        next_d_t = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                next_d_t += d_t[s] * policy[s,a] * P[(s,a)]\n",
    "        d_t = next_d_t\n",
    "    return svf\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6) Generate \"Expert\" Trajectories\n",
    "# -------------------------------------------------\n",
    "def generate_soft_optimal_trajectories(policy, P, n_trajectories=100, trajectory_length=5):\n",
    "    \"\"\"\n",
    "    Samples states by following 'policy'.\n",
    "    \"\"\"\n",
    "    trajectories = []\n",
    "    for _ in range(n_trajectories):\n",
    "        traj = []\n",
    "        state = np.random.choice(n_states)\n",
    "        for __ in range(trajectory_length):\n",
    "            action = np.random.choice(n_actions, p=policy[state])\n",
    "            next_state = np.random.choice(n_states, p=P[(state, action)])\n",
    "            traj.append((state, action, next_state))  # store (s,a,s')\n",
    "            state = next_state\n",
    "        trajectories.append(traj)\n",
    "    return trajectories\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 7) Log-Likelihood of Expert Trajectories\n",
    "# -------------------------------------------------\n",
    "def compute_expert_log_likelihood(reward, expert_trajectories, P):\n",
    "    \"\"\"\n",
    "    Compute sum of log p_\\theta(\\tau_i) for the expert's trajectories,\n",
    "    using the learned policy derived from 'reward'.\n",
    "    p_\\theta(s0, a0, s1, a1, ...) = product_t [ pi(a_t | s_t) * P(s_{t+1} | s_t, a_t) ].\n",
    "    We'll ignore any initial state distribution factor for simplicity,\n",
    "    or we can assume it's uniform.\n",
    "    \"\"\"\n",
    "    # 1) Value iteration + policy\n",
    "    V_est = soft_value_iteration(reward, P)\n",
    "    policy_est, _ = compute_policy(V_est, reward, P)\n",
    "\n",
    "    # 2) For each trajectory, compute log-prob\n",
    "    total_loglik = 0.0\n",
    "    for traj in expert_trajectories:\n",
    "        # traj is a list of (s, a, s') steps\n",
    "        logp_tau = 0.0\n",
    "        for (s, a, s_next) in traj:\n",
    "            # log p_\\theta(s,a,s_next) = log pi(a|s) + log P(s_next|s,a)\n",
    "            # ignoring any discount for demonstration\n",
    "            # ignoring initial state distribution\n",
    "            if policy_est[s,a] > 0:\n",
    "                logp_tau += np.log(policy_est[s,a])\n",
    "            else:\n",
    "                logp_tau += -1e9  # or some large negative if policy_est[s,a] = 0\n",
    "            # log P(s_next|s,a)\n",
    "            if P[(s,a)][s_next] > 0:\n",
    "                logp_tau += np.log(P[(s,a)][s_next])\n",
    "            else:\n",
    "                logp_tau += -1e9\n",
    "        total_loglik += logp_tau\n",
    "    return total_loglik\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 8) MaxEnt IRL\n",
    "# -------------------------------------------------\n",
    "def maxent_irl(\n",
    "    features,\n",
    "    expert_trajectories,\n",
    "    P,\n",
    "    anchor_mode=0,           # 0=fix none, 1=fix first, 2=fix first&second\n",
    "    anchor_values=None,      # e.g. [0.0], or [0.0, 0.25], or None\n",
    "    reg_lambda=0.0,          # L2 penalty\n",
    "    gamma=0.9,\n",
    "    lr=0.01,\n",
    "    n_iters=10000,\n",
    "    print_every=1000,\n",
    "    verbose=True,\n",
    "    true_rewards=None,\n",
    "    true_policy=None,\n",
    "    true_value=None\n",
    "):\n",
    "    \"\"\"\n",
    "    anchor_mode:\n",
    "      0 -> no anchoring\n",
    "      1 -> fix R(0)=anchor_values[0]\n",
    "      2 -> fix R(0)=anchor_values[0], R(1)=anchor_values[1]\n",
    "\n",
    "    We do gradient-based MaxEnt IRL, with an L2 penalty.\n",
    "    \"\"\"\n",
    "    n_states, d_features = features.shape\n",
    "    reward_weights = np.random.uniform(low=-1e-3, high=1e-3, size=d_features)\n",
    "\n",
    "    # If anchoring, set those dimensions\n",
    "    if anchor_mode >= 1:\n",
    "        reward_weights[0] = anchor_values[0]\n",
    "    if anchor_mode == 2:\n",
    "        reward_weights[1] = anchor_values[1]\n",
    "\n",
    "    # Expert SVF\n",
    "    expert_svf = np.zeros(n_states)\n",
    "    total_steps = 0\n",
    "    for traj in expert_trajectories:\n",
    "        for (s,a,s_next) in traj:\n",
    "            expert_svf[s] += 1\n",
    "        total_steps += len(traj)\n",
    "    expert_svf /= total_steps\n",
    "\n",
    "    for it in range(n_iters):\n",
    "        reward_est = features @ reward_weights\n",
    "        V_est = soft_value_iteration(reward_est, P)\n",
    "        policy_est, _ = compute_policy(V_est, reward_est, P)\n",
    "\n",
    "        # Compute learned SVF\n",
    "        svf_est = compute_svf(policy_est, P, start_state=0, trajectory_length=5)\n",
    "        svf_est /= np.sum(svf_est)\n",
    "\n",
    "        # IRL gradient\n",
    "        grad_main = expert_svf - svf_est\n",
    "        grad_for_weights = features.T @ grad_main\n",
    "\n",
    "        # L2 penalty\n",
    "        if reg_lambda > 0:\n",
    "            grad_for_weights -= reg_lambda * reward_weights\n",
    "\n",
    "        # Update\n",
    "        reward_weights += lr * grad_for_weights\n",
    "\n",
    "        # Re-pin anchors if needed\n",
    "        if anchor_mode >= 1:\n",
    "            reward_weights[0] = anchor_values[0]\n",
    "        if anchor_mode == 2:\n",
    "            reward_weights[1] = anchor_values[1]\n",
    "\n",
    "        # Optional prints\n",
    "        if verbose and (it+1) % print_every == 0:\n",
    "            loss_svf = np.linalg.norm(grad_main)\n",
    "            msg = f\"Iter {it+1:05d} | Loss(SVF): {loss_svf:.4f}\"\n",
    "            if true_rewards is not None:\n",
    "                rew_diff = np.sum(np.abs(reward_est - true_rewards))\n",
    "                msg += f\" | RewDiff: {rew_diff:.4f}\"\n",
    "            print(msg)\n",
    "\n",
    "    return reward_weights  # We'll convert to R(s) outside\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 9) Main Demo\n",
    "# -------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Print Base Experiment Info\n",
    "    # -----------------------------------------\n",
    "    print(\"=== Base Experiment Info ===\")\n",
    "    print(f\"States:        {n_states}\")\n",
    "    print(f\"Actions:       {n_actions}\")\n",
    "    print(f\"eps (noise):   {eps}\")\n",
    "    print(f\"gamma:         {gamma}\")\n",
    "    print(f\"True Rewards:  {true_rewards}\")\n",
    "    n_sample_trajectories = 500\n",
    "    trajectory_length = 50\n",
    "    print(f\"NumTraj:       {n_sample_trajectories}\")\n",
    "    print(f\"TrajLen:       {trajectory_length}\")\n",
    "    print(\"============================\\n\")\n",
    "\n",
    "    # Build transitions, get \"true\" V, policy\n",
    "    V_true = soft_value_iteration(true_rewards, P)\n",
    "    policy_true, _ = compute_policy(V_true, true_rewards, P)\n",
    "\n",
    "    # Generate expert data\n",
    "    expert_trajectories = generate_soft_optimal_trajectories(\n",
    "        policy_true, P,\n",
    "        n_trajectories=n_sample_trajectories,\n",
    "        trajectory_length=trajectory_length\n",
    "    )\n",
    "\n",
    "    # We'll evaluate anchor_mode in [0,1,2] and reg_lambda in [0,0.05,0.1,0.2,0.7].\n",
    "    anchor_modes = [0, 1, 2]\n",
    "    reg_lambdas = [0.0, 0.05, 0.1, 0.2, 0.7]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Precompute the expert state distribution to measure \"svf_diff\"\n",
    "    # We'll also keep the total steps for reference\n",
    "    expert_svf_raw = np.zeros(n_states)\n",
    "    tot_steps = 0\n",
    "    for traj in expert_trajectories:\n",
    "        for (s,a,s_next) in traj:\n",
    "            expert_svf_raw[s] += 1\n",
    "        tot_steps += len(traj)\n",
    "    expert_svf_raw /= tot_steps\n",
    "\n",
    "    for am in anchor_modes:\n",
    "        if am == 0:\n",
    "            anc_vals = None\n",
    "            am_str = \"FixNone\"\n",
    "        elif am == 1:\n",
    "            anc_vals = [0.0]\n",
    "            am_str = \"FixFirst\"\n",
    "        else:\n",
    "            anc_vals = [0.0, 0.25]\n",
    "            am_str = \"FixFirstSecond\"\n",
    "\n",
    "        for rl in reg_lambdas:\n",
    "            # (1) Run IRL -> returns the final reward weights\n",
    "            final_weights = maxent_irl(\n",
    "                features,\n",
    "                expert_trajectories,\n",
    "                P,\n",
    "                anchor_mode=am,\n",
    "                anchor_values=anc_vals,\n",
    "                reg_lambda=rl,\n",
    "                gamma=gamma,\n",
    "                lr=0.01,\n",
    "                n_iters=10000,\n",
    "                print_every=2000,\n",
    "                verbose=False,\n",
    "                true_rewards=true_rewards,\n",
    "                true_policy=policy_true,\n",
    "                true_value=V_true\n",
    "            )\n",
    "            # Convert final weights -> final reward\n",
    "            est_rew = features @ final_weights\n",
    "\n",
    "            # (2) Recompute final policy etc.\n",
    "            V_est = soft_value_iteration(est_rew, P)\n",
    "            policy_est, _ = compute_policy(V_est, est_rew, P)\n",
    "\n",
    "            # 2a) PolDiff\n",
    "            pol_diff = np.sum(np.abs(policy_est - policy_true))\n",
    "\n",
    "            # 2b) RewDiff\n",
    "            rew_diff = np.sum(np.abs(est_rew - true_rewards))\n",
    "\n",
    "            # 2c) SVF difference\n",
    "            svf_est = compute_svf(policy_est, P, 0, trajectory_length=5)\n",
    "            svf_est /= np.sum(svf_est)\n",
    "            svf_diff = np.sum(np.abs(svf_est - expert_svf_raw))\n",
    "\n",
    "            # 2d) Gradient norm\n",
    "            grad_main = expert_svf_raw - svf_est\n",
    "            grad_norm = np.linalg.norm(grad_main)\n",
    "\n",
    "            # 2e) Value difference\n",
    "            val_diff = np.sum(np.abs(V_est - V_true))\n",
    "\n",
    "            # 2f) Log-likelihood of expert demos\n",
    "            loglik = compute_expert_log_likelihood(est_rew, expert_trajectories, P)\n",
    "\n",
    "            # Store\n",
    "            results[(am_str, rl)] = {\n",
    "                'R': est_rew,\n",
    "                'PolDiff': pol_diff,\n",
    "                'RewDiff': rew_diff,\n",
    "                'GradNorm': grad_norm,\n",
    "                'ValDiff': val_diff,\n",
    "                'SvfDiff': svf_diff,\n",
    "                'LogLik': loglik\n",
    "            }\n",
    "\n",
    "    # Print final table\n",
    "    print(\"\\n=== Final Results Table ===\")\n",
    "    print(\"AnchorMode       | reg_lambda |   R(0)    R(1)    R(2)   | RewDiff | PolDiff | GradNorm | ValDiff | SvfDiff | LogLik\")\n",
    "    print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    for am in anchor_modes:\n",
    "        if am == 0:\n",
    "            am_str = \"FixNone\"\n",
    "        elif am == 1:\n",
    "            am_str = \"FixFirst\"\n",
    "        else:\n",
    "            am_str = \"FixFirstSecond\"\n",
    "\n",
    "        for rl in reg_lambdas:\n",
    "            rvals = results[(am_str, rl)]\n",
    "            R_0, R_1, R_2 = rvals['R']\n",
    "            print(f\"{am_str:<16} | {rl:<9.2f} | {R_0:7.4f} {R_1:7.4f} {R_2:7.4f} \"\n",
    "                  f\"| {rvals['RewDiff']:.4f}  \"\n",
    "                  f\"| {rvals['PolDiff']:.4f}  \"\n",
    "                  f\"| {rvals['GradNorm']:.4f}  \"\n",
    "                  f\"| {rvals['ValDiff']:.4f}  \"\n",
    "                  f\"| {rvals['SvfDiff']:.4f}  \"\n",
    "                  f\"| {rvals['LogLik']:.4f}\")\n",
    "    print(\"==================================================================================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NxFP Demo Over Table of (anchor_mode, reg_lambda) ===\n",
      "States=3, Actions=3, eps=0.05, gamma=0.9\n",
      "True Rewards: [0.   0.25 0.9 ]\n",
      "NumTraj=5000, TrajLen=50\n",
      "\n",
      "=== NxFP Results Table ===\n",
      "AnchorMode       | reg_lambda |    R(0)    R(1)    R(2)   | RewDiff | PolDiff | GradNorm | ValDiff | SvfDiff | LogLik     | -LL(Obj)\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "FixNone          | 0.00      |  -0.4380  -0.1861   0.4639 | 1.3102 | 0.0017 | 0.1875 | 13.0971 | 0.3032 | -260695.14 | 260695.14\n",
      "FixNone          | 0.05      |  -0.4394  -0.1870   0.4630 | 1.3134 | 0.0021 | 0.1874 | 13.1281 | 0.3031 | -260695.15 | 260695.17\n",
      "FixNone          | 0.10      |  -0.4303  -0.1795   0.4714 | 1.2883 | 0.0016 | 0.1875 | 12.8774 | 0.3033 | -260695.15 | 260695.19\n",
      "FixNone          | 0.20      |  -0.4366  -0.1852   0.4660 | 1.3058 | 0.0023 | 0.1874 | 13.0498 | 0.3031 | -260695.16 | 260695.25\n",
      "FixNone          | 0.70      |  -0.4362  -0.1853   0.4659 | 1.3056 | 0.0021 | 0.1875 | 13.0491 | 0.3033 | -260695.16 | 260695.46\n",
      "FixFirst         | 0.00      |   0.0000   0.2512   0.9013 | 0.0025 | 0.0011 | 0.1875 | 0.0288 | 0.3033 | -260695.14 | 260695.14\n",
      "FixFirst         | 0.05      |   0.0000   0.2505   0.9007 | 0.0012 | 0.0006 | 0.1876 | 0.0147 | 0.3035 | -260695.16 | 260695.20\n",
      "FixFirst         | 0.10      |   0.0000   0.2498   0.9016 | 0.0018 | 0.0022 | 0.1875 | 0.0215 | 0.3034 | -260695.19 | 260695.28\n",
      "FixFirst         | 0.20      |   0.0000   0.2515   0.9019 | 0.0034 | 0.0016 | 0.1875 | 0.0391 | 0.3032 | -260695.14 | 260695.31\n",
      "FixFirst         | 0.70      |   0.0000   0.2528   0.9032 | 0.0060 | 0.0028 | 0.1873 | 0.0692 | 0.3029 | -260695.17 | 260695.78\n",
      "FixFirstSecond   | 0.00      |   0.0000   0.2500   0.9006 | 0.0006 | 0.0007 | 0.1877 | 0.0079 | 0.3036 | -260695.17 | 260695.17\n",
      "FixFirstSecond   | 0.05      |   0.0000   0.2500   0.9006 | 0.0006 | 0.0008 | 0.1877 | 0.0082 | 0.3036 | -260695.17 | 260695.21\n",
      "FixFirstSecond   | 0.10      |   0.0000   0.2500   0.9008 | 0.0008 | 0.0010 | 0.1876 | 0.0111 | 0.3035 | -260695.17 | 260695.25\n",
      "FixFirstSecond   | 0.20      |   0.0000   0.2500   0.9017 | 0.0017 | 0.0022 | 0.1875 | 0.0238 | 0.3034 | -260695.18 | 260695.35\n",
      "FixFirstSecond   | 0.70      |   0.0000   0.2500   0.9007 | 0.0007 | 0.0009 | 0.1876 | 0.0099 | 0.3035 | -260695.17 | 260695.74\n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy.special import logsumexp\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1) Triangular MDP Setup\n",
    "# -------------------------------------------------\n",
    "eps = 0.05\n",
    "n_states = 3\n",
    "n_actions = 3\n",
    "gamma = 0.9\n",
    "\n",
    "def build_transition_matrix(eps=0.05):\n",
    "    \"\"\"\n",
    "    Triangular 3-state MDP with 3 actions.\n",
    "    P[(s, a)] = probability distribution over next states [p0, p1, p2].\n",
    "    \n",
    "    Action 0=left, 1=right, 2=stay\n",
    "    \"\"\"\n",
    "    P = {}\n",
    "    def dist(target):\n",
    "        d = np.ones(n_states) * (eps / n_states)\n",
    "        d[target] += (1 - eps)\n",
    "        return d\n",
    "    \n",
    "    for s in range(n_states):\n",
    "        if s == 0:\n",
    "            P[(0,0)] = dist(target=2)  # left\n",
    "            P[(0,1)] = dist(target=1)  # right\n",
    "            P[(0,2)] = dist(target=0)  # stay\n",
    "        elif s == 1:\n",
    "            P[(1,0)] = dist(target=0)\n",
    "            P[(1,1)] = dist(target=2)\n",
    "            P[(1,2)] = dist(target=1)\n",
    "        else:  # s=2\n",
    "            P[(2,0)] = dist(target=1)\n",
    "            P[(2,1)] = dist(target=0)\n",
    "            P[(2,2)] = dist(target=2)\n",
    "    return P\n",
    "\n",
    "P = build_transition_matrix(eps)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) Features & True Rewards\n",
    "# -------------------------------------------------\n",
    "features = np.eye(n_states)\n",
    "true_rewards = np.array([0.0, 0.25, 0.9])\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3) Soft Value Iteration (Inner Loop)\n",
    "# -------------------------------------------------\n",
    "def soft_value_iteration(reward, P, tol=1e-6, max_iter=200):\n",
    "    \"\"\"\n",
    "    Returns the value function V(s) under the 'soft' (MaxEnt) Bellman update:\n",
    "      V(s) = log sum_a exp( reward[s] + gamma * sum_{s'} P(s'|s,a)*V(s') ).\n",
    "    \"\"\"\n",
    "    V = np.zeros(n_states)\n",
    "    for _ in range(max_iter):\n",
    "        V_prev = V.copy()\n",
    "        for s in range(n_states):\n",
    "            Q_sa = []\n",
    "            for a in range(n_actions):\n",
    "                Q_sa.append(reward[s] + gamma * np.dot(P[(s,a)], V_prev))\n",
    "            V[s] = logsumexp(Q_sa)\n",
    "        if np.max(np.abs(V - V_prev)) < tol:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def compute_policy(V, reward, P):\n",
    "    \"\"\"\n",
    "    Softmax policy pi(a|s) = exp(Q(s,a)) / sum_{a'} exp(Q(s,a')).\n",
    "    Returns policy[s,a] and Q_values[s,a].\n",
    "    \"\"\"\n",
    "    policy = np.zeros((n_states, n_actions))\n",
    "    Q_values = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        Q_sa = [\n",
    "            reward[s] + gamma * np.dot(P[(s,a)], V)\n",
    "            for a in range(n_actions)\n",
    "        ]\n",
    "        Q_values[s] = Q_sa\n",
    "        shift = Q_sa - np.max(Q_sa)\n",
    "        policy[s] = np.exp(shift) / np.sum(np.exp(shift))\n",
    "    return policy, Q_values\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4) Generating Expert Data\n",
    "# -------------------------------------------------\n",
    "def generate_soft_optimal_trajectories(policy, P, n_trajectories=100, trajectory_length=5):\n",
    "    \"\"\"\n",
    "    Samples states by following 'policy'.\n",
    "    \"\"\"\n",
    "    trajectories = []\n",
    "    for _ in range(n_trajectories):\n",
    "        traj = []\n",
    "        state = np.random.choice(n_states)\n",
    "        for __ in range(trajectory_length):\n",
    "            action = np.random.choice(n_actions, p=policy[state])\n",
    "            next_state = np.random.choice(n_states, p=P[(state, action)])\n",
    "            traj.append((state, action, next_state))\n",
    "            state = next_state\n",
    "        trajectories.append(traj)\n",
    "    return trajectories\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5) NxFP with L2 Penalty + Anchors\n",
    "# -------------------------------------------------\n",
    "def nfxp_objective(unconstrained_params, anchor_mode, anchor_values, reg_lambda, expert_trajectories):\n",
    "    \"\"\"\n",
    "    This function takes a parameter vector whose dimension depends on anchor_mode:\n",
    "      anchor_mode=0 => len(unconstrained_params) = 3\n",
    "      anchor_mode=1 => len(unconstrained_params) = 2\n",
    "      anchor_mode=2 => len(unconstrained_params) = 1\n",
    "\n",
    "    We reconstruct the full reward vector [r0, r1, r2], compute negative log-likelihood,\n",
    "    then add reg_lambda * || unanchored_params ||^2.\n",
    "    \"\"\"\n",
    "    # 1) Reconstruct full 3D reward from anchored + unconstrained\n",
    "    if anchor_mode == 0:\n",
    "        # No anchors, unconstrained_params = [r0, r1, r2]\n",
    "        r0, r1, r2 = unconstrained_params\n",
    "    elif anchor_mode == 1:\n",
    "        # anchor R(0) = anchor_values[0], optimize R(1), R(2)\n",
    "        r0 = anchor_values[0]\n",
    "        r1, r2 = unconstrained_params\n",
    "    else:\n",
    "        # anchor R(0), R(1), only optimize R(2)\n",
    "        r0 = anchor_values[0]\n",
    "        r1 = anchor_values[1]\n",
    "        (r2,) = unconstrained_params\n",
    "\n",
    "    reward = np.array([r0, r1, r2])\n",
    "\n",
    "    # 2) Inner loop: compute policy\n",
    "    V = soft_value_iteration(reward, P)\n",
    "    policy_est, _ = compute_policy(V, reward, P)\n",
    "\n",
    "    # 3) Compute negative log-likelihood\n",
    "    eps_small = 1e-12\n",
    "    nll = 0.0\n",
    "    for traj in expert_trajectories:\n",
    "        for (s, a, s_next) in traj:\n",
    "            prob_a = policy_est[s,a]\n",
    "            if prob_a < eps_small:\n",
    "                nll += -np.log(eps_small)\n",
    "            else:\n",
    "                nll += -np.log(prob_a)\n",
    "            # (Optional) If you want transitions in the likelihood:\n",
    "            # nll += -np.log(P[(s,a)][s_next] + eps_small)\n",
    "\n",
    "    # 4) L2 penalty only on the \"free\" parameters\n",
    "    penalty = reg_lambda * np.sum(unconstrained_params**2)\n",
    "\n",
    "    return nll + penalty\n",
    "\n",
    "def estimate_nfxp(expert_trajectories, anchor_mode, anchor_values, reg_lambda):\n",
    "    \"\"\"\n",
    "    Minimizes NxFP objective for each (anchor_mode, anchor_values, reg_lambda).\n",
    "    \"\"\"\n",
    "    # Build initial guess depending on anchor_mode\n",
    "    if anchor_mode == 0:\n",
    "        # 3 free params\n",
    "        x0 = np.array([0.1, 0.1, 0.1])\n",
    "        bnds = [(-2,2)]*3\n",
    "    elif anchor_mode == 1:\n",
    "        # 2 free params for R(1), R(2), anchor R(0)\n",
    "        x0 = np.array([0.1, 0.1])\n",
    "        bnds = [(-2,2)]*2\n",
    "    else:\n",
    "        # 1 free param for R(2), anchor R(0), R(1)\n",
    "        x0 = np.array([0.1])\n",
    "        bnds = [(-2,2)]\n",
    "\n",
    "    # We'll pass a wrapper for objective that closes over anchor_mode, anchor_values, etc.\n",
    "    def wrapper(unconstrained_params):\n",
    "        return nfxp_objective(\n",
    "            unconstrained_params, anchor_mode, anchor_values, reg_lambda,\n",
    "            expert_trajectories\n",
    "        )\n",
    "\n",
    "    result = minimize(\n",
    "        wrapper,\n",
    "        x0,\n",
    "        method='L-BFGS-B',\n",
    "        bounds=bnds,\n",
    "        options={'maxiter':300, 'disp':False}\n",
    "    )\n",
    "\n",
    "    # Reconstruct full reward from result.x\n",
    "    if anchor_mode == 0:\n",
    "        r0, r1, r2 = result.x\n",
    "    elif anchor_mode == 1:\n",
    "        r0 = anchor_values[0]\n",
    "        r1, r2 = result.x\n",
    "    else:\n",
    "        r0 = anchor_values[0]\n",
    "        r1 = anchor_values[1]\n",
    "        (r2,) = result.x\n",
    "\n",
    "    est_rew = np.array([r0, r1, r2])\n",
    "    return est_rew, result.fun\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6) Evaluate NxFP for All Table Scenarios\n",
    "# -------------------------------------------------\n",
    "def compute_expert_log_likelihood(reward, expert_trajectories, P):\n",
    "    # Rebuild policy\n",
    "    V_est = soft_value_iteration(reward, P)\n",
    "    policy_est, _ = compute_policy(V_est, reward, P)\n",
    "    total_loglik = 0.0\n",
    "    eps_small = 1e-12\n",
    "    for traj in expert_trajectories:\n",
    "        for (s, a, s_next) in traj:\n",
    "            p_a = policy_est[s,a]\n",
    "            total_loglik += np.log(p_a + eps_small)\n",
    "            # If transitions matter: total_loglik += np.log(P[(s,a)][s_next] + eps_small)\n",
    "    return total_loglik\n",
    "\n",
    "def compute_svf(policy, P, start_state=0, trajectory_length=5):\n",
    "    d_t = np.zeros(n_states)\n",
    "    d_t[start_state] = 1.0\n",
    "    svf = np.zeros(n_states)\n",
    "    for _ in range(trajectory_length):\n",
    "        svf += d_t\n",
    "        next_d_t = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                next_d_t += d_t[s] * policy[s,a] * P[(s,a)]\n",
    "        d_t = next_d_t\n",
    "    return svf\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Basic Info\n",
    "    print(\"=== NxFP Demo Over Table of (anchor_mode, reg_lambda) ===\")\n",
    "    print(f\"States={n_states}, Actions={n_actions}, eps={eps}, gamma={gamma}\")\n",
    "    print(\"True Rewards:\", true_rewards)\n",
    "    n_sample_trajectories = 5000\n",
    "    trajectory_length = 50\n",
    "    print(f\"NumTraj={n_sample_trajectories}, TrajLen={trajectory_length}\\n\")\n",
    "\n",
    "    # 2) Generate Expert Data\n",
    "    V_true = soft_value_iteration(true_rewards, P)\n",
    "    policy_true, _ = compute_policy(V_true, true_rewards, P)\n",
    "    expert_trajectories = generate_soft_optimal_trajectories(\n",
    "        policy_true, P,\n",
    "        n_trajectories=n_sample_trajectories,\n",
    "        trajectory_length=trajectory_length\n",
    "    )\n",
    "\n",
    "    # 3) Evaluate all combinations of anchor_mode and reg_lambda\n",
    "    anchor_modes = [0, 1, 2]          # 0=FixNone, 1=FixFirst, 2=FixFirstSecond\n",
    "    reg_lambdas = [0.0, 0.05, 0.1, 0.2, 0.7]\n",
    "\n",
    "    # Predefine anchor_values\n",
    "    #  mode=1 => anchor R(0)=0.0\n",
    "    #  mode=2 => anchor R(0)=0.0, R(1)=0.25\n",
    "    results = {}\n",
    "\n",
    "    for am in anchor_modes:\n",
    "        if am == 0:\n",
    "            am_str = \"FixNone\"\n",
    "            anc_vals = None\n",
    "        elif am == 1:\n",
    "            am_str = \"FixFirst\"\n",
    "            anc_vals = [0.0]\n",
    "        else:\n",
    "            am_str = \"FixFirstSecond\"\n",
    "            anc_vals = [0.0, 0.25]\n",
    "\n",
    "        for rl in reg_lambdas:\n",
    "            # (1) Run NxFP estimation\n",
    "            est_rew, final_obj = estimate_nfxp(expert_trajectories, am, anc_vals, rl)\n",
    "\n",
    "            # (2) Evaluate\n",
    "            # 2a) Policy\n",
    "            V_est = soft_value_iteration(est_rew, P)\n",
    "            policy_est, _ = compute_policy(V_est, est_rew, P)\n",
    "            pol_diff = np.sum(np.abs(policy_est - policy_true))\n",
    "\n",
    "            # 2b) RewDiff\n",
    "            rew_diff = np.sum(np.abs(est_rew - true_rewards))\n",
    "\n",
    "            # 2c) SVF difference vs. the \"raw\" expert distribution (s-only)\n",
    "            #    We do the same 5-step distribution from state=0 as in the IRL script.\n",
    "            svf_est = compute_svf(policy_est, P, 0, trajectory_length=5)\n",
    "            # The expert distribution was from actual rollouts of length=50 with random starts,\n",
    "            # so this 'svf' won't match exactly. For consistency with IRL script, we do the same approach:\n",
    "            # We'll just track it anyway:\n",
    "            svf_est /= np.sum(svf_est)\n",
    "\n",
    "            # Build \"expert_svf_raw\" from the data\n",
    "            expert_svf_raw = np.zeros(n_states)\n",
    "            tot_steps = 0\n",
    "            for traj in expert_trajectories:\n",
    "                for (s,a,s_next) in traj:\n",
    "                    expert_svf_raw[s] += 1\n",
    "                tot_steps += len(traj)\n",
    "            expert_svf_raw /= tot_steps\n",
    "\n",
    "            svf_diff = np.sum(np.abs(svf_est - expert_svf_raw))\n",
    "            grad_norm = np.linalg.norm(expert_svf_raw - svf_est)\n",
    "\n",
    "            # 2d) Value difference\n",
    "            val_diff = np.sum(np.abs(V_est - V_true))\n",
    "\n",
    "            # 2e) Log-likelihood of expert demos\n",
    "            loglik = compute_expert_log_likelihood(est_rew, expert_trajectories, P)\n",
    "\n",
    "            # Store\n",
    "            results[(am_str, rl)] = {\n",
    "                'R': est_rew,\n",
    "                'Obj': final_obj,\n",
    "                'PolDiff': pol_diff,\n",
    "                'RewDiff': rew_diff,\n",
    "                'GradNorm': grad_norm,\n",
    "                'ValDiff': val_diff,\n",
    "                'SvfDiff': svf_diff,\n",
    "                'LogLik': loglik\n",
    "            }\n",
    "\n",
    "    # 4) Print table of results\n",
    "    print(\"=== NxFP Results Table ===\")\n",
    "    print(\"AnchorMode       | reg_lambda |    R(0)    R(1)    R(2)   | RewDiff | PolDiff | GradNorm | ValDiff | SvfDiff | LogLik     | -LL(Obj)\")\n",
    "    print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    for am in anchor_modes:\n",
    "        if am == 0:\n",
    "            am_str = \"FixNone\"\n",
    "        elif am == 1:\n",
    "            am_str = \"FixFirst\"\n",
    "        else:\n",
    "            am_str = \"FixFirstSecond\"\n",
    "\n",
    "        for rl in reg_lambdas:\n",
    "            rvals = results[(am_str, rl)]\n",
    "            R_0, R_1, R_2 = rvals['R']\n",
    "            print(f\"{am_str:<16} | {rl:<9.2f} | {R_0:8.4f} {R_1:8.4f} {R_2:8.4f} \"\n",
    "                  f\"| {rvals['RewDiff']:.4f} \"\n",
    "                  f\"| {rvals['PolDiff']:.4f} \"\n",
    "                  f\"| {rvals['GradNorm']:.4f} \"\n",
    "                  f\"| {rvals['ValDiff']:.4f} \"\n",
    "                  f\"| {rvals['SvfDiff']:.4f} \"\n",
    "                  f\"| {rvals['LogLik']:.2f} \"\n",
    "                  f\"| {rvals['Obj']:.2f}\")\n",
    "    print(\"====================================================================================\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
